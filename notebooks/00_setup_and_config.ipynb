{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 00: Setup and Configuration\n",
    "\n",
    "**Purpose**: Environment setup, code packaging, and configuration validation\n",
    "\n",
    "**Key Tasks**:\n",
    "1. Validate environment (Python, S3 access)\n",
    "2. Load and validate training configuration\n",
    "3. Package `query_predictor` for Spark distribution\n",
    "4. Upload to S3 with versioning\n",
    "5. Generate Spark configuration snippet for next notebooks\n",
    "\n",
    "**Prerequisites**:\n",
    "- Python 3.8+\n",
    "- AWS credentials configured\n",
    "- S3 bucket access\n",
    "\n",
    "**Duration**: ~5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies and Validate Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:51:51.297601Z",
     "iopub.status.busy": "2025-10-16T21:51:51.297419Z",
     "iopub.status.idle": "2025-10-16T21:51:51.779254Z",
     "shell.execute_reply": "2025-10-16T21:51:51.778749Z",
     "shell.execute_reply.started": "2025-10-16T21:51:51.297582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 (main, Jun 20 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]\n",
      "Python executable: /usr/local/bin/python\n",
      "‚úÖ Python version validated\n",
      "‚úÖ S3 access validated: uip-datalake-bucket-prod\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import boto3\n",
    "import yaml\n",
    "import shutil\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Validate Python version\n",
    "assert sys.version_info >= (3, 8), \"Python 3.8+ required\"\n",
    "print(\"‚úÖ Python version validated\")\n",
    "\n",
    "# Validate S3 access\n",
    "try:\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket = 'uip-datalake-bucket-prod'\n",
    "    s3.head_bucket(Bucket=bucket)\n",
    "    print(f\"‚úÖ S3 access validated: {bucket}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå S3 access failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Validate Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:51:51.779990Z",
     "iopub.status.busy": "2025-10-16T21:51:51.779821Z",
     "iopub.status.idle": "2025-10-16T21:51:51.863896Z",
     "shell.execute_reply": "2025-10-16T21:51:51.863449Z",
     "shell.execute_reply.started": "2025-10-16T21:51:51.779975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded from local file\n",
      "‚úÖ Configuration validated\n",
      "\n",
      "üìã Training Configuration:\n",
      "  Date range: 2025-08-01 to 2025-10-01\n",
      "  Features: 78 base + 17 historical + 250 TF-IDF = 345 total\n",
      "  Balance ratio: 5.0:1 (Small:Heavy)\n",
      "  Model: XGBOOST\n",
      "  Target recall: ‚â•0.98\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Load from local file (PRIMARY - for initial setup)\n",
    "config_path = '../config/training_config.yaml'\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded from local file\")\n",
    "\n",
    "# Method 2: Download from S3 (ALTERNATIVE - for reproducibility)\n",
    "# Uncomment to use config from previous run:\n",
    "# import boto3\n",
    "# s3_client = boto3.client('s3')\n",
    "# s3_bucket = 'uip-datalake-bucket-prod'\n",
    "# s3_key = 'sf_trino/trino_query_predictor/config/training_config_latest.yaml'\n",
    "# config_path = '/tmp/training_config.yaml'\n",
    "# s3_client.download_file(s3_bucket, s3_key, config_path)\n",
    "# with open(config_path) as f:\n",
    "#     config = yaml.safe_load(f)\n",
    "# print(\"‚úÖ Configuration loaded from S3\")\n",
    "\n",
    "# Validate configuration using ConfigValidator\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from query_predictor.training.config_validator import ConfigValidator\n",
    "\n",
    "validator = ConfigValidator()\n",
    "errors = validator.validate(config)\n",
    "\n",
    "if errors:\n",
    "    print(\"\\n‚ùå Configuration validation failed:\")\n",
    "    for error in errors:\n",
    "        print(f\"  - {error}\")\n",
    "    raise ValueError(f\"Invalid configuration: {len(errors)} errors found\")\n",
    "\n",
    "print(\"‚úÖ Configuration validated\")\n",
    "\n",
    "# OPTIONAL: Override config parameters after loading\n",
    "# Example: Change date range for quick testing\n",
    "# config['data_loading']['start_date'] = '2025-09-01'\n",
    "# config['data_loading']['end_date'] = '2025-09-15'\n",
    "# Example: Change sampling ratio\n",
    "# config['boundary_sampling']['balance_ratio'] = 3.0\n",
    "# Example: Disable analysis for faster execution\n",
    "# config['analysis']['enabled'] = False\n",
    "\n",
    "# Display key configuration\n",
    "print(\"\\nüìã Training Configuration:\")\n",
    "print(f\"  Date range: {config['data_loading']['start_date']} to {config['data_loading']['end_date']}\")\n",
    "print(f\"  Features: {config['features']['base_feature_count']} base + \"\n",
    "      f\"{config['features']['historical_feature_count']} historical + \"\n",
    "      f\"{config['features']['tfidf_vocab_size']} TF-IDF = \"\n",
    "      f\"{config['features']['total_features']} total\")\n",
    "print(f\"  Balance ratio: {config['boundary_sampling']['balance_ratio']}:1 (Small:Heavy)\")\n",
    "print(f\"  Model: {config['model']['algorithm'].upper()}\")\n",
    "print(f\"  Target recall: ‚â•{config['prd_requirements']['target_heavy_recall']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Package query_predictor for Spark Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:51:51.864529Z",
     "iopub.status.busy": "2025-10-16T21:51:51.864375Z",
     "iopub.status.idle": "2025-10-16T21:51:52.084077Z",
     "shell.execute_reply": "2025-10-16T21:51:52.083603Z",
     "shell.execute_reply.started": "2025-10-16T21:51:51.864515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository root: /home/pmannem/trino-query-predictor\n",
      "Source directory: /home/pmannem/trino-query-predictor/query_predictor\n",
      "\n",
      "üì¶ Packaging query_predictor...\n",
      "‚úÖ Package created: /tmp/query_predictor.zip (0.08 MB)\n"
     ]
    }
   ],
   "source": [
    "# Determine paths\n",
    "repo_root = Path.cwd().parent\n",
    "src_dir = repo_root / 'query_predictor'\n",
    "temp_dir = Path('/tmp/query_predictor_package')\n",
    "zip_path = Path('/tmp/query_predictor.zip')\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")\n",
    "print(f\"Source directory: {src_dir}\")\n",
    "\n",
    "# Create temp directory\n",
    "if temp_dir.exists():\n",
    "    shutil.rmtree(temp_dir)\n",
    "temp_dir.mkdir(parents=True)\n",
    "\n",
    "# Copy query_predictor package\n",
    "print(\"\\nüì¶ Packaging query_predictor...\")\n",
    "dest_dir = temp_dir / 'query_predictor'\n",
    "shutil.copytree(src_dir, dest_dir, ignore=shutil.ignore_patterns('__pycache__', '*.pyc', '*.pyo', '.DS_Store'))\n",
    "\n",
    "# Create ZIP file\n",
    "if zip_path.exists():\n",
    "    zip_path.unlink()\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        # Exclude __pycache__ directories\n",
    "        dirs[:] = [d for d in dirs if d != '__pycache__']\n",
    "        for file in files:\n",
    "            if not file.endswith(('.pyc', '.pyo')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, temp_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "zip_size_mb = os.path.getsize(zip_path) / 1024 / 1024\n",
    "print(f\"‚úÖ Package created: {zip_path} ({zip_size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload to S3 with Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:51:52.084641Z",
     "iopub.status.busy": "2025-10-16T21:51:52.084493Z",
     "iopub.status.idle": "2025-10-16T21:51:52.221723Z",
     "shell.execute_reply": "2025-10-16T21:51:52.221239Z",
     "shell.execute_reply.started": "2025-10-16T21:51:52.084626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚òÅÔ∏è  Uploading to S3...\n",
      "  Bucket: uip-datalake-bucket-prod\n",
      "  Key: sf_trino/trino_query_predictor/code/query_predictor_20251016_215152.zip\n",
      "‚úÖ Uploaded versioned package: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_20251016_215152.zip\n",
      "‚úÖ Updated latest package: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip\n",
      "\n",
      "üìç Package S3 path: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip\n"
     ]
    }
   ],
   "source": [
    "# Generate version timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "s3_key = f\"{config['s3']['prefix']}/code/query_predictor_{timestamp}.zip\"\n",
    "\n",
    "print(f\"\\n‚òÅÔ∏è  Uploading to S3...\")\n",
    "print(f\"  Bucket: {config['s3']['bucket']}\")\n",
    "print(f\"  Key: {s3_key}\")\n",
    "\n",
    "# Upload versioned file\n",
    "s3.upload_file(str(zip_path), config['s3']['bucket'], s3_key)\n",
    "print(f\"‚úÖ Uploaded versioned package: s3://{config['s3']['bucket']}/{s3_key}\")\n",
    "\n",
    "# Also save as \"latest\"\n",
    "latest_key = f\"{config['s3']['prefix']}/code/query_predictor_latest.zip\"\n",
    "s3.copy_object(\n",
    "    Bucket=config['s3']['bucket'],\n",
    "    CopySource={'Bucket': config['s3']['bucket'], 'Key': s3_key},\n",
    "    Key=latest_key\n",
    ")\n",
    "print(f\"‚úÖ Updated latest package: s3://{config['s3']['bucket']}/{latest_key}\")\n",
    "\n",
    "# Store for next notebooks\n",
    "package_s3_path = f\"s3://{config['s3']['bucket']}/{latest_key}\"\n",
    "print(f\"\\nüìç Package S3 path: {package_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Spark Configuration Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:51:52.222320Z",
     "iopub.status.busy": "2025-10-16T21:51:52.222167Z",
     "iopub.status.idle": "2025-10-16T21:51:52.226065Z",
     "shell.execute_reply": "2025-10-16T21:51:52.225620Z",
     "shell.execute_reply.started": "2025-10-16T21:51:52.222305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SPARK CONFIGURATION FOR NEXT NOTEBOOKS\n",
      "======================================================================\n",
      "\n",
      "Copy this to the first cell of notebooks 01-04:\n",
      "\n",
      "%%configure -f\n",
      "{\n",
      "    \"pyFiles\": [\n",
      "        \"s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip\",\n",
      "        \"s3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py\"\n",
      "    ],\n",
      "    \"driverMemory\": \"16G\",\n",
      "    \"driverCores\": 4,\n",
      "    \"executorMemory\": \"20G\",\n",
      "    \"executorCores\": 5,\n",
      "    \"conf\": {\n",
      "        \"spark.driver.maxResultSize\": \"8G\",\n",
      "        \"spark.dynamicAllocation.enabled\": \"true\",\n",
      "        \"spark.dynamicAllocation.minExecutors\": \"2\",\n",
      "        \"spark.dynamicAllocation.maxExecutors\": \"20\"\n",
      "    }\n",
      "}\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "spark_config = f'''%%configure -f\n",
    "{{\n",
    "    \"pyFiles\": [\n",
    "        \"{package_s3_path}\",\n",
    "        \"s3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py\"\n",
    "    ],\n",
    "    \"driverMemory\": \"{config['spark']['driver_memory']}\",\n",
    "    \"driverCores\": {config['spark']['driver_cores']},\n",
    "    \"executorMemory\": \"{config['spark']['executor_memory']}\",\n",
    "    \"executorCores\": {config['spark']['executor_cores']},\n",
    "    \"conf\": {{\n",
    "        \"spark.driver.maxResultSize\": \"8G\",\n",
    "        \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "        \"spark.dynamicAllocation.minExecutors\": \"{config['spark']['min_executors']}\",\n",
    "        \"spark.dynamicAllocation.maxExecutors\": \"{config['spark']['max_executors']}\"\n",
    "    }}\n",
    "}}'''\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SPARK CONFIGURATION FOR NEXT NOTEBOOKS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nCopy this to the first cell of notebooks 01-04:\\n\")\n",
    "print(spark_config)\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Configuration Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:51:52.227207Z",
     "iopub.status.busy": "2025-10-16T21:51:52.227053Z",
     "iopub.status.idle": "2025-10-16T21:51:52.449430Z",
     "shell.execute_reply": "2025-10-16T21:51:52.448936Z",
     "shell.execute_reply.started": "2025-10-16T21:51:52.227193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading training config to S3...\n",
      "‚úÖ Config uploaded: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/config/training_config_20251016_215152.yaml\n",
      "‚úÖ Updated latest config: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/config/training_config_latest.yaml\n",
      "‚úÖ Metadata saved locally: /tmp/setup_metadata.json\n",
      "‚úÖ Metadata uploaded: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/metadata/setup_20251016_215152.json\n",
      "\n",
      "üìç For notebooks 01-04, config will be downloaded from: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/config/training_config_latest.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Upload training config to S3 for reproducibility\n",
    "print(\"\\nUploading training config to S3...\")\n",
    "config_s3_key = f\"{config['s3']['prefix']}/config/training_config_{timestamp}.yaml\"\n",
    "s3.upload_file(config_path, config['s3']['bucket'], config_s3_key)\n",
    "config_s3_path = f\"s3://{config['s3']['bucket']}/{config_s3_key}\"\n",
    "print(f\"‚úÖ Config uploaded: {config_s3_path}\")\n",
    "\n",
    "# Also save as \"latest\"\n",
    "latest_config_key = f\"{config['s3']['prefix']}/config/training_config_latest.yaml\"\n",
    "s3.copy_object(\n",
    "    Bucket=config['s3']['bucket'],\n",
    "    CopySource={'Bucket': config['s3']['bucket'], 'Key': config_s3_key},\n",
    "    Key=latest_config_key\n",
    ")\n",
    "latest_config_path = f\"s3://{config['s3']['bucket']}/{latest_config_key}\"\n",
    "print(f\"‚úÖ Updated latest config: {latest_config_path}\")\n",
    "\n",
    "# Prepare metadata\n",
    "metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'package_version': timestamp,\n",
    "    'package_s3_path': package_s3_path,\n",
    "    'config_path': config_s3_path,  # S3 path to versioned config\n",
    "    'latest_config_path': latest_config_path,  # S3 path to latest config\n",
    "    'date_range': f\"{config['data_loading']['start_date']} to {config['data_loading']['end_date']}\",\n",
    "    'total_features': config['features']['total_features'],\n",
    "    'spark_config': config['spark']\n",
    "}\n",
    "\n",
    "# Save locally\n",
    "metadata_path = '/tmp/setup_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Metadata saved locally: {metadata_path}\")\n",
    "\n",
    "# Upload to S3\n",
    "metadata_s3_key = f\"{config['s3']['prefix']}/metadata/setup_{timestamp}.json\"\n",
    "s3.upload_file(metadata_path, config['s3']['bucket'], metadata_s3_key)\n",
    "print(f\"‚úÖ Metadata uploaded: s3://{config['s3']['bucket']}/{metadata_s3_key}\")\n",
    "\n",
    "print(f\"\\nüìç For notebooks 01-04, config will be downloaded from: {latest_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:51:52.450075Z",
     "iopub.status.busy": "2025-10-16T21:51:52.449885Z",
     "iopub.status.idle": "2025-10-16T21:51:52.456549Z",
     "shell.execute_reply": "2025-10-16T21:51:52.456098Z",
     "shell.execute_reply.started": "2025-10-16T21:51:52.450059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleanup completed\n",
      "\n",
      "======================================================================\n",
      "SETUP COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Next Steps:\n",
      "1. Copy the Spark configuration above\n",
      "2. Open notebook 01_data_loading.ipynb\n",
      "3. Paste Spark configuration in first cell\n",
      "4. Run the data loading pipeline\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary files\n",
    "if temp_dir.exists():\n",
    "    shutil.rmtree(temp_dir)\n",
    "if zip_path.exists():\n",
    "    zip_path.unlink()\n",
    "\n",
    "print(\"‚úÖ Cleanup completed\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SETUP COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Copy the Spark configuration above\")\n",
    "print(\"2. Open notebook 01_data_loading.ipynb\")\n",
    "print(\"3. Paste Spark configuration in first cell\")\n",
    "print(\"4. Run the data loading pipeline\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
