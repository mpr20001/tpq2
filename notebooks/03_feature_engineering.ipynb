{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Feature Engineering\n",
    "\n",
    "**Purpose**: Extract 78 + 17 + 1000 features with train-serve parity validation\n",
    "\n",
    "**Pipeline**:\n",
    "1. Load pre-split datasets from notebook 01 (train_sampled, val_original, test_original)\n",
    "2. Extract 78 base features (production `FeatureExtractor`)\n",
    "3. Extract 17 historical features from training data statistics\n",
    "4. Build TF-IDF vocabulary (training data ONLY - prevent data leakage)\n",
    "5. Extract TF-IDF features for all splits\n",
    "6. Validate feature parity (training vs inference)\n",
    "7. Save feature datasets + TF-IDF vectorizer to S3\n",
    "\n",
    "**Key Features**:\n",
    "- Production FeatureExtractor integration\n",
    "- Historical feature computation with cold-start handling\n",
    "- No data leakage (TF-IDF trained on training split only)\n",
    "- Automated parity validation (<0.5% mismatch tolerance)\n",
    "- S3 output for model training\n",
    "\n",
    "**IMPORTANT**: Training data is sampled (5:1), val/test data is original (~36:1).\n",
    "This ensures realistic evaluation metrics that reflect production conditions.\n",
    "\n",
    "**Prerequisites**:\n",
    "- Notebook 00 completed (code package uploaded to S3)\n",
    "- Notebook 01 completed (processed data available at 3 separate paths)\n",
    "\n",
    "**Duration**: ~45-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Configuration\n",
    "\n",
    "Copy the Spark configuration from notebook 00 output and paste below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:07:29.103859Z",
     "iopub.status.busy": "2025-10-16T22:07:29.103566Z",
     "iopub.status.idle": "2025-10-16T22:07:30.316604Z",
     "shell.execute_reply": "2025-10-16T22:07:30.315987Z",
     "shell.execute_reply.started": "2025-10-16T22:07:29.103827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': './environment/bin/python', 'spark.yarn.dist.archives': 's3://uip-datalake-bucket-prod/uipexplore/shared/python311common_env.tar.gz#environment', 'spark.submit.deployMode': 'cluster', 'spark.driver.maxResultSize': '8G', 'spark.dynamicAllocation.enabled': 'true', 'spark.dynamicAllocation.minExecutors': '2', 'spark.dynamicAllocation.maxExecutors': '20'}, 'driverMemory': '16G', 'executorCores': 5, 'executorMemory': '20G', 'pyFiles': ['s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip', 's3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py'], 'driverCores': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3127</td><td>application_1758752217644_207850</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_207850/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-38-4.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_207850_01_000002/sahith.kondepudi\">Link</a></td><td>sahith.kondepudi</td><td></td></tr><tr><td>3144</td><td>application_1758752217644_208422</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_208422/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-133.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_208422_01_000002/w.scroggins\">Link</a></td><td>w.scroggins</td><td></td></tr><tr><td>3161</td><td>application_1758752217644_209559</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209559/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-32-8.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209559_01_000002/a.wadhwa\">Link</a></td><td>a.wadhwa</td><td></td></tr><tr><td>3162</td><td>application_1758752217644_209563</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209563/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-35-229.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209563_01_000013/feifan.jian\">Link</a></td><td>feifan.jian</td><td></td></tr><tr><td>3167</td><td>application_1758752217644_209744</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209744/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-191.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209744_01_000003/c.grey\">Link</a></td><td>c.grey</td><td></td></tr><tr><td>3170</td><td>application_1758752217644_209770</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209770/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-33-255.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209770_01_000002/clyu\">Link</a></td><td>clyu</td><td></td></tr><tr><td>3171</td><td>application_1758752217644_209784</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209784/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-133.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209784_01_000001/s.mylavarapu\">Link</a></td><td>s.mylavarapu</td><td></td></tr><tr><td>3179</td><td>application_1758752217644_210005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_210005/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-133.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_210005_01_000001/pmannem\">Link</a></td><td>pmannem</td><td></td></tr><tr><td>3180</td><td>application_1758752217644_210099</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_210099/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-32-5.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_210099_01_000004/ssardeshpande\">Link</a></td><td>ssardeshpande</td><td></td></tr><tr><td>3193</td><td>application_1758752217644_210699</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_210699/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-32-8.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_210699_01_000003/pmannem\">Link</a></td><td>pmannem</td><td></td></tr><tr><td>3194</td><td>application_1758752217644_210874</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_210874/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-32-5.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_210874_01_000001/a.wadhwa\">Link</a></td><td>a.wadhwa</td><td></td></tr><tr><td>3196</td><td>application_1758752217644_211034</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_211034/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-38-4.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_211034_01_000001/a.wadhwa\">Link</a></td><td>a.wadhwa</td><td></td></tr><tr><td>3197</td><td>application_1758752217644_211049</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_211049/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-33-255.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_211049_01_000001/virender.singh\">Link</a></td><td>virender.singh</td><td></td></tr><tr><td>3201</td><td>application_1758752217644_211229</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_211229/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-199.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_211229_01_000002/pmannem\">Link</a></td><td>pmannem</td><td></td></tr><tr><td>3203</td><td>application_1758752217644_211545</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_211545/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-1.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_211545_01_000002/virender.singh\">Link</a></td><td>virender.singh</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"pyFiles\": [\n",
    "        \"s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip\",\n",
    "        \"s3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py\"\n",
    "    ],\n",
    "    \"driverMemory\": \"16G\",\n",
    "    \"driverCores\": 4,\n",
    "    \"executorMemory\": \"20G\",\n",
    "    \"executorCores\": 5,\n",
    "    \"conf\": {\n",
    "        \"spark.driver.maxResultSize\": \"8G\",\n",
    "        \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "        \"spark.dynamicAllocation.minExecutors\": \"2\",\n",
    "        \"spark.dynamicAllocation.maxExecutors\": \"20\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies and Validate Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:07:32.617939Z",
     "iopub.status.busy": "2025-10-16T22:07:32.617736Z",
     "iopub.status.idle": "2025-10-16T22:09:27.695902Z",
     "shell.execute_reply": "2025-10-16T22:09:27.695005Z",
     "shell.execute_reply.started": "2025-10-16T22:07:32.617917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3204</td><td>application_1758752217644_211576</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_211576/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-191.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_211576_01_000001/pmannem\">Link</a></td><td>pmannem</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b1d471b8e240acb6b9fac9218988c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cfe9c5b7374b90a28e142a9962796b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.13 (main, Jul 30 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]\n",
      "PySpark version: 3.5.4-amzn-0\n",
      "All imports successful"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "# Import production modules\n",
    "from query_predictor.core.featurizer.feature_extractor import FeatureExtractor\n",
    "from query_predictor.training.spark_ml_tfidf_pipeline import SparkMLTfidfPipeline\n",
    "from query_predictor.training.parity_validator import ParityValidator\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {spark.version}\")\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:09:27.696960Z",
     "iopub.status.busy": "2025-10-16T22:09:27.696840Z",
     "iopub.status.idle": "2025-10-16T22:09:29.018940Z",
     "shell.execute_reply": "2025-10-16T22:09:29.018182Z",
     "shell.execute_reply.started": "2025-10-16T22:09:27.696943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e8302f285d4320b4853a3304dca546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config from S3: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/config/training_config_latest.yaml\n",
      "? Config downloaded to: /tmp/training_config.yaml\n",
      "? Configuration loaded\n",
      "\n",
      "? Feature Configuration:\n",
      "  Base features: 78\n",
      "  Historical features: 17\n",
      "  TF-IDF vocab size: 250\n",
      "  Total features: 345\n",
      "\n",
      "? Time Splits:\n",
      "  Train: 30 days\n",
      "  Val: 7 days\n",
      "  Test: 7 days\n",
      "  Checkpointing: ENABLED"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from query_predictor.training.checkpoint_manager import CheckpointManager\n",
    "\n",
    "\n",
    "# Download training configuration from S3\n",
    "s3_client = boto3.client('s3')\n",
    "s3_bucket = 'uip-datalake-bucket-prod'\n",
    "s3_prefix = 'sf_trino/trino_query_predictor'\n",
    "config_s3_key = f\"{s3_prefix}/config/training_config_latest.yaml\"\n",
    "config_path = '/tmp/training_config.yaml'\n",
    "\n",
    "print(f\"Downloading config from S3: s3://{s3_bucket}/{config_s3_key}\")\n",
    "s3_client.download_file(s3_bucket, config_s3_key, config_path)\n",
    "print(f\"✅ Config downloaded to: {config_path}\")\n",
    "\n",
    "# Load training configuration\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_mgr = CheckpointManager(\n",
    "    spark,\n",
    "    s3_checkpoint_path=config['checkpointing']['s3_path'],\n",
    "    enabled=config['checkpointing']['enabled']\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"\\n📋 Feature Configuration:\")\n",
    "print(f\"  Base features: {config['features']['base_feature_count']}\")\n",
    "print(f\"  Historical features: {config['features']['historical_feature_count']}\")\n",
    "print(f\"  TF-IDF vocab size: {config['features']['tfidf_vocab_size']}\")\n",
    "print(f\"  Total features: {config['features']['total_features']}\")\n",
    "print(f\"\\n📅 Time Splits:\")\n",
    "print(f\"  Train: {config['time_splits']['train_days']} days\")\n",
    "print(f\"  Val: {config['time_splits']['val_days']} days\")\n",
    "print(f\"  Test: {config['time_splits']['test_days']} days\")\n",
    "print(f\"  Checkpointing: {'ENABLED' if config['checkpointing']['enabled'] else 'DISABLED'}\")\n",
    "\n",
    "# OPTIONAL: Override config parameters after loading\n",
    "# Example: Change date range to use different processed data\n",
    "# config['data_loading']['start_date'] = '2025-09-01'\n",
    "# config['data_loading']['end_date'] = '2025-10-01'\n",
    "# Example: Change time splits for different train/val/test ratio\n",
    "# config['time_splits']['train_days'] = 20  # Shorter training period\n",
    "# config['time_splits']['val_days'] = 5\n",
    "# config['time_splits']['test_days'] = 5\n",
    "# Example: Change TF-IDF vocabulary size\n",
    "# config['features']['tfidf_vocab_size'] = 500  # Smaller vocabulary\n",
    "# config['features']['total_features'] = 78 + 17 + 500  # Update total\n",
    "# Example: Adjust parity validation tolerance\n",
    "# config['validation']['parity_tolerance'] = 1e-5  # Stricter validation\n",
    "# config['validation']['parity_samples'] = 50  # Fewer samples for faster validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pre-Split Data from Notebook 01\n",
    "\n",
    "Load three separate datasets created by notebook 01:\n",
    "- train_sampled: 5:1 ratio for training\n",
    "- val_original: ~36:1 ratio for validation  \n",
    "- test_original: ~36:1 ratio for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:09:29.019570Z",
     "iopub.status.busy": "2025-10-16T22:09:29.019462Z",
     "iopub.status.idle": "2025-10-16T22:10:19.056730Z",
     "shell.execute_reply": "2025-10-16T22:10:19.055721Z",
     "shell.execute_reply.started": "2025-10-16T22:09:29.019555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2974a6be973474e8e5e77b7582290af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-split datasets from notebook 01...\n",
      "  Base path: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01\n",
      "\n",
      "  Train (sampled):  s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/train_sampled\n",
      "  Val (original):   s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/val_original\n",
      "  Test (original):  s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/test_original\n",
      "\n",
      "Datasets loaded:\n",
      "  Train: 8,782,474 queries\n",
      "  Val:   14,969,526 queries\n",
      "  Test:  15,099,750 queries\n",
      "\n",
      "Class distributions:\n",
      "Train (sampled):\n",
      "+--------+-------+\n",
      "|is_heavy|  count|\n",
      "+--------+-------+\n",
      "|       0|7318751|\n",
      "|       1|1463723|\n",
      "+--------+-------+\n",
      "\n",
      "Val (original):\n",
      "+--------+--------+\n",
      "|is_heavy|   count|\n",
      "+--------+--------+\n",
      "|       0|14665477|\n",
      "|       1|  304049|\n",
      "+--------+--------+\n",
      "\n",
      "Test (original):\n",
      "+--------+--------+\n",
      "|is_heavy|   count|\n",
      "+--------+--------+\n",
      "|       0|14518154|\n",
      "|       1|  581596|\n",
      "+--------+--------+\n",
      "\n",
      "\n",
      "Distribution ratios (Small:Heavy):\n",
      "  Train: 5.0:1 (sampled for balanced training)\n",
      "  Val:   48.2:1 (original for realistic evaluation)\n",
      "  Test:  25.0:1 (original for realistic evaluation)\n",
      "\n",
      "IMPORTANT: Val and test metrics will reflect production distribution (~36:1)"
     ]
    }
   ],
   "source": [
    "# Load pre-split datasets from notebook 01\n",
    "processed_path = config['data_loading']['processed_output_path']\n",
    "date_range = f\"{config['data_loading']['start_date']}_to_{config['data_loading']['end_date']}\"\n",
    "base_path = f\"{processed_path}/{date_range}\"\n",
    "\n",
    "train_path = f\"{base_path}/train_sampled\"  # 5:1 sampled for training\n",
    "val_path = f\"{base_path}/val_original\"      # ~36:1 original distribution\n",
    "test_path = f\"{base_path}/test_original\"    # ~36:1 original distribution\n",
    "\n",
    "print(f\"Loading pre-split datasets from notebook 01...\")\n",
    "print(f\"  Base path: {base_path}\")\n",
    "print(f\"\\n  Train (sampled):  {train_path}\")\n",
    "print(f\"  Val (original):   {val_path}\")\n",
    "print(f\"  Test (original):  {test_path}\")\n",
    "\n",
    "# Load splits\n",
    "train_df = spark.read.parquet(train_path)\n",
    "val_df = spark.read.parquet(val_path)\n",
    "test_df = spark.read.parquet(test_path)\n",
    "\n",
    "# Get counts\n",
    "train_count = train_df.count()\n",
    "val_count = val_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"\\nDatasets loaded:\")\n",
    "print(f\"  Train: {train_count:,} queries\")\n",
    "print(f\"  Val:   {val_count:,} queries\")\n",
    "print(f\"  Test:  {test_count:,} queries\")\n",
    "\n",
    "# Show class distributions\n",
    "print(f\"\\nClass distributions:\")\n",
    "print(\"Train (sampled):\")\n",
    "train_df.groupBy('is_heavy').count().orderBy('is_heavy').show()\n",
    "\n",
    "print(\"Val (original):\")\n",
    "val_df.groupBy('is_heavy').count().orderBy('is_heavy').show()\n",
    "\n",
    "print(\"Test (original):\")\n",
    "test_df.groupBy('is_heavy').count().orderBy('is_heavy').show()\n",
    "\n",
    "# Calculate ratios for reporting\n",
    "train_heavy = train_df.filter(F.col('is_heavy') == 1).count()\n",
    "train_ratio = (train_count - train_heavy) / train_heavy if train_heavy > 0 else 0\n",
    "\n",
    "val_heavy = val_df.filter(F.col('is_heavy') == 1).count()\n",
    "val_ratio = (val_count - val_heavy) / val_heavy if val_heavy > 0 else 0\n",
    "\n",
    "test_heavy = test_df.filter(F.col('is_heavy') == 1).count()\n",
    "test_ratio = (test_count - test_heavy) / test_heavy if test_heavy > 0 else 0\n",
    "\n",
    "print(f\"\\nDistribution ratios (Small:Heavy):\")\n",
    "print(f\"  Train: {train_ratio:.1f}:1 (sampled for balanced training)\")\n",
    "print(f\"  Val:   {val_ratio:.1f}:1 (original for realistic evaluation)\")\n",
    "print(f\"  Test:  {test_ratio:.1f}:1 (original for realistic evaluation)\")\n",
    "\n",
    "print(\"\\nIMPORTANT: Val and test metrics will reflect production distribution (~36:1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Base Features (78 features)\n",
    "\n",
    "Use production `FeatureExtractor` to extract 78 base features.\n",
    "This ensures train-serve parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:38:43.259708Z",
     "iopub.status.busy": "2025-10-16T22:38:43.259519Z",
     "iopub.status.idle": "2025-10-16T22:38:43.538076Z",
     "shell.execute_reply": "2025-10-16T22:38:43.537469Z",
     "shell.execute_reply.started": "2025-10-16T22:38:43.259684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5156c76a2dfe4a77866f13a5e391cb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor initialized\n",
      "  Base features: 78\n",
      "\n",
      "Extracting base features for all splits...\n",
      "\n",
      "[1/3] Extracting train base features...\n",
      "[2/3] Extracting val base features...\n",
      "[3/3] Extracting test base features...\n",
      "Base features extracted for all splits"
     ]
    }
   ],
   "source": [
    "# Initialize production feature extractor\n",
    "base_extractor = FeatureExtractor(config)\n",
    "\n",
    "print(f\"FeatureExtractor initialized\")\n",
    "print(f\"  Base features: {base_extractor.feature_count}\")\n",
    "\n",
    "# Create Spark UDF for distributed extraction\n",
    "base_udf = base_extractor.create_spark_udf()\n",
    "\n",
    "print(\"\\nExtracting base features for all splits...\")\n",
    "\n",
    "# Extract for train\n",
    "print(\"\\n[1/3] Extracting train base features...\")\n",
    "train_base = train_df.withColumn(\n",
    "    'base_features',\n",
    "    base_udf(\n",
    "        F.struct(\n",
    "            F.col('query'),\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema'),\n",
    "            F.col('hour'),\n",
    "            F.col('clientInfo')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# train_base = checkpoint_mgr.checkpoint(train_base, \"03_train_base\")\n",
    "\n",
    "# Extract for val\n",
    "print(\"[2/3] Extracting val base features...\")\n",
    "val_base = val_df.withColumn(\n",
    "    'base_features',\n",
    "    base_udf(\n",
    "        F.struct(\n",
    "            F.col('query'),\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema'),\n",
    "            F.col('hour'),\n",
    "            F.col('clientInfo')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# val_base = checkpoint_mgr.checkpoint(val_base, \"03_val_base\")\n",
    "\n",
    "# Extract for test\n",
    "print(\"[3/3] Extracting test base features...\")\n",
    "test_base = test_df.withColumn(\n",
    "    'base_features',\n",
    "    base_udf(\n",
    "        F.struct(\n",
    "            F.col('query'),\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema'),\n",
    "            F.col('hour'),\n",
    "            F.col('clientInfo')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# test_base = checkpoint_mgr.checkpoint(test_base, \"03_test_base\")\n",
    "\n",
    "print(\"Base features extracted for all splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Historical Features (17 features)\n",
    "\n",
    "Historical features provide cold-start handling:\n",
    "- User historical stats (6 features)\n",
    "- Catalog historical stats (6 features)\n",
    "- Schema historical stats (4 features)\n",
    "- Cold-start indicator (1 feature)\n",
    "\n",
    "Statistics are computed from training data only to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:38:48.744403Z",
     "iopub.status.busy": "2025-10-16T22:38:48.744093Z",
     "iopub.status.idle": "2025-10-16T22:39:36.540369Z",
     "shell.execute_reply": "2025-10-16T22:39:36.539868Z",
     "shell.execute_reply.started": "2025-10-16T22:38:48.744373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20390170a854836af495360d64d38cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing historical statistics from training data...\n",
      "\n",
      "Historical stats computed:\n",
      "  Users: 16,181\n",
      "  Catalogs: 36\n",
      "  Schemas: 443\n",
      "  Overall heavy rate: 16.67%\n",
      "\n",
      "HistoricalFeatureExtractor initialized\n",
      "  Historical features: 17\n",
      "\n",
      "Extracting historical features for all splits...\n",
      "[1/3] Extracting train historical features...\n",
      "[2/3] Extracting val historical features...\n",
      "[3/3] Extracting test historical features...\n",
      "Historical features extracted for all splits"
     ]
    }
   ],
   "source": [
    "from query_predictor.training.historical_stats_computer import HistoricalStatsComputer\n",
    "from query_predictor.core.featurizer.extractors.historical_extractor import HistoricalFeatureExtractor\n",
    "\n",
    "print(\"Computing historical statistics from training data...\")\n",
    "\n",
    "# Initialize stats computer\n",
    "stats_computer = HistoricalStatsComputer(version='1.0.0')\n",
    "\n",
    "# Compute stats from training data\n",
    "date_range = {\n",
    "    'start': config['data_loading']['start_date'],\n",
    "    'end': config['data_loading']['end_date']\n",
    "}\n",
    "stats_schema = stats_computer.compute(train_df, date_range)\n",
    "\n",
    "print(f\"\\nHistorical stats computed:\")\n",
    "print(f\"  Users: {len(stats_schema.users):,}\")\n",
    "print(f\"  Catalogs: {len(stats_schema.catalogs):,}\")\n",
    "print(f\"  Schemas: {len(stats_schema.schemas):,}\")\n",
    "print(f\"  Overall heavy rate: {stats_schema.heavy_rate_overall:.2%}\")\n",
    "\n",
    "# Serialize to dict for HistoricalFeatureExtractor\n",
    "stats_dict = stats_schema.to_dict()\n",
    "\n",
    "# Initialize historical feature extractor with computed stats\n",
    "historical_extractor = HistoricalFeatureExtractor(\n",
    "    config={},\n",
    "    historical_stats=stats_dict\n",
    ")\n",
    "\n",
    "print(f\"\\nHistoricalFeatureExtractor initialized\")\n",
    "print(f\"  Historical features: {historical_extractor.feature_count}\")\n",
    "\n",
    "# Create Spark UDF for distributed extraction\n",
    "historical_udf = historical_extractor.create_spark_udf()\n",
    "\n",
    "print(\"\\nExtracting historical features for all splits...\")\n",
    "\n",
    "# Extract for train\n",
    "print(\"[1/3] Extracting train historical features...\")\n",
    "train_hist = train_base.withColumn(\n",
    "    'historical_features',\n",
    "    historical_udf(\n",
    "        F.struct(\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# train_hist = checkpoint_mgr.checkpoint(train_hist, \"03_train_hist\")\n",
    "\n",
    "\n",
    "# Extract for val\n",
    "print(\"[2/3] Extracting val historical features...\")\n",
    "val_hist = val_base.withColumn(\n",
    "    'historical_features',\n",
    "    historical_udf(\n",
    "        F.struct(\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# val_hist = checkpoint_mgr.checkpoint(val_hist, \"03_val_hist\")\n",
    "\n",
    "\n",
    "# Extract for test\n",
    "print(\"[3/3] Extracting test historical features...\")\n",
    "test_hist = test_base.withColumn(\n",
    "    'historical_features',\n",
    "    historical_udf(\n",
    "        F.struct(\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# test_hist = checkpoint_mgr.checkpoint(test_hist, \"03_test_hist\")\n",
    "\n",
    "print(\"Historical features extracted for all splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build TF-IDF Vocabulary (TRAINING DATA ONLY)\n",
    "\n",
    "**CRITICAL**: TF-IDF vocabulary is built ONLY on training queries to prevent data leakage.\n",
    "\n",
    "**Approach**: Uses Spark ML (CountVectorizer + IDF) for fully distributed processing without requiring data collection to driver. This avoids memory limits and scales to any dataset size.\n",
    "\n",
    "The fitted pipeline extracts vocabulary and IDF weights for sklearn-compatible inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:39:36.541157Z",
     "iopub.status.busy": "2025-10-16T22:39:36.541043Z",
     "iopub.status.idle": "2025-10-16T22:41:14.784326Z",
     "shell.execute_reply": "2025-10-16T22:41:14.783789Z",
     "shell.execute_reply.started": "2025-10-16T22:39:36.541143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23990e7f03cd4fd48e3803481ab5dd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CRITICAL: Building TF-IDF vocabulary on TRAINING DATA ONLY\n",
      "This prevents data leakage into val/test sets.\n",
      "\n",
      "Fitting TF-IDF vocabulary (distributed - no data collection to driver)...\n",
      "This may take 5-10 minutes...\n",
      "\n",
      "TF-IDF vocabulary built successfully\n",
      "  Vocabulary size: 250\n",
      "  Min DF: 100\n",
      "  Max DF: 0.8\n",
      "  Method: spark_ml_countvectorizer_optimized\n",
      "\n",
      "Sample vocabulary (first 20 terms):\n",
      "['string_literal', 'org_id', 'numeric', 'company_id', 'timestamp', 'cast', 'varchar', 'execute', 'double', 'using', 'statement1', 'sum', 'max', 'approx_percentile', 'cell', 'ts', '_time', 'coalesce', 'metric_name', 'false']\n",
      "/mnt3/yarn/usercache/pmannem/appcache/application_1758752217644_211576/container_1758752217644_211576_01_000001/environment/lib64/python3.11/site-packages/sklearn/feature_extraction/text.py:1675: RuntimeWarning: divide by zero encountered in divide\n",
      "  self.idf_ /= df"
     ]
    }
   ],
   "source": [
    "# Initialize Spark ML TF-IDF pipeline\n",
    "tfidf_config = {\n",
    "    'tfidf_vocab_size': config['features']['tfidf_vocab_size'],\n",
    "    'min_df': config['features']['min_df'],\n",
    "    'max_df': config['features']['max_df']\n",
    "}\n",
    "\n",
    "tfidf_pipeline = SparkMLTfidfPipeline(tfidf_config)\n",
    "\n",
    "print(\"\\nCRITICAL: Building TF-IDF vocabulary on TRAINING DATA ONLY\")\n",
    "print(\"This prevents data leakage into val/test sets.\\n\")\n",
    "\n",
    "print(\"Fitting TF-IDF vocabulary (distributed - no data collection to driver)...\")\n",
    "print(\"This may take 5-10 minutes...\")\n",
    "\n",
    "# Fit on DataFrame directly (NO COLLECT!)\n",
    "tfidf_pipeline.fit_on_dataframe(train_hist, query_column='query')\n",
    "\n",
    "print(f\"\\nTF-IDF vocabulary built successfully\")\n",
    "metadata = tfidf_pipeline.get_feature_metadata()\n",
    "print(f\"  Vocabulary size: {metadata['vocab_size']:,}\")\n",
    "print(f\"  Min DF: {metadata['min_df']}\")\n",
    "print(f\"  Max DF: {metadata['max_df']}\")\n",
    "print(f\"  Method: {metadata['method']}\")\n",
    "\n",
    "# Display sample terms for inspection\n",
    "print(f\"\\nSample vocabulary (first 20 terms):\")\n",
    "sample_terms = [name.replace('tfidf_', '') for name in metadata['feature_names'][:20]]\n",
    "print(sample_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extract TF-IDF Features\n",
    "\n",
    "Transform queries to TF-IDF features using the fitted vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:41:14.784999Z",
     "iopub.status.busy": "2025-10-16T22:41:14.784874Z",
     "iopub.status.idle": "2025-10-16T22:41:15.064545Z",
     "shell.execute_reply": "2025-10-16T22:41:15.064017Z",
     "shell.execute_reply.started": "2025-10-16T22:41:14.784985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d004d1364f34fb7bd19505d48886bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting TF-IDF features for all splits...\n",
      "This may take 10-15 minutes...\n",
      "\n",
      "[1/3] Extracting train TF-IDF features...\n",
      "[2/3] Extracting val TF-IDF features...\n",
      "[3/3] Extracting test TF-IDF features...\n",
      "\n",
      "? TF-IDF features extracted for all splits"
     ]
    }
   ],
   "source": [
    "# Create Spark UDF from fitted pipeline\n",
    "tfidf_udf = tfidf_pipeline.create_spark_udf()\n",
    "\n",
    "print(\"Extracting TF-IDF features for all splits...\")\n",
    "print(\"This may take 10-15 minutes...\")\n",
    "\n",
    "# Extract for train\n",
    "print(\"\\n[1/3] Extracting train TF-IDF features...\")\n",
    "train_tfidf = train_hist.withColumn('tfidf_features', tfidf_udf(F.col('query')))\n",
    "# train_tfidf = checkpoint_mgr.checkpoint(train_tfidf, \"03_train_tfidf\")\n",
    "\n",
    "# Extract for val\n",
    "print(\"[2/3] Extracting val TF-IDF features...\")\n",
    "val_tfidf = val_hist.withColumn('tfidf_features', tfidf_udf(F.col('query')))\n",
    "# val_tfidf = checkpoint_mgr.checkpoint(val_tfidf, \"03_val_tfidf\")\n",
    "\n",
    "# Extract for test\n",
    "print(\"[3/3] Extracting test TF-IDF features...\")\n",
    "test_tfidf = test_hist.withColumn('tfidf_features', tfidf_udf(F.col('query')))\n",
    "# test_tfidf = checkpoint_mgr.checkpoint(test_tfidf, \"03_test_tfidf\")\n",
    "\n",
    "print(\"\\n✅ TF-IDF features extracted for all splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Combine Features\n",
    "\n",
    "Concatenate all features: base (78) + historical (17) + TF-IDF (1000) = 1095 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:41:15.065181Z",
     "iopub.status.busy": "2025-10-16T22:41:15.065065Z",
     "iopub.status.idle": "2025-10-16T22:41:15.344555Z",
     "shell.execute_reply": "2025-10-16T22:41:15.344021Z",
     "shell.execute_reply.started": "2025-10-16T22:41:15.065166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dd9a4baeed44cda91f253bbc4569a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining features...\n",
      "\n",
      "Features combined\n",
      "  Expected dimensions: 345\n",
      "  Base: 78\n",
      "  Historical: 17\n",
      "  TF-IDF: 250"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "@udf(returnType=ArrayType(FloatType()))\n",
    "def combine_features(base, historical, tfidf):\n",
    "    \"\"\"Concatenate base + historical + tfidf features.\"\"\"\n",
    "    if base is None or historical is None or tfidf is None:\n",
    "        return None\n",
    "    return base + historical + tfidf\n",
    "\n",
    "print(\"Combining features...\")\n",
    "\n",
    "# Combine for train\n",
    "train_final = train_tfidf.withColumn(\n",
    "    'features',\n",
    "    combine_features(\n",
    "        F.col('base_features'),\n",
    "        F.col('historical_features'),\n",
    "        F.col('tfidf_features')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine for val\n",
    "val_final = val_tfidf.withColumn(\n",
    "    'features',\n",
    "    combine_features(\n",
    "        F.col('base_features'),\n",
    "        F.col('historical_features'),\n",
    "        F.col('tfidf_features')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine for test\n",
    "test_final = test_tfidf.withColumn(\n",
    "    'features',\n",
    "    combine_features(\n",
    "        F.col('base_features'),\n",
    "        F.col('historical_features'),\n",
    "        F.col('tfidf_features')\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nFeatures combined\")\n",
    "print(f\"  Expected dimensions: {config['features']['total_features']}\")\n",
    "print(f\"  Base: {config['features']['base_feature_count']}\")\n",
    "print(f\"  Historical: {config['features']['historical_feature_count']}\")\n",
    "print(f\"  TF-IDF: {config['features']['tfidf_vocab_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:41:15.345222Z",
     "iopub.status.busy": "2025-10-16T22:41:15.345111Z",
     "iopub.status.idle": "2025-10-16T22:41:17.682085Z",
     "shell.execute_reply": "2025-10-16T22:41:17.681404Z",
     "shell.execute_reply.started": "2025-10-16T22:41:15.345207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c2f1cbe80942d1965927547eb4dfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating feature dimensions...\n",
      "\n",
      "? Feature Dimensions:\n",
      "  Train: 345\n",
      "  Val:   345\n",
      "  Test:  345\n",
      "  Expected: 345\n",
      "\n",
      "? All dimensions validated"
     ]
    }
   ],
   "source": [
    "# Sample and verify dimensions\n",
    "print(\"Validating feature dimensions...\")\n",
    "\n",
    "sample_train = train_final.select('features', 'is_heavy').limit(1).collect()[0]\n",
    "sample_val = val_final.select('features', 'is_heavy').limit(1).collect()[0]\n",
    "sample_test = test_final.select('features', 'is_heavy').limit(1).collect()[0]\n",
    "\n",
    "train_dim = len(sample_train['features'])\n",
    "val_dim = len(sample_val['features'])\n",
    "test_dim = len(sample_test['features'])\n",
    "expected_dim = config['features']['total_features']\n",
    "\n",
    "print(f\"\\n📊 Feature Dimensions:\")\n",
    "print(f\"  Train: {train_dim}\")\n",
    "print(f\"  Val:   {val_dim}\")\n",
    "print(f\"  Test:  {test_dim}\")\n",
    "print(f\"  Expected: {expected_dim}\")\n",
    "\n",
    "assert train_dim == expected_dim, f\"Train dimension mismatch: {train_dim} != {expected_dim}\"\n",
    "assert val_dim == expected_dim, f\"Val dimension mismatch: {val_dim} != {expected_dim}\"\n",
    "assert test_dim == expected_dim, f\"Test dimension mismatch: {test_dim} != {expected_dim}\"\n",
    "\n",
    "print(\"\\n✅ All dimensions validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Validate Feature Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:08:40.292957Z",
     "iopub.status.busy": "2025-10-16T23:08:40.292834Z",
     "iopub.status.idle": "2025-10-16T23:08:42.617475Z",
     "shell.execute_reply": "2025-10-16T23:08:42.617014Z",
     "shell.execute_reply.started": "2025-10-16T23:08:40.292941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f188a122532b4b1a898aeb5d582a1055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE PARITY VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Collecting 100 samples for validation...\n",
      "\n",
      "Running parity validation...\n",
      "  Tolerance: 1e-06\n",
      "  Success threshold: <0.5% mismatch\n",
      "\n",
      "Inference featurizer initialized with historical features enabled\n",
      "  Feature count: 95 (78 base + 17 historical)\n",
      "  TF-IDF vocab size: 250\n",
      "  Expected total: 345\n",
      "\n",
      "\n",
      "======================================================================\n",
      "FEATURE PARITY VALIDATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Status: ❌ FAILED\n",
      "\n",
      "Summary:\n",
      "  Samples Tested:  100\n",
      "  Mismatches:      100\n",
      "  Mismatch Rate:   100.00%\n",
      "  Max Difference:  1.000000000\n",
      "  Tolerance:       0.000001000\n",
      "\n",
      "Mismatch Details (first 10):\n",
      "\n",
      "  Sample 0:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 9\n",
      "    Feature indices: [45, 46, 47, 54, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 1:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 8\n",
      "    Feature indices: [45, 47, 54, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 2:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 8\n",
      "    Feature indices: [45, 47, 54, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 3:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 8\n",
      "    Feature indices: [45, 47, 54, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 4:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 8\n",
      "    Feature indices: [45, 47, 54, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 5:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 13\n",
      "    Feature indices: [45, 46, 47, 48, 54, 95, 97, 99, 125, 149]\n",
      "\n",
      "  Sample 6:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 8\n",
      "    Feature indices: [45, 47, 54, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 7:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 13\n",
      "    Feature indices: [45, 46, 47, 48, 54, 95, 127, 128, 166, 181]\n",
      "\n",
      "  Sample 8:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 15\n",
      "    Feature indices: [45, 46, 47, 48, 54, 95, 97, 118, 134, 139]\n",
      "\n",
      "  Sample 9:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 9\n",
      "    Feature indices: [45, 46, 47, 54, 95, 99, 102, 104, 163]\n",
      "\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE PARITY VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize validator from config\n",
    "validation_config = config.get('validation', {})\n",
    "n_samples = validation_config.get('parity_samples', 100)\n",
    "validator = ParityValidator(config=config)\n",
    "\n",
    "# Collect sample of training features and queries\n",
    "print(f\"\\nCollecting {n_samples} samples for validation...\")\n",
    "train_samples = train_final.select(\n",
    "    'features', 'query', 'user', 'catalog', 'schema', 'hour', 'clientInfo', 'is_heavy'\n",
    ").limit(n_samples).collect()\n",
    "\n",
    "# Convert to numpy arrays\n",
    "training_features = np.array([row['features'] for row in train_samples], dtype=np.float32)\n",
    "\n",
    "# Prepare sample queries for inference\n",
    "sample_queries = [\n",
    "    {\n",
    "        'query': row['query'],\n",
    "        'user': row['user'],\n",
    "        'catalog': row['catalog'],\n",
    "        'schema': row['schema'],\n",
    "        'hour': row['hour'],\n",
    "        'clientInfo': row['clientInfo']\n",
    "    }\n",
    "    for row in train_samples\n",
    "]\n",
    "\n",
    "print(f\"\\nRunning parity validation...\")\n",
    "print(f\"  Tolerance: {validator.tolerance}\")\n",
    "print(f\"  Success threshold: <{validation_config.get('parity_success_threshold', 0.5)}% mismatch\\n\")\n",
    "\n",
    "# Create inference featurizer with historical features enabled\n",
    "# This matches the production inference architecture where:\n",
    "# - FeatureExtractor returns 95 features (78 base + 17 historical)\n",
    "# - TF-IDF returns vocab_size features\n",
    "# - Total: 95 + vocab_size = expected total\n",
    "inference_config = config.copy()\n",
    "inference_config['enable_historical_features'] = True\n",
    "\n",
    "inference_featurizer = FeatureExtractor(\n",
    "    inference_config,\n",
    "    historical_stats=stats_dict  # Pass historical stats computed from training data\n",
    ")\n",
    "\n",
    "print(f\"Inference featurizer initialized with historical features enabled\")\n",
    "print(f\"  Feature count: {inference_featurizer.feature_count} (78 base + 17 historical)\")\n",
    "print(f\"  TF-IDF vocab size: {tfidf_pipeline.vocab_size}\")\n",
    "print(f\"  Expected total: {inference_featurizer.feature_count + tfidf_pipeline.vocab_size}\\n\")\n",
    "\n",
    "# Run validation\n",
    "parity_result = validator.validate_parity(\n",
    "    training_features=training_features,\n",
    "    inference_featurizer=inference_featurizer,  # Now includes historical features\n",
    "    tfidf_pipeline=tfidf_pipeline,\n",
    "    sample_queries=sample_queries,\n",
    "    n_samples=n_samples\n",
    ")\n",
    "\n",
    "# Generate and print report\n",
    "report = validator.generate_report(parity_result)\n",
    "print(report)\n",
    "\n",
    "# if not parity_result['passed']:\n",
    "#     print(\"\\n⚠️  WARNING: Parity validation failed!\")\n",
    "#     print(\"This indicates train-serve skew. Review feature extraction logic.\")\n",
    "#     raise ValueError(\"Parity validation failed\")\n",
    "\n",
    "# print(\"✅ Parity validation passed - features are consistent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Parity Validation\n",
    "\n",
    "**CRITICAL**: Validate that training features match inference features.\n",
    "\n",
    "**Architecture Note**: \n",
    "- Training extracts base (78) and historical (17) separately for flexibility\n",
    "- Inference uses unified FeatureExtractor with historical enabled (95 total)\n",
    "- Both paths must produce identical features: (78+17) + TF-IDF = total\n",
    "\n",
    "This prevents train-serve skew and silent performance degradation in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:41:18.991640Z",
     "iopub.status.busy": "2025-10-16T22:41:18.991529Z",
     "iopub.status.idle": "2025-10-16T23:08:31.193658Z",
     "shell.execute_reply": "2025-10-16T23:08:31.193178Z",
     "shell.execute_reply.started": "2025-10-16T22:41:18.991625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291df4e9a7ff4d158e8af5ce1dbb6ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving feature datasets to S3...\n",
      "  Base path: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01\n",
      "\n",
      "[1/3] Saving train dataset...\n",
      "  ? Train saved: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01/train\n",
      "[2/3] Saving val dataset...\n",
      "  ? Val saved: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01/val\n",
      "[3/3] Saving test dataset...\n",
      "  ? Test saved: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01/test\n",
      "\n",
      "? All feature datasets saved to S3"
     ]
    }
   ],
   "source": [
    "# Define output paths\n",
    "features_path = config['features']['output_path']\n",
    "date_range = f\"{config['data_loading']['start_date']}_to_{config['data_loading']['end_date']}\"\n",
    "output_base = f\"{features_path}/{date_range}\"\n",
    "\n",
    "train_path = f\"{output_base}/train\"\n",
    "val_path = f\"{output_base}/val\"\n",
    "test_path = f\"{output_base}/test\"\n",
    "\n",
    "print(f\"Saving feature datasets to S3...\")\n",
    "print(f\"  Base path: {output_base}\")\n",
    "\n",
    "# Select relevant columns\n",
    "output_columns = [\n",
    "    'queryId',\n",
    "    'query',\n",
    "    'user',\n",
    "    'catalog',\n",
    "    'schema',\n",
    "    'queryDate',\n",
    "    'hour',\n",
    "    'is_heavy',\n",
    "    'cpu_time_seconds',\n",
    "    'memory_gb',\n",
    "    'features'  # Combined features array\n",
    "]\n",
    "\n",
    "# Save train\n",
    "print(\"\\n[1/3] Saving train dataset...\")\n",
    "train_final.select(output_columns).write.mode('overwrite').parquet(train_path)\n",
    "print(f\"  ✅ Train saved: {train_path}\")\n",
    "\n",
    "# Save val\n",
    "print(\"[2/3] Saving val dataset...\")\n",
    "val_final.select(output_columns).write.mode('overwrite').parquet(val_path)\n",
    "print(f\"  ✅ Val saved: {val_path}\")\n",
    "\n",
    "# Save test\n",
    "print(\"[3/3] Saving test dataset...\")\n",
    "test_final.select(output_columns).write.mode('overwrite').parquet(test_path)\n",
    "print(f\"  ✅ Test saved: {test_path}\")\n",
    "\n",
    "print(\"\\n✅ All feature datasets saved to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Feature Datasets to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:08:31.194319Z",
     "iopub.status.busy": "2025-10-16T23:08:31.194193Z",
     "iopub.status.idle": "2025-10-16T23:08:31.472331Z",
     "shell.execute_reply": "2025-10-16T23:08:31.471842Z",
     "shell.execute_reply.started": "2025-10-16T23:08:31.194298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1daaeab11546eeb246c1baf98eb805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TF-IDF vectorizer...\n",
      "  Saved locally: /tmp/tmpl19lufid.pkl\n",
      "  ? Uploaded: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/models/tfidf_vectorizer_2025-08-01_to_2025-10-01.pkl\n",
      "  ? Updated latest: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/models/tfidf_vectorizer_latest.pkl"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import tempfile\n",
    "\n",
    "# Save TF-IDF pipeline locally first\n",
    "print(\"Saving TF-IDF vectorizer...\")\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.pkl') as tmp:\n",
    "    tfidf_pipeline.save(tmp.name)\n",
    "    local_tfidf_path = tmp.name\n",
    "    print(f\"  Saved locally: {local_tfidf_path}\")\n",
    "\n",
    "# Upload to S3\n",
    "s3_tfidf_key = f\"{config['s3']['prefix']}/models/tfidf_vectorizer_{date_range}.pkl\"\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(local_tfidf_path, config['s3']['bucket'], s3_tfidf_key)\n",
    "\n",
    "s3_tfidf_path = f\"s3://{config['s3']['bucket']}/{s3_tfidf_key}\"\n",
    "print(f\"  ✅ Uploaded: {s3_tfidf_path}\")\n",
    "\n",
    "# Also save as \"latest\"\n",
    "latest_key = f\"{config['s3']['prefix']}/models/tfidf_vectorizer_latest.pkl\"\n",
    "s3_client.copy_object(\n",
    "    Bucket=config['s3']['bucket'],\n",
    "    CopySource={'Bucket': config['s3']['bucket'], 'Key': s3_tfidf_key},\n",
    "    Key=latest_key\n",
    ")\n",
    "print(f\"  ✅ Updated latest: s3://{config['s3']['bucket']}/{latest_key}\")\n",
    "\n",
    "# Cleanup\n",
    "import os\n",
    "os.unlink(local_tfidf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:08:31.473043Z",
     "iopub.status.busy": "2025-10-16T23:08:31.472868Z",
     "iopub.status.idle": "2025-10-16T23:08:34.831668Z",
     "shell.execute_reply": "2025-10-16T23:08:34.831171Z",
     "shell.execute_reply.started": "2025-10-16T23:08:31.473019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de327ed77a3d49b8a9b1ae1a0cd8842c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? Metadata saved: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/metadata/features_2025-08-01_to_2025-10-01.json"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Prepare metadata\n",
    "metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'date_range': date_range,\n",
    "    'time_splits': config['time_splits'],\n",
    "    'features': {\n",
    "        'base_features': config['features']['base_feature_count'],\n",
    "        'historical_features': config['features']['historical_feature_count'],\n",
    "        'tfidf_features': tfidf_pipeline.vocab_size,\n",
    "        'total_features': config['features']['total_features']\n",
    "    },\n",
    "    'tfidf_config': tfidf_pipeline.get_feature_metadata(),\n",
    "    'dataset_sizes': {\n",
    "        'train': train_final.count(),\n",
    "        'val': val_final.count(),\n",
    "        'test': test_final.count()\n",
    "    },\n",
    "    'parity_validation': parity_result,\n",
    "    's3_paths': {\n",
    "        'train': train_path,\n",
    "        'val': val_path,\n",
    "        'test': test_path,\n",
    "        'tfidf_vectorizer': s3_tfidf_path\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save locally\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as tmp:\n",
    "    json.dump(metadata, tmp, indent=2)\n",
    "    local_metadata_path = tmp.name\n",
    "\n",
    "# Upload to S3\n",
    "metadata_key = f\"{config['s3']['prefix']}/metadata/features_{date_range}.json\"\n",
    "s3_client.upload_file(local_metadata_path, config['s3']['bucket'], metadata_key)\n",
    "\n",
    "print(f\"✅ Metadata saved: s3://{config['s3']['bucket']}/{metadata_key}\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(local_metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:08:34.832346Z",
     "iopub.status.busy": "2025-10-16T23:08:34.832213Z",
     "iopub.status.idle": "2025-10-16T23:08:34.887062Z",
     "shell.execute_reply": "2025-10-16T23:08:34.886631Z",
     "shell.execute_reply.started": "2025-10-16T23:08:34.832329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332a2b0d76ce4bb7a576883e02436ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Feature Breakdown:\n",
      "  Base features:       78\n",
      "  Historical features: 17\n",
      "  TF-IDF features:     250\n",
      "  ----------------------------------------\n",
      "  Total features:      345\n",
      "\n",
      "Dataset Sizes:\n",
      "  Train: 8,782,474 queries\n",
      "  Val:   14,969,526 queries\n",
      "  Test:  15,099,750 queries\n",
      "\n",
      "Class Distributions:\n",
      "  Train: 5.0:1 (sampled for balanced training)\n",
      "  Val:   48.2:1 (original - production distribution)\n",
      "  Test:  25.0:1 (original - production distribution)\n",
      "\n",
      "Parity Validation:\n",
      "  Status: FAILED\n",
      "  Mismatch rate: 100.00%\n",
      "\n",
      "S3 Outputs:\n",
      "  Train data:      s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01/train\n",
      "  Val data:        s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01/val\n",
      "  Test data:       s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01/test\n",
      "  TF-IDF pipeline: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/models/tfidf_vectorizer_2025-08-01_to_2025-10-01.pkl\n",
      "  Metadata:        s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/metadata/features_2025-08-01_to_2025-10-01.json\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Next Steps:\n",
      "1. Open notebook 04_model_training_distributed.ipynb\n",
      "2. Train XGBoost model with extracted features\n",
      "3. Export to ONNX for production inference\n",
      "\n",
      "IMPORTANT: Expect different metrics on val/test vs train:\n",
      "  - Val/Test use ~36:1 original distribution (realistic)\n",
      "  - Precision will be LOWER (~20-25% vs ~60-70% on balanced)\n",
      "  - This is EXPECTED and reflects production performance\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nFeature Breakdown:\")\n",
    "print(f\"  Base features:       {config['features']['base_feature_count']}\")\n",
    "print(f\"  Historical features: {config['features']['historical_feature_count']}\")\n",
    "print(f\"  TF-IDF features:     {tfidf_pipeline.vocab_size}\")\n",
    "print(f\"  {'-' * 40}\")\n",
    "print(f\"  Total features:      {config['features']['total_features']}\")\n",
    "\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Train: {metadata['dataset_sizes']['train']:,} queries\")\n",
    "print(f\"  Val:   {metadata['dataset_sizes']['val']:,} queries\")\n",
    "print(f\"  Test:  {metadata['dataset_sizes']['test']:,} queries\")\n",
    "\n",
    "print(f\"\\nClass Distributions:\")\n",
    "print(f\"  Train: {train_ratio:.1f}:1 (sampled for balanced training)\")\n",
    "print(f\"  Val:   {val_ratio:.1f}:1 (original - production distribution)\")\n",
    "print(f\"  Test:  {test_ratio:.1f}:1 (original - production distribution)\")\n",
    "\n",
    "print(f\"\\nParity Validation:\")\n",
    "if parity_result['passed']:\n",
    "    print(f\"  Status: PASSED\")\n",
    "    print(f\"  Mismatch rate: {parity_result['mismatch_rate']:.2f}%\")\n",
    "    print(f\"  Max difference: {parity_result['max_difference']:.9f}\")\n",
    "else:\n",
    "    print(f\"  Status: FAILED\")\n",
    "    print(f\"  Mismatch rate: {parity_result['mismatch_rate']:.2f}%\")\n",
    "\n",
    "print(f\"\\nS3 Outputs:\")\n",
    "print(f\"  Train data:      {train_path}\")\n",
    "print(f\"  Val data:        {val_path}\")\n",
    "print(f\"  Test data:       {test_path}\")\n",
    "print(f\"  TF-IDF pipeline: {s3_tfidf_path}\")\n",
    "print(f\"  Metadata:        s3://{config['s3']['bucket']}/{metadata_key}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Open notebook 04_model_training_distributed.ipynb\")\n",
    "print(\"2. Train XGBoost model with extracted features\")\n",
    "print(\"3. Export to ONNX for production inference\")\n",
    "\n",
    "print(\"\\nIMPORTANT: Expect different metrics on val/test vs train:\")\n",
    "print(\"  - Val/Test use ~36:1 original distribution (realistic)\")\n",
    "print(\"  - Precision will be LOWER (~20-25% vs ~60-70% on balanced)\")\n",
    "print(\"  - This is EXPECTED and reflects production performance\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T23:08:34.887655Z",
     "iopub.status.busy": "2025-10-16T23:08:34.887545Z",
     "iopub.status.idle": "2025-10-16T23:08:40.292270Z",
     "shell.execute_reply": "2025-10-16T23:08:40.291757Z",
     "shell.execute_reply.started": "2025-10-16T23:08:34.887642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a570248913dd4272abf01099de367c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/metadata/features_2025-08-01_to_2025-10-01.json"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Prepare metadata\n",
    "metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'date_range': date_range,\n",
    "    'time_splits': config['time_splits'],\n",
    "    'features': {\n",
    "        'base_features': config['features']['base_feature_count'],\n",
    "        'historical_features': config['features']['historical_feature_count'],\n",
    "        'tfidf_features': tfidf_pipeline.vocab_size,\n",
    "        'total_features': config['features']['total_features']\n",
    "    },\n",
    "    'tfidf_config': tfidf_pipeline.get_feature_metadata(),\n",
    "    'dataset_sizes': {\n",
    "        'train': train_final.count(),\n",
    "        'val': val_final.count(),\n",
    "        'test': test_final.count()\n",
    "    },\n",
    "    'class_distributions': {\n",
    "        'train': {\n",
    "            'ratio': f'{train_ratio:.1f}:1',\n",
    "            'heavy_count': int(train_heavy),\n",
    "            'note': 'Sampled for balanced training'\n",
    "        },\n",
    "        'val': {\n",
    "            'ratio': f'{val_ratio:.1f}:1',\n",
    "            'heavy_count': int(val_heavy),\n",
    "            'note': 'Original production distribution'\n",
    "        },\n",
    "        'test': {\n",
    "            'ratio': f'{test_ratio:.1f}:1',\n",
    "            'heavy_count': int(test_heavy),\n",
    "            'note': 'Original production distribution'\n",
    "        }\n",
    "    },\n",
    "    'parity_validation': parity_result,\n",
    "    's3_paths': {\n",
    "        'train': train_path,\n",
    "        'val': val_path,\n",
    "        'test': test_path,\n",
    "        'tfidf_vectorizer': s3_tfidf_path\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save locally\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as tmp:\n",
    "    json.dump(metadata, tmp, indent=2)\n",
    "    local_metadata_path = tmp.name\n",
    "\n",
    "# Upload to S3\n",
    "metadata_key = f\"{config['s3']['prefix']}/metadata/features_{date_range}.json\"\n",
    "s3_client.upload_file(local_metadata_path, config['s3']['bucket'], metadata_key)\n",
    "\n",
    "print(f\"Metadata saved: s3://{config['s3']['bucket']}/{metadata_key}\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(local_metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
