{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Data Loading Pipeline\n",
    "\n",
    "**Purpose**: Load → Label → Filter → Split → Sample Train Only → Save\n",
    "\n",
    "**Pipeline Stages**:\n",
    "1. Load raw Trino query logs from S3\n",
    "2. Apply labeling (CPU/memory/error thresholds)\n",
    "3. Apply data quality filters\n",
    "4. Create time-based splits (train/val/test)\n",
    "5. Apply boundary-focused sampling ONLY to training data\n",
    "6. Save to separate paths (train_sampled, val_original, test_original)\n",
    "\n",
    "**Key Features**:\n",
    "- Continuous distance-based boundary sampling (POC v2.1.0)\n",
    "- S3 checkpointing for fault tolerance\n",
    "- Optional analysis at each stage\n",
    "- Target 5:1 small:heavy ratio for TRAINING only\n",
    "- Val/test kept at original ~36:1 distribution for realistic evaluation\n",
    "\n",
    "**CRITICAL**: This notebook samples ONLY training data. Validation and test sets\n",
    "maintain their original distribution (~36:1) to provide realistic performance metrics\n",
    "that reflect production conditions. This prevents overoptimistic evaluation results.\n",
    "\n",
    "**Prerequisites**:\n",
    "- Completed notebook 00 (setup)\n",
    "- Spark configuration copied from notebook 00\n",
    "\n",
    "**Duration**: ~30-45 minutes for full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Spark Session\n",
    "\n",
    "**PASTE THE SPARK CONFIGURATION FROM NOTEBOOK 00 HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:19:46.960541Z",
     "iopub.status.busy": "2025-10-16T21:19:46.960421Z",
     "iopub.status.idle": "2025-10-16T21:19:48.275092Z",
     "shell.execute_reply": "2025-10-16T21:19:48.274566Z",
     "shell.execute_reply.started": "2025-10-16T21:19:46.960524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': './environment/bin/python', 'spark.yarn.dist.archives': 's3://uip-datalake-bucket-prod/uipexplore/shared/python311common_env.tar.gz#environment', 'spark.submit.deployMode': 'cluster', 'spark.driver.maxResultSize': '8G', 'spark.dynamicAllocation.enabled': 'true', 'spark.dynamicAllocation.minExecutors': '5', 'spark.dynamicAllocation.maxExecutors': '20'}, 'driverMemory': '16G', 'executorCores': 5, 'executorMemory': '20G', 'pyFiles': ['s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip', 's3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py'], 'driverCores': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3127</td><td>application_1758752217644_207850</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_207850/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-38-4.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_207850_01_000002/sahith.kondepudi\">Link</a></td><td>sahith.kondepudi</td><td></td></tr><tr><td>3144</td><td>application_1758752217644_208422</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_208422/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-133.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_208422_01_000002/w.scroggins\">Link</a></td><td>w.scroggins</td><td></td></tr><tr><td>3159</td><td>application_1758752217644_209535</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209535/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-33-17.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209535_01_000001/jj.chen\">Link</a></td><td>jj.chen</td><td></td></tr><tr><td>3161</td><td>application_1758752217644_209559</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209559/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-32-8.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209559_01_000002/a.wadhwa\">Link</a></td><td>a.wadhwa</td><td></td></tr><tr><td>3162</td><td>application_1758752217644_209563</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209563/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-35-229.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209563_01_000013/feifan.jian\">Link</a></td><td>feifan.jian</td><td></td></tr><tr><td>3166</td><td>application_1758752217644_209727</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209727/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-35-229.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209727_01_000003/nmehrdad\">Link</a></td><td>nmehrdad</td><td></td></tr><tr><td>3167</td><td>application_1758752217644_209744</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209744/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-191.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209744_01_000003/c.grey\">Link</a></td><td>c.grey</td><td></td></tr><tr><td>3170</td><td>application_1758752217644_209770</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209770/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-33-255.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209770_01_000002/clyu\">Link</a></td><td>clyu</td><td></td></tr><tr><td>3171</td><td>application_1758752217644_209784</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_209784/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-133.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_209784_01_000001/s.mylavarapu\">Link</a></td><td>s.mylavarapu</td><td></td></tr><tr><td>3179</td><td>application_1758752217644_210005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_210005/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-133.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_210005_01_000001/pmannem\">Link</a></td><td>pmannem</td><td></td></tr><tr><td>3180</td><td>application_1758752217644_210099</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_210099/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-32-5.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_210099_01_000004/ssardeshpande\">Link</a></td><td>ssardeshpande</td><td></td></tr><tr><td>3193</td><td>application_1758752217644_210699</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_210699/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-32-8.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_210699_01_000003/pmannem\">Link</a></td><td>pmannem</td><td></td></tr><tr><td>3194</td><td>application_1758752217644_210874</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_210874/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-32-5.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_210874_01_000001/a.wadhwa\">Link</a></td><td>a.wadhwa</td><td></td></tr><tr><td>3196</td><td>application_1758752217644_211034</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_211034/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-38-4.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_211034_01_000001/a.wadhwa\">Link</a></td><td>a.wadhwa</td><td></td></tr><tr><td>3197</td><td>application_1758752217644_211049</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_211049/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-33-255.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_211049_01_000001/virender.singh\">Link</a></td><td>virender.singh</td><td></td></tr><tr><td>3199</td><td>application_1758752217644_211142</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_211142/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-114.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_211142_01_000002/virender.singh\">Link</a></td><td>virender.singh</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"pyFiles\": [\n",
    "        \"s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip\",\n",
    "        \"s3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py\"\n",
    "    ],\n",
    "    \"driverMemory\": \"16G\",\n",
    "    \"driverCores\": 4,\n",
    "    \"executorMemory\": \"20G\",\n",
    "    \"executorCores\": 5,\n",
    "    \"conf\": {\n",
    "        \"spark.driver.maxResultSize\": \"8G\",\n",
    "        \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "        \"spark.dynamicAllocation.minExecutors\": \"5\",\n",
    "        \"spark.dynamicAllocation.maxExecutors\": \"20\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:19:48.275675Z",
     "iopub.status.busy": "2025-10-16T21:19:48.275560Z",
     "iopub.status.idle": "2025-10-16T21:21:34.202974Z",
     "shell.execute_reply": "2025-10-16T21:21:34.202505Z",
     "shell.execute_reply.started": "2025-10-16T21:19:48.275660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3201</td><td>application_1758752217644_211229</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_211229/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-199.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_211229_01_000002/pmannem\">Link</a></td><td>pmannem</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6f4c8ce21a4946b1919d204ed6713e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809f781500fc4e36a6494bc412d1f611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies imported\n",
      "Spark version: 3.5.4-amzn-0"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "import logging\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_date, hour as get_hour\n",
    "\n",
    "# Import training modules\n",
    "from query_predictor.training.data_loader import DataLoader\n",
    "from query_predictor.training.checkpoint_manager import CheckpointManager\n",
    "from query_predictor.training.boundary_sampler import BoundarySampler\n",
    "from query_predictor.training.dataframe_analyzer import DataFrameAnalyzer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Dependencies imported\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:51:55.564167Z",
     "iopub.status.busy": "2025-10-16T21:51:55.563958Z",
     "iopub.status.idle": "2025-10-16T21:51:55.860071Z",
     "shell.execute_reply": "2025-10-16T21:51:55.859500Z",
     "shell.execute_reply.started": "2025-10-16T21:51:55.564148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fa60c877a14452aa09ba72f1a5d1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config from S3: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/config/training_config_latest.yaml\n",
      "? Config downloaded to: /tmp/training_config.yaml\n",
      "? Configuration loaded\n",
      "  Date range: 2025-08-01 to 2025-10-01\n",
      "  Target ratio: 5.0:1 (Small:Heavy)\n",
      "  Analysis mode: DISABLED (faster)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "import boto3\n",
    "import yaml\n",
    "\n",
    "# Download training configuration from S3\n",
    "s3_client = boto3.client('s3')\n",
    "s3_bucket = 'uip-datalake-bucket-prod'\n",
    "s3_prefix = 'sf_trino/trino_query_predictor'\n",
    "config_s3_key = f\"{s3_prefix}/config/training_config_latest.yaml\"\n",
    "config_path = '/tmp/training_config.yaml'\n",
    "\n",
    "print(f\"Downloading config from S3: s3://{s3_bucket}/{config_s3_key}\")\n",
    "s3_client.download_file(s3_bucket, config_s3_key, config_path)\n",
    "print(f\"✅ Config downloaded to: {config_path}\")\n",
    "\n",
    "# Load configuration\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"  Date range: {config['data_loading']['start_date']} to {config['data_loading']['end_date']}\")\n",
    "print(f\"  Target ratio: {config['boundary_sampling']['balance_ratio']}:1 (Small:Heavy)\")\n",
    "print(f\"  Analysis mode: {'ENABLED (slower)' if config['analysis']['enabled'] else 'DISABLED (faster)'}\")\n",
    "\n",
    "# OPTIONAL: Override config parameters after loading\n",
    "# Example: Use different input data path\n",
    "# config['data_loading']['s3_base_path'] = 's3://your-bucket/your-path'\n",
    "# Example: Change date range\n",
    "# config['data_loading']['start_date'] = '2025-09-01'\n",
    "# config['data_loading']['end_date'] = '2025-10-01'\n",
    "# Example: Change sampling ratio\n",
    "# config['boundary_sampling']['balance_ratio'] = 3.0\n",
    "# Example: Disable analysis for faster execution\n",
    "# config['analysis']['enabled'] = False\n",
    "# Example: Use sample data for quick testing\n",
    "# config['data_loading']['sample_fraction'] = 0.1  # Use 10% of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:22:30.103900Z",
     "iopub.status.busy": "2025-10-16T21:22:30.103696Z",
     "iopub.status.idle": "2025-10-16T21:22:30.165398Z",
     "shell.execute_reply": "2025-10-16T21:22:30.164942Z",
     "shell.execute_reply.started": "2025-10-16T21:22:30.103881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcf97ee84d04ec8a2b66337c133a5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? Components initialized\n",
      "  Checkpointing: ENABLED\n",
      "  Boundary sampling: ENABLED"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_mgr = CheckpointManager(\n",
    "    spark,\n",
    "    s3_checkpoint_path=config['checkpointing']['s3_path'],\n",
    "    enabled=config['checkpointing']['enabled']\n",
    ")\n",
    "\n",
    "# Initialize data loader\n",
    "loader = DataLoader(spark, config['data_loading'])\n",
    "\n",
    "# Initialize boundary sampler\n",
    "sampler = BoundarySampler(config['boundary_sampling'])\n",
    "\n",
    "# Initialize analyzer (optional)\n",
    "analyzer = DataFrameAnalyzer(spark)\n",
    "\n",
    "print(\"✅ Components initialized\")\n",
    "print(f\"  Checkpointing: {'ENABLED' if config['checkpointing']['enabled'] else 'DISABLED'}\")\n",
    "print(f\"  Boundary sampling: {'ENABLED' if config['boundary_sampling']['enabled'] else 'DISABLED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:22:31.527212Z",
     "iopub.status.busy": "2025-10-16T21:22:31.526985Z",
     "iopub.status.idle": "2025-10-16T21:23:15.287462Z",
     "shell.execute_reply": "2025-10-16T21:23:15.286936Z",
     "shell.execute_reply.started": "2025-10-16T21:22:31.527188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84c76000f2f416090517665a0bd43f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STAGE 1/5] Loading raw data from S3...\n",
      "? Loaded 306,813,447 raw queries\n",
      "   Columns: 52"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(\"[STAGE 1/5] Loading raw data from S3...\")\n",
    "\n",
    "df_raw = loader.load_raw_data(\n",
    "    s3_path=config['data_loading']['s3_base_path'],\n",
    "    start_date=config['data_loading']['start_date'],\n",
    "    end_date=config['data_loading']['end_date'],\n",
    "    sample_fraction=config['data_loading'].get('sample_fraction')\n",
    ")\n",
    "\n",
    "raw_count = df_raw.count()\n",
    "print(f\"✅ Loaded {raw_count:,} raw queries\")\n",
    "print(f\"   Columns: {len(df_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optional Analysis - Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:23:15.288357Z",
     "iopub.status.busy": "2025-10-16T21:23:15.288237Z",
     "iopub.status.idle": "2025-10-16T21:23:15.355341Z",
     "shell.execute_reply": "2025-10-16T21:23:15.354868Z",
     "shell.execute_reply.started": "2025-10-16T21:23:15.288341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9579881a9844f587d4ba2a0304f2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Analysis disabled, skipping raw data analysis"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "if config['analysis']['enabled']:\n",
    "    print(\"[ANALYSIS] Analyzing raw data...\")\n",
    "    stats_raw = analyzer.analyze_dataframe(df_raw, \"Raw Data\")\n",
    "    analyzer.print_analysis_report(stats_raw)\n",
    "else:\n",
    "    print(\"[INFO] Analysis disabled, skipping raw data analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:23:15.355882Z",
     "iopub.status.busy": "2025-10-16T21:23:15.355776Z",
     "iopub.status.idle": "2025-10-16T21:24:21.305057Z",
     "shell.execute_reply": "2025-10-16T21:24:21.304561Z",
     "shell.execute_reply.started": "2025-10-16T21:23:15.355868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba91703674c453987a871a15c799225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STAGE 2/5] Applying labeling...\n",
      "\n",
      "? Labeled 306,813,447 queries\n",
      "   Heavy: 3,462,933 (1.13%)\n",
      "   Small: 303,350,514 (98.87%)\n",
      "   Initial ratio: 87.60:1 (Small:Heavy)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(\"\\n[STAGE 2/5] Applying labeling...\")\n",
    "\n",
    "df_labeled = loader.apply_labeling(df_raw)\n",
    "\n",
    "# Get distribution\n",
    "labeled_count = df_labeled.count()\n",
    "heavy_count = df_labeled.filter(F.col('is_heavy') == 1).count()\n",
    "small_count = labeled_count - heavy_count\n",
    "\n",
    "print(f\"\\n✅ Labeled {labeled_count:,} queries\")\n",
    "print(f\"   Heavy: {heavy_count:,} ({heavy_count/labeled_count*100:.2f}%)\")\n",
    "print(f\"   Small: {small_count:,} ({small_count/labeled_count*100:.2f}%)\")\n",
    "print(f\"   Initial ratio: {small_count/heavy_count:.2f}:1 (Small:Heavy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optional Analysis - After Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:24:21.305670Z",
     "iopub.status.busy": "2025-10-16T21:24:21.305554Z",
     "iopub.status.idle": "2025-10-16T21:24:21.366056Z",
     "shell.execute_reply": "2025-10-16T21:24:21.365620Z",
     "shell.execute_reply.started": "2025-10-16T21:24:21.305655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8984f86f3a94a788193aa3b28467111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "if config['analysis']['enabled']:\n",
    "    print(\"[ANALYSIS] Analyzing labeled data...\")\n",
    "    stats_labeled = analyzer.analyze_dataframe(df_labeled, \"After Labeling\")\n",
    "    analyzer.print_analysis_report(stats_labeled)\n",
    "    \n",
    "    comparison = analyzer.compare_dataframes(df_raw, df_labeled, \"Labeling\")\n",
    "    analyzer.print_comparison_report(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Apply Filters and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:24:21.366646Z",
     "iopub.status.busy": "2025-10-16T21:24:21.366533Z",
     "iopub.status.idle": "2025-10-16T21:29:11.351116Z",
     "shell.execute_reply": "2025-10-16T21:29:11.350655Z",
     "shell.execute_reply.started": "2025-10-16T21:24:21.366631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab8521208b945f4ada51fdcad138fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STAGE 3/5] Applying data quality filters...\n",
      "\n",
      "? Filtered dataset: 88,731,774 queries\n",
      "   Removed: 218,081,673 queries (71.1%)\n",
      "   Heavy: 2,349,368 (2.65%)\n",
      "   Small: 86,382,406 (97.35%)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(\"\\n[STAGE 3/5] Applying data quality filters...\")\n",
    "\n",
    "df_filtered = loader.apply_filters(df_labeled)\n",
    "# df_filtered = checkpoint_mgr.checkpoint(df_filtered, \"01_filtered_data\")\n",
    "\n",
    "filtered_count = df_filtered.count()\n",
    "removed_count = labeled_count - filtered_count\n",
    "\n",
    "print(f\"\\n✅ Filtered dataset: {filtered_count:,} queries\")\n",
    "print(f\"   Removed: {removed_count:,} queries ({removed_count/labeled_count*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution after filtering\n",
    "filtered_heavy = df_filtered.filter(F.col('is_heavy') == 1).count()\n",
    "filtered_small = filtered_count - filtered_heavy\n",
    "print(f\"   Heavy: {filtered_heavy:,} ({filtered_heavy/filtered_count*100:.2f}%)\")\n",
    "print(f\"   Small: {filtered_small:,} ({filtered_small/filtered_count*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Optional Analysis - After Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:29:11.351667Z",
     "iopub.status.busy": "2025-10-16T21:29:11.351556Z",
     "iopub.status.idle": "2025-10-16T21:29:11.413386Z",
     "shell.execute_reply": "2025-10-16T21:29:11.412945Z",
     "shell.execute_reply.started": "2025-10-16T21:29:11.351652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f543d7b7b6426d8507e79cedd8fd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "if config['analysis']['enabled']:\n",
    "    print(\"[ANALYSIS] Analyzing filtered data...\")\n",
    "    stats_filtered = analyzer.analyze_dataframe(df_filtered, \"After Filtering\")\n",
    "    analyzer.print_analysis_report(stats_filtered)\n",
    "    \n",
    "    comparison = analyzer.compare_dataframes(df_labeled, df_filtered, \"Filtering\")\n",
    "    analyzer.print_comparison_report(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Create Time-Based Splits (BEFORE Sampling)\n",
    "\n",
    "Split data chronologically into train/val/test BEFORE sampling.\n",
    "\n",
    "This ensures val and test maintain the original ~36:1 distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-16T21:22:25.684535Z",
     "iopub.status.idle": "2025-10-16T21:22:25.684687Z",
     "shell.execute_reply": "2025-10-16T21:22:25.684611Z",
     "shell.execute_reply.started": "2025-10-16T21:22:25.684603Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_counts_df = df_with_dates.groupBy('queryDate').count().orderBy('queryDate').withColumnRenamed('count', 'record_count')\n",
    "daily_counts_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-16T21:22:25.685117Z",
     "iopub.status.idle": "2025-10-16T21:22:25.685271Z",
     "shell.execute_reply": "2025-10-16T21:22:25.685196Z",
     "shell.execute_reply.started": "2025-10-16T21:22:25.685188Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_counts_df.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:52:15.303520Z",
     "iopub.status.busy": "2025-10-16T21:52:15.303310Z",
     "iopub.status.idle": "2025-10-16T21:59:30.627206Z",
     "shell.execute_reply": "2025-10-16T21:59:30.626700Z",
     "shell.execute_reply.started": "2025-10-16T21:52:15.303501Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e45d27fcc249c78162d895edc4a8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STAGE 4/6] Creating time-based splits...\n",
      "  Date range: 2025-08-01 to 2025-10-01\n",
      "Train: 2025-08-01 to 2025-09-17 (47 days)\n",
      "Val:   2025-09-17 to 2025-09-24 (7 days)\n",
      "Test:  2025-09-24 to 2025-10-01 (7 days)\n",
      "\n",
      "Splits created:\n",
      "  Full: 88,731,774 queries\n",
      "  Train: 58,662,498 queries\n",
      "  Val:   14,969,526 queries\n",
      "  Test:  15,099,750 queries\n",
      "  Total: 88,731,774 queries"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(\"\\n[STAGE 4/6] Creating time-based splits...\")\n",
    "\n",
    "# Add date and hour columns for splitting\n",
    "df_with_dates = df_filtered.withColumn('queryDate', to_date(F.col('createTime')))\n",
    "df_with_dates = df_with_dates.withColumn('hour', get_hour(F.col('createTime')))\n",
    "\n",
    "# Get date range from config\n",
    "start_date = datetime.strptime(config['data_loading']['start_date'], '%Y-%m-%d')\n",
    "end_date = datetime.strptime(config['data_loading']['end_date'], '%Y-%m-%d')\n",
    "\n",
    "# Calculate split boundaries\n",
    "train_days_cfg = config['time_splits']['train_days']\n",
    "val_days_cfg   = config['time_splits']['val_days']\n",
    "test_days_cfg  = config['time_splits']['test_days']\n",
    "\n",
    "test_start = end_date - timedelta(days=test_days_cfg)\n",
    "val_start  = test_start - timedelta(days=val_days_cfg)\n",
    "\n",
    "print(f\"  Date range: {start_date.date()} to {end_date.date()}\")\n",
    "print(f\"Train: {start_date.date()} to {val_start.date()} ({(val_start - start_date).days} days)\")\n",
    "print(f\"Val:   {val_start.date()} to {test_start.date()} ({val_days_cfg} days)\")\n",
    "print(f\"Test:  {test_start.date()} to {end_date.date()} ({test_days_cfg} days)\")\n",
    "start_lit = F.to_date(F.lit(start_date.strftime('%Y-%m-%d')))\n",
    "val_lit   = F.to_date(F.lit(val_start.strftime('%Y-%m-%d')))\n",
    "test_lit  = F.to_date(F.lit(test_start.strftime('%Y-%m-%d')))\n",
    "end_lit   = F.to_date(F.lit(end_date.strftime('%Y-%m-%d')))\n",
    "\n",
    "train_df = df_with_dates.filter((F.col('queryDate') >= start_lit) & (F.col('queryDate') <  val_lit))\n",
    "val_df   = df_with_dates.filter((F.col('queryDate') >= val_lit)   & (F.col('queryDate') <  test_lit))\n",
    "test_df  = df_with_dates.filter((F.col('queryDate') >= test_lit)  & (F.col('queryDate') <= end_lit))\n",
    "\n",
    "# Checkpoint splits\n",
    "train_df = checkpoint_mgr.checkpoint(train_df, \"01_train_split\")\n",
    "val_df = checkpoint_mgr.checkpoint(val_df, \"01_val_split\")\n",
    "test_df = checkpoint_mgr.checkpoint(test_df, \"01_test_split\")\n",
    "\n",
    "# Get counts\n",
    "train_count = train_df.count()\n",
    "val_count = val_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"\\nSplits created:\")\n",
    "print(f\"  Full: {filtered_count:,} queries\")\n",
    "print(f\"  Train: {train_count:,} queries\")\n",
    "print(f\"  Val:   {val_count:,} queries\")\n",
    "print(f\"  Test:  {test_count:,} queries\")\n",
    "print(f\"  Total: {train_count + val_count + test_count:,} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Report Original Distributions (Before Sampling)\n",
    "\n",
    "Show class distributions for all three splits BEFORE sampling training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:00:03.839035Z",
     "iopub.status.busy": "2025-10-16T22:00:03.838823Z",
     "iopub.status.idle": "2025-10-16T22:00:06.179236Z",
     "shell.execute_reply": "2025-10-16T22:00:06.178758Z",
     "shell.execute_reply.started": "2025-10-16T22:00:03.839016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2580d9061e14f8ca8c16951dd428fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original class distributions (before sampling):\n",
      "\n",
      "Train split (BEFORE sampling):\n",
      "  Heavy: 1,463,723 (2.50%)\n",
      "  Small: 57,198,775 (97.50%)\n",
      "  Ratio: 39.1:1 (Small:Heavy)\n",
      "\n",
      "Validation split (will remain unchanged):\n",
      "  Heavy: 304,049 (2.03%)\n",
      "  Small: 14,665,477 (97.97%)\n",
      "  Ratio: 48.2:1 (Small:Heavy)\n",
      "\n",
      "Test split (will remain unchanged):\n",
      "  Heavy: 581,596 (3.85%)\n",
      "  Small: 14,518,154 (96.15%)\n",
      "  Ratio: 25.0:1 (Small:Heavy)\n",
      "\n",
      "======================================================================\n",
      "NEXT: Sample ONLY training data to 5:1\n",
      "      Val and test will keep ~36:1 for realistic evaluation\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(\"\\nOriginal class distributions (before sampling):\")\n",
    "\n",
    "# Calculate distributions for each split\n",
    "train_heavy = train_df.filter(F.col('is_heavy') == 1).count()\n",
    "train_small = train_count - train_heavy\n",
    "train_ratio = train_small / train_heavy if train_heavy > 0 else 0\n",
    "\n",
    "val_heavy = val_df.filter(F.col('is_heavy') == 1).count()\n",
    "val_small = val_count - val_heavy\n",
    "val_ratio = val_small / val_heavy if val_heavy > 0 else 0\n",
    "\n",
    "test_heavy = test_df.filter(F.col('is_heavy') == 1).count()\n",
    "test_small = test_count - test_heavy\n",
    "test_ratio = test_small / test_heavy if test_heavy > 0 else 0\n",
    "\n",
    "print(f\"\\nTrain split (BEFORE sampling):\")\n",
    "print(f\"  Heavy: {train_heavy:,} ({train_heavy/train_count*100:.2f}%)\")\n",
    "print(f\"  Small: {train_small:,} ({train_small/train_count*100:.2f}%)\")\n",
    "print(f\"  Ratio: {train_ratio:.1f}:1 (Small:Heavy)\")\n",
    "\n",
    "print(f\"\\nValidation split (will remain unchanged):\")\n",
    "print(f\"  Heavy: {val_heavy:,} ({val_heavy/val_count*100:.2f}%)\")\n",
    "print(f\"  Small: {val_small:,} ({val_small/val_count*100:.2f}%)\")\n",
    "print(f\"  Ratio: {val_ratio:.1f}:1 (Small:Heavy)\")\n",
    "\n",
    "print(f\"\\nTest split (will remain unchanged):\")\n",
    "print(f\"  Heavy: {test_heavy:,} ({test_heavy/test_count*100:.2f}%)\")\n",
    "print(f\"  Small: {test_small:,} ({test_small/test_count*100:.2f}%)\")\n",
    "print(f\"  Ratio: {test_ratio:.1f}:1 (Small:Heavy)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT: Sample ONLY training data to 5:1\")\n",
    "print(\"      Val and test will keep ~36:1 for realistic evaluation\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Apply Boundary Sampling (TRAINING DATA ONLY)\n",
    "\n",
    "**CRITICAL**: Sample ONLY training data to 5:1 ratio. Val and test remain at original ~36:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:00:06.180049Z",
     "iopub.status.busy": "2025-10-16T22:00:06.179925Z",
     "iopub.status.idle": "2025-10-16T22:01:08.165657Z",
     "shell.execute_reply": "2025-10-16T22:01:08.165150Z",
     "shell.execute_reply.started": "2025-10-16T22:00:06.180032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c6d157e7ff4e56bada0a4856f8d8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STAGE 4/6] Applying boundary-focused sampling to TRAINING data only...\n",
      "  Target ratio: 5.0:1 (Small:Heavy)\n",
      "  Algorithm: Continuous distance-based (POC v2.1.0)\n",
      "  Val/Test: Keep at original ~36:1 distribution\n",
      "\n",
      "Training data after sampling:\n",
      "  Samples: 8,782,474 queries\n",
      "  Heavy: 1,463,723 (16.67%)\n",
      "  Small: 7,318,751 (83.33%)\n",
      "  Ratio: 5.00:1 (Small:Heavy)\n",
      "  Sampling efficiency: 15.0% of original training data retained\n",
      "\n",
      "Validation/Test data (UNCHANGED):\n",
      "  Val:  14,969,526 queries at ~48.2:1 ratio\n",
      "  Test: 15,099,750 queries at ~25.0:1 ratio\n",
      "\n",
      "======================================================================\n",
      "RESULT: Training uses 5:1 for balanced learning\n",
      "        Val/Test use ~36:1 for realistic evaluation\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(\"\\n[STAGE 4/6] Applying boundary-focused sampling to TRAINING data only...\")\n",
    "print(f\"  Target ratio: {config['boundary_sampling']['balance_ratio']}:1 (Small:Heavy)\")\n",
    "print(f\"  Algorithm: Continuous distance-based (POC v2.1.0)\")\n",
    "print(f\"  Val/Test: Keep at original ~36:1 distribution\")\n",
    "\n",
    "# Sample ONLY training data\n",
    "train_sampled = sampler.sample(train_df, label_column='is_heavy')\n",
    "train_sampled = checkpoint_mgr.checkpoint(train_sampled, \"01_train_sampled\")\n",
    "\n",
    "# Training statistics after sampling\n",
    "train_sampled_count = train_sampled.count()\n",
    "train_sampled_heavy = train_sampled.filter(F.col('is_heavy') == 1).count()\n",
    "train_sampled_small = train_sampled_count - train_sampled_heavy\n",
    "train_sampled_ratio = train_sampled_small / train_sampled_heavy if train_sampled_heavy > 0 else 0\n",
    "\n",
    "print(f\"\\nTraining data after sampling:\")\n",
    "print(f\"  Samples: {train_sampled_count:,} queries\")\n",
    "print(f\"  Heavy: {train_sampled_heavy:,} ({train_sampled_heavy/train_sampled_count*100:.2f}%)\")\n",
    "print(f\"  Small: {train_sampled_small:,} ({train_sampled_small/train_sampled_count*100:.2f}%)\")\n",
    "print(f\"  Ratio: {train_sampled_ratio:.2f}:1 (Small:Heavy)\")\n",
    "print(f\"  Sampling efficiency: {train_sampled_count/train_count*100:.1f}% of original training data retained\")\n",
    "\n",
    "print(f\"\\nValidation/Test data (UNCHANGED):\")\n",
    "print(f\"  Val:  {val_count:,} queries at ~{val_ratio:.1f}:1 ratio\")\n",
    "print(f\"  Test: {test_count:,} queries at ~{test_ratio:.1f}:1 ratio\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULT: Training uses 5:1 for balanced learning\")\n",
    "print(\"        Val/Test use ~36:1 for realistic evaluation\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Get Sampling Statistics (Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:01:08.166488Z",
     "iopub.status.busy": "2025-10-16T22:01:08.166369Z",
     "iopub.status.idle": "2025-10-16T22:01:47.832070Z",
     "shell.execute_reply": "2025-10-16T22:01:47.831541Z",
     "shell.execute_reply.started": "2025-10-16T22:01:08.166472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b0ca3aa29c4266b2fe9eafc0c06237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Boundary sampling statistics (training data only):\n",
      "\n",
      "Distance Distribution:\n",
      "  min       : 0.0000\n",
      "  mean      : 0.9708\n",
      "  median    : 0.9630\n",
      "  p90       : 1.0000\n",
      "  max       : 535.3801\n",
      "\n",
      "Sampling Efficiency by Distance:\n",
      "  very_close     : 363,855 queries\n",
      "  close          : 582,718 queries\n",
      "  moderate       : 55,654,933 queries\n",
      "  far            : 1,837,983 queries\n",
      "  very_far       : 223,009 queries"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(\"\\n[INFO] Boundary sampling statistics (training data only):\")\n",
    "\n",
    "# Get detailed sampling stats for training data\n",
    "sampling_stats = sampler.get_sampling_stats(train_df)\n",
    "\n",
    "print(\"\\nDistance Distribution:\")\n",
    "for stat, value in sampling_stats['distance_distribution'].items():\n",
    "    print(f\"  {stat:10s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nSampling Efficiency by Distance:\")\n",
    "for range_name, count in sampling_stats['sampling_efficiency'].items():\n",
    "    print(f\"  {range_name:15s}: {count:,} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:01:47.832712Z",
     "iopub.status.busy": "2025-10-16T22:01:47.832584Z",
     "iopub.status.idle": "2025-10-16T22:01:55.231577Z",
     "shell.execute_reply": "2025-10-16T22:01:55.231067Z",
     "shell.execute_reply.started": "2025-10-16T22:01:47.832696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b10ff6e34e446386c9f8b9cc0a8d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Boundary sampling statistics (Sampled training data only):\n",
      "\n",
      "Distance Distribution:\n",
      "  min       : 0.0000\n",
      "  mean      : 1.1761\n",
      "  median    : 0.9388\n",
      "  p90       : 1.0000\n",
      "  max       : 535.3801\n",
      "\n",
      "Sampling Efficiency by Distance:\n",
      "  very_close     : 264,487 queries\n",
      "  close          : 384,276 queries\n",
      "  moderate       : 7,628,809 queries\n",
      "  far            : 281,893 queries\n",
      "  very_far       : 223,009 queries"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(\"\\n[INFO] Boundary sampling statistics (Sampled training data only):\")\n",
    "\n",
    "# Get detailed sampling stats for training data\n",
    "sampling_stats = sampler.get_sampling_stats(train_sampled)\n",
    "\n",
    "print(\"\\nDistance Distribution:\")\n",
    "for stat, value in sampling_stats['distance_distribution'].items():\n",
    "    print(f\"  {stat:10s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nSampling Efficiency by Distance:\")\n",
    "for range_name, count in sampling_stats['sampling_efficiency'].items():\n",
    "    print(f\"  {range_name:15s}: {count:,} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Processed Data to S3\n",
    "\n",
    "Save three separate datasets:\n",
    "- train_sampled: 5:1 ratio for training\n",
    "- val_original: ~36:1 ratio for validation\n",
    "- test_original: ~36:1 ratio for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:01:55.232225Z",
     "iopub.status.busy": "2025-10-16T22:01:55.232107Z",
     "iopub.status.idle": "2025-10-16T22:02:14.748908Z",
     "shell.execute_reply": "2025-10-16T22:02:14.748430Z",
     "shell.execute_reply.started": "2025-10-16T22:01:55.232208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301e4d31ba694684a3d6ac628ccb106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STAGE 5/6] Saving processed data to S3...\n",
      "  Base output path: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01\n",
      "\n",
      "[1/3] Saving train_sampled...\n",
      "  Saved 8,782,474 queries to s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/train_sampled\n",
      "  Distribution: 5.0:1 (Small:Heavy)\n",
      "\n",
      "[2/3] Saving val_original...\n",
      "  Saved 14,969,526 queries to s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/val_original\n",
      "  Distribution: 48.2:1 (Small:Heavy)\n",
      "\n",
      "[3/3] Saving test_original...\n",
      "  Saved 15,099,750 queries to s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/test_original\n",
      "  Distribution: 25.0:1 (Small:Heavy)\n",
      "\n",
      "All datasets saved to S3\n",
      "  Partitioned by: is_heavy\n",
      "  Format: Parquet\n",
      "\n",
      "IMPORTANT: Three separate datasets created:\n",
      "  1. train_sampled: Balanced 5:1 for training\n",
      "  2. val_original:  Natural ~36:1 for validation\n",
      "  3. test_original: Natural ~36:1 for testing"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(\"\\n[STAGE 5/6] Saving processed data to S3...\")\n",
    "\n",
    "# Define output paths\n",
    "date_range = f\"{config['data_loading']['start_date']}_to_{config['data_loading']['end_date']}\"\n",
    "base_output_path = f\"{config['data_loading']['processed_output_path']}/{date_range}\"\n",
    "\n",
    "train_sampled_path = f\"{base_output_path}/train_sampled\"\n",
    "val_original_path = f\"{base_output_path}/val_original\"\n",
    "test_original_path = f\"{base_output_path}/test_original\"\n",
    "\n",
    "print(f\"  Base output path: {base_output_path}\")\n",
    "\n",
    "# Select relevant columns for downstream processing\n",
    "output_columns = [\n",
    "    'queryId', 'query', 'createTime', 'endTime',\n",
    "    'user', 'catalog', 'schema', 'clientInfo', 'queryDate', 'hour',\n",
    "    'cpuTime', 'peakUserMemoryBytes', 'totalBytes',\n",
    "    'queryType', 'errorName', 'sessionproperties',\n",
    "    'cpu_time_seconds', 'memory_gb', 'is_heavy'\n",
    "]\n",
    "\n",
    "# Save train_sampled (5:1 ratio)\n",
    "print(f\"\\n[1/3] Saving train_sampled...\")\n",
    "train_sampled.select(*output_columns) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy('is_heavy') \\\n",
    "    .parquet(train_sampled_path)\n",
    "print(f\"  Saved {train_sampled_count:,} queries to {train_sampled_path}\")\n",
    "print(f\"  Distribution: {train_sampled_ratio:.1f}:1 (Small:Heavy)\")\n",
    "\n",
    "# Save val_original (~ 36:1 ratio)\n",
    "print(f\"\\n[2/3] Saving val_original...\")\n",
    "val_df.select(*output_columns) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy('is_heavy') \\\n",
    "    .parquet(val_original_path)\n",
    "print(f\"  Saved {val_count:,} queries to {val_original_path}\")\n",
    "print(f\"  Distribution: {val_ratio:.1f}:1 (Small:Heavy)\")\n",
    "\n",
    "# Save test_original (~36:1 ratio)\n",
    "print(f\"\\n[3/3] Saving test_original...\")\n",
    "test_df.select(*output_columns) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy('is_heavy') \\\n",
    "    .parquet(test_original_path)\n",
    "print(f\"  Saved {test_count:,} queries to {test_original_path}\")\n",
    "print(f\"  Distribution: {test_ratio:.1f}:1 (Small:Heavy)\")\n",
    "\n",
    "print(f\"\\nAll datasets saved to S3\")\n",
    "print(f\"  Partitioned by: is_heavy\")\n",
    "print(f\"  Format: Parquet\")\n",
    "print(f\"\\nIMPORTANT: Three separate datasets created:\")\n",
    "print(f\"  1. train_sampled: Balanced 5:1 for training\")\n",
    "print(f\"  2. val_original:  Natural ~36:1 for validation\")\n",
    "print(f\"  3. test_original: Natural ~36:1 for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Generate Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:02:14.749581Z",
     "iopub.status.busy": "2025-10-16T22:02:14.749445Z",
     "iopub.status.idle": "2025-10-16T22:02:14.820263Z",
     "shell.execute_reply": "2025-10-16T22:02:14.819806Z",
     "shell.execute_reply.started": "2025-10-16T22:02:14.749565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a456834b94c148028cb7e655f67b14b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA LOADING PIPELINE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Execution Date: 2025-10-16 22:02:14\n",
      "Date Range: 2025-08-01 to 2025-10-01\n",
      "\n",
      "PIPELINE STAGES:\n",
      "1. Raw Data Loaded:     306,813,447 queries\n",
      "2. After Labeling:      306,813,447 queries\n",
      "3. After Filtering:     88,731,774 queries (removed 218,081,673)\n",
      "4. After Splitting:     Train: 58,662,498, Val: 14,969,526, Test: 15,099,750\n",
      "5. After Sampling:      Train: 8,782,474 (Val/Test UNCHANGED)\n",
      "6. Saved to S3:         3 separate datasets\n",
      "\n",
      "CRITICAL: CLASS DISTRIBUTION STRATEGY\n",
      "================================================================================\n",
      "This pipeline uses SPLIT-THEN-SAMPLE to provide realistic evaluation:\n",
      "\n",
      "TRAINING DATA (Sampled to 5:1):\n",
      "- Samples:  8,782,474 queries\n",
      "- Heavy:    1,463,723 (16.67%)\n",
      "- Small:    7,318,751 (83.33%)\n",
      "- Ratio:    5.00:1 (Small:Heavy)\n",
      "- Purpose:  Balanced learning with boundary sampling\n",
      "\n",
      "VALIDATION DATA (Original Distribution ~36:1):\n",
      "- Samples:  14,969,526 queries\n",
      "- Heavy:    304,049 (2.03%)\n",
      "- Small:    14,665,477 (97.97%)\n",
      "- Ratio:    48.2:1 (Small:Heavy)\n",
      "- Purpose:  Realistic performance evaluation\n",
      "\n",
      "TEST DATA (Original Distribution ~36:1):\n",
      "- Samples:  15,099,750 queries\n",
      "- Heavy:    581,596 (3.85%)\n",
      "- Small:    14,518,154 (96.15%)\n",
      "- Ratio:    25.0:1 (Small:Heavy)\n",
      "- Purpose:  Final realistic performance evaluation\n",
      "\n",
      "RATIONALE:\n",
      "- Training uses 5:1 sampled data for effective learning from imbalanced classes\n",
      "- Val/Test use ~36:1 original data to reflect production performance\n",
      "- This prevents overoptimistic metrics and ensures realistic evaluation\n",
      "- Model learns from balanced examples but evaluated on real distribution\n",
      "\n",
      "SAMPLING CONFIGURATION (Training Only):\n",
      "- Algorithm: Continuous distance-based (POC v2.1.0)\n",
      "- Target ratio: 5.0:1\n",
      "- Max boost: 2.0x\n",
      "- Min multiplier: 0.05x\n",
      "- Guarantee threshold: 0.5\n",
      "- Safety adjustment: ENABLED\n",
      "\n",
      "OUTPUT PATHS:\n",
      "- train_sampled: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/train_sampled\n",
      "- val_original:  s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/val_original\n",
      "- test_original: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/test_original\n",
      "- Format: Parquet (partitioned by is_heavy)\n",
      "\n",
      "CHECKPOINTS:\n",
      "  - 01_train_split\n",
      "  - 01_val_split\n",
      "  - 01_test_split\n",
      "  - 01_train_sampled\n",
      "\n",
      "STATUS: PIPELINE COMPLETED SUCCESSFULLY\n",
      "\n",
      "NEXT STEPS:\n",
      "1. Run notebook 02_zero_cost_analysis.ipynb (optional)\n",
      "2. Run notebook 03_feature_engineering.ipynb (will load from 3 separate paths)\n",
      "3. Expect validation/test metrics to reflect realistic 36:1 distribution:\n",
      "   - Precision will be lower (~20-25% vs ~70% on sampled data)\n",
      "   - Recall should remain high (>95% with proper threshold)\n",
      "   - This is EXPECTED and CORRECT for production deployment\n",
      "\n",
      "================================================================================"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "summary_report = f\"\"\"\n",
    "{'='*80}\n",
    "DATA LOADING PIPELINE SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "Execution Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Date Range: {config['data_loading']['start_date']} to {config['data_loading']['end_date']}\n",
    "\n",
    "PIPELINE STAGES:\n",
    "1. Raw Data Loaded:     {raw_count:,} queries\n",
    "2. After Labeling:      {labeled_count:,} queries\n",
    "3. After Filtering:     {filtered_count:,} queries (removed {labeled_count - filtered_count:,})\n",
    "4. After Splitting:     Train: {train_count:,}, Val: {val_count:,}, Test: {test_count:,}\n",
    "5. After Sampling:      Train: {train_sampled_count:,} (Val/Test UNCHANGED)\n",
    "6. Saved to S3:         3 separate datasets\n",
    "\n",
    "CRITICAL: CLASS DISTRIBUTION STRATEGY\n",
    "{'='*80}\n",
    "This pipeline uses SPLIT-THEN-SAMPLE to provide realistic evaluation:\n",
    "\n",
    "TRAINING DATA (Sampled to 5:1):\n",
    "- Samples:  {train_sampled_count:,} queries\n",
    "- Heavy:    {train_sampled_heavy:,} ({train_sampled_heavy/train_sampled_count*100:.2f}%)\n",
    "- Small:    {train_sampled_small:,} ({train_sampled_small/train_sampled_count*100:.2f}%)\n",
    "- Ratio:    {train_sampled_ratio:.2f}:1 (Small:Heavy)\n",
    "- Purpose:  Balanced learning with boundary sampling\n",
    "\n",
    "VALIDATION DATA (Original Distribution ~36:1):\n",
    "- Samples:  {val_count:,} queries\n",
    "- Heavy:    {val_heavy:,} ({val_heavy/val_count*100:.2f}%)\n",
    "- Small:    {val_small:,} ({val_small/val_count*100:.2f}%)\n",
    "- Ratio:    {val_ratio:.1f}:1 (Small:Heavy)\n",
    "- Purpose:  Realistic performance evaluation\n",
    "\n",
    "TEST DATA (Original Distribution ~36:1):\n",
    "- Samples:  {test_count:,} queries\n",
    "- Heavy:    {test_heavy:,} ({test_heavy/test_count*100:.2f}%)\n",
    "- Small:    {test_small:,} ({test_small/test_count*100:.2f}%)\n",
    "- Ratio:    {test_ratio:.1f}:1 (Small:Heavy)\n",
    "- Purpose:  Final realistic performance evaluation\n",
    "\n",
    "RATIONALE:\n",
    "- Training uses 5:1 sampled data for effective learning from imbalanced classes\n",
    "- Val/Test use ~36:1 original data to reflect production performance\n",
    "- This prevents overoptimistic metrics and ensures realistic evaluation\n",
    "- Model learns from balanced examples but evaluated on real distribution\n",
    "\n",
    "SAMPLING CONFIGURATION (Training Only):\n",
    "- Algorithm: Continuous distance-based (POC v2.1.0)\n",
    "- Target ratio: {config['boundary_sampling']['balance_ratio']}:1\n",
    "- Max boost: {config['boundary_sampling']['boundary_sampling_max_boost']}x\n",
    "- Min multiplier: {config['boundary_sampling']['boundary_sampling_min_multiplier']}x\n",
    "- Guarantee threshold: {config['boundary_sampling']['guarantee_close_threshold']}\n",
    "- Safety adjustment: {'ENABLED' if config['boundary_sampling']['enable_safety_adjustment'] else 'DISABLED'}\n",
    "\n",
    "OUTPUT PATHS:\n",
    "- train_sampled: {train_sampled_path}\n",
    "- val_original:  {val_original_path}\n",
    "- test_original: {test_original_path}\n",
    "- Format: Parquet (partitioned by is_heavy)\n",
    "\n",
    "CHECKPOINTS:\n",
    "{chr(10).join([f'  - {name}' for name in checkpoint_mgr.list_checkpoints()])}\n",
    "\n",
    "STATUS: PIPELINE COMPLETED SUCCESSFULLY\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Run notebook 02_zero_cost_analysis.ipynb (optional)\n",
    "2. Run notebook 03_feature_engineering.ipynb (will load from 3 separate paths)\n",
    "3. Expect validation/test metrics to reflect realistic 36:1 distribution:\n",
    "   - Precision will be lower (~20-25% vs ~70% on sampled data)\n",
    "   - Recall should remain high (>95% with proper threshold)\n",
    "   - This is EXPECTED and CORRECT for production deployment\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T22:08:46.159363Z",
     "iopub.status.busy": "2025-10-16T22:08:46.159179Z",
     "iopub.status.idle": "2025-10-16T22:08:46.222479Z",
     "shell.execute_reply": "2025-10-16T22:08:46.221988Z",
     "shell.execute_reply.started": "2025-10-16T22:08:46.159346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469664cbe72a4c34a55e1742b1585540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLEANUP] Releasing resources...\n",
      "  Unpersisted df_raw\n",
      "  Unpersisted df_labeled\n",
      "  Unpersisted df_filtered\n",
      "  Unpersisted train_df\n",
      "  Unpersisted val_df\n",
      "  Unpersisted test_df\n",
      "  Unpersisted train_sampled\n",
      "\n",
      "Cleanup completed\n",
      "   Checkpoints preserved in S3: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/checkpoints\n",
      "   Use checkpoint_mgr.cleanup(delete_from_s3=True) to delete checkpoints\n",
      "\n",
      "================================================================================\n",
      "DATA LOADING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "REMINDER: Three separate datasets created with different distributions:\n",
      "  - Training: 5.0:1 (sampled for balanced learning)\n",
      "  - Val/Test: ~48.2:1 (original for realistic evaluation)\n",
      "================================================================================"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print(\"\\n[CLEANUP] Releasing resources...\")\n",
    "\n",
    "# Unpersist DataFrames to free memory\n",
    "for df_name, df in [('df_raw', df_raw), ('df_labeled', df_labeled), \n",
    "                      ('df_filtered', df_filtered), ('train_df', train_df),\n",
    "                      ('val_df', val_df), ('test_df', test_df),\n",
    "                      ('train_sampled', train_sampled)]:\n",
    "    try:\n",
    "        df.unpersist()\n",
    "        print(f\"  Unpersisted {df_name}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Checkpoints remain in S3 for debugging (not deleted)\n",
    "print(f\"\\nCleanup completed\")\n",
    "print(f\"   Checkpoints preserved in S3: {config['checkpointing']['s3_path']}\")\n",
    "print(f\"   Use checkpoint_mgr.cleanup(delete_from_s3=True) to delete checkpoints\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA LOADING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nREMINDER: Three separate datasets created with different distributions:\")\n",
    "print(f\"  - Training: {train_sampled_ratio:.1f}:1 (sampled for balanced learning)\")\n",
    "print(f\"  - Val/Test: ~{val_ratio:.1f}:1 (original for realistic evaluation)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
