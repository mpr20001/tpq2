{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Model Training\n",
    "\n",
    "**Purpose**: XGBoost training with PRD compliance and ONNX export\n",
    "\n",
    "**Pipeline**:\n",
    "1. Load feature datasets from notebook 03\n",
    "2. Convert Spark DataFrames to NumPy arrays\n",
    "3. Train XGBoost with 5-fold cross-validation\n",
    "4. Optimize threshold (100:1 FN:FP cost ratio)\n",
    "5. Evaluate on test set\n",
    "6. Check PRD compliance (recall ≥98%, FNR ≤2%)\n",
    "7. Export to ONNX with validation\n",
    "8. Save model artifacts to S3\n",
    "\n",
    "**Key Features**:\n",
    "- Stratified cross-validation\n",
    "- Early stopping\n",
    "- Cost-based threshold optimization\n",
    "- PRD requirement validation\n",
    "- ONNX export with parity check\n",
    "\n",
    "**Prerequisites**:\n",
    "- Notebook 03 completed (feature datasets available)\n",
    "\n",
    "**Duration**: ~30-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Configuration\n",
    "\n",
    "Copy the Spark configuration from notebook 00 output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T19:53:19.125025Z",
     "iopub.status.busy": "2025-10-17T19:53:19.124917Z",
     "iopub.status.idle": "2025-10-17T19:53:20.360848Z",
     "shell.execute_reply": "2025-10-17T19:53:20.359969Z",
     "shell.execute_reply.started": "2025-10-17T19:53:19.125009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': './environment/bin/python', 'spark.yarn.dist.archives': 's3://uip-datalake-bucket-prod/uipexplore/shared/python311common_env.tar.gz#environment', 'spark.submit.deployMode': 'cluster', 'spark.driver.maxResultSize': '8G', 'spark.dynamicAllocation.enabled': 'true', 'spark.dynamicAllocation.minExecutors': '2', 'spark.dynamicAllocation.maxExecutors': '20'}, 'driverMemory': '16G', 'executorCores': 5, 'executorMemory': '20G', 'pyFiles': ['s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip', 's3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py'], 'driverCores': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3283</td><td>application_1758752217644_217293</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_217293/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-33-255.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_217293_01_000001/ankit.mani\">Link</a></td><td>ankit.mani</td><td></td></tr><tr><td>3288</td><td>application_1758752217644_218326</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_218326/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-114.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_218326_01_000001/mumath\">Link</a></td><td>mumath</td><td></td></tr><tr><td>3301</td><td>application_1758752217644_219296</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_219296/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-33-224.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_219296_01_000003/pmannem\">Link</a></td><td>pmannem</td><td></td></tr><tr><td>3306</td><td>application_1758752217644_219830</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_219830/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-33.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_219830_01_000001/xiao.zhang\">Link</a></td><td>xiao.zhang</td><td></td></tr><tr><td>3308</td><td>application_1758752217644_219918</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_219918/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-199.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_219918_01_000001/mumath\">Link</a></td><td>mumath</td><td></td></tr><tr><td>3312</td><td>application_1758752217644_220089</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_220089/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-199.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_220089_01_000001/kautilya.patel\">Link</a></td><td>kautilya.patel</td><td></td></tr><tr><td>3313</td><td>application_1758752217644_220121</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_220121/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-191.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_220121_01_000001/kenyu.yu\">Link</a></td><td>kenyu.yu</td><td></td></tr><tr><td>3314</td><td>application_1758752217644_220177</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_220177/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-35-229.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_220177_01_000001/kenyu.yu\">Link</a></td><td>kenyu.yu</td><td></td></tr><tr><td>3315</td><td>application_1758752217644_220216</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_220216/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-33.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_220216_01_000001/rsinghchouhan\">Link</a></td><td>rsinghchouhan</td><td></td></tr><tr><td>3321</td><td>application_1758752217644_220635</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_220635/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-191.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_220635_01_000002/w.scroggins\">Link</a></td><td>w.scroggins</td><td></td></tr><tr><td>3322</td><td>application_1758752217644_220647</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_220647/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-191.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_220647_01_000002/dsogani\">Link</a></td><td>dsogani</td><td></td></tr><tr><td>3333</td><td>None</td><td>pyspark</td><td>starting</td><td></td><td></td><td>sahith.kondepudi</td><td></td></tr><tr><td>3334</td><td>application_1758752217644_221289</td><td>pyspark</td><td>starting</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_221289/\">Link</a></td><td></td><td>feifan.jian</td><td></td></tr><tr><td>3335</td><td>None</td><td>pyspark</td><td>starting</td><td></td><td></td><td>jameslee</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"pyFiles\": [\n",
    "        \"s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip\",\n",
    "        \"s3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py\"\n",
    "    ],\n",
    "    \"driverMemory\": \"16G\",\n",
    "    \"driverCores\": 4,\n",
    "    \"executorMemory\": \"20G\",\n",
    "    \"executorCores\": 5,\n",
    "    \"conf\": {\n",
    "        \"spark.driver.maxResultSize\": \"8G\",\n",
    "        \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "        \"spark.dynamicAllocation.minExecutors\": \"2\",\n",
    "        \"spark.dynamicAllocation.maxExecutors\": \"20\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T19:53:20.361568Z",
     "iopub.status.busy": "2025-10-17T19:53:20.361450Z",
     "iopub.status.idle": "2025-10-17T20:02:33.416017Z",
     "shell.execute_reply": "2025-10-17T20:02:33.415141Z",
     "shell.execute_reply.started": "2025-10-17T19:53:20.361551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3336</td><td>application_1758752217644_221350</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_221350/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-33-224.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_221350_01_000002/pmannem\">Link</a></td><td>pmannem</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de38465c08847a18d33beeefa247145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c50322d051c455bb36acfdcb623f1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.13 (main, Jul 30 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]\n",
      "PySpark version: 3.5.4-amzn-0\n",
      "? All imports successful"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import training modules\n",
    "from query_predictor.training.model_trainer import ModelTrainer\n",
    "from query_predictor.training.prd_checker import PRDChecker\n",
    "from query_predictor.training.onnx_validator import ONNXValidator\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {spark.version}\")\n",
    "print(\"✅ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T20:02:33.416912Z",
     "iopub.status.busy": "2025-10-17T20:02:33.416785Z",
     "iopub.status.idle": "2025-10-17T20:02:34.489487Z",
     "shell.execute_reply": "2025-10-17T20:02:34.488992Z",
     "shell.execute_reply.started": "2025-10-17T20:02:33.416894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42dafa6fe394aa9a0fd0a492ac81a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config from S3: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/config/training_config_latest.yaml\n",
      "? Config downloaded to: /tmp/training_config.yaml\n",
      "? Configuration loaded\n",
      "\n",
      "? Model Configuration:\n",
      "  Algorithm: xgboost\n",
      "  N estimators: 100\n",
      "  CV folds: 5\n",
      "  Cost FN: 100.0\n",
      "  Cost FP: 1.0\n",
      "\n",
      "? PRD Requirements:\n",
      "  Target recall: ?0.98\n",
      "  Target FNR: ?0.02\n",
      "  Target F1: ?0.85\n",
      "  Target ROC-AUC: ?0.9"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Download training configuration from S3\n",
    "s3_client = boto3.client('s3')\n",
    "s3_bucket = 'uip-datalake-bucket-prod'\n",
    "s3_prefix = 'sf_trino/trino_query_predictor'\n",
    "config_s3_key = f\"{s3_prefix}/config/training_config_latest.yaml\"\n",
    "config_path = '/tmp/training_config.yaml'\n",
    "\n",
    "print(f\"Downloading config from S3: s3://{s3_bucket}/{config_s3_key}\")\n",
    "s3_client.download_file(s3_bucket, config_s3_key, config_path)\n",
    "print(f\"✅ Config downloaded to: {config_path}\")\n",
    "\n",
    "# Load training configuration\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"\\n📋 Model Configuration:\")\n",
    "print(f\"  Algorithm: {config['model']['algorithm']}\")\n",
    "print(f\"  N estimators: {config['model']['n_estimators']}\")\n",
    "print(f\"  CV folds: {config['model']['cv_folds']}\")\n",
    "print(f\"  Cost FN: {config['model']['cost_fn']}\")\n",
    "print(f\"  Cost FP: {config['model']['cost_fp']}\")\n",
    "print(f\"\\n📊 PRD Requirements:\")\n",
    "print(f\"  Target recall: ≥{config['prd_requirements']['target_heavy_recall']}\")\n",
    "print(f\"  Target FNR: ≤{config['prd_requirements']['target_fnr']}\")\n",
    "print(f\"  Target F1: ≥{config['prd_requirements']['target_f1']}\")\n",
    "print(f\"  Target ROC-AUC: ≥{config['prd_requirements']['target_roc_auc']}\")\n",
    "\n",
    "# OPTIONAL: Override config parameters after loading\n",
    "# Example: Change model hyperparameters\n",
    "# config['model']['n_estimators'] = 150  # More trees\n",
    "# config['model']['max_depth'] = 8  # Deeper trees\n",
    "# config['model']['learning_rate'] = 0.05  # Slower learning\n",
    "# Example: Adjust cost ratio for threshold optimization\n",
    "# config['model']['cost_fn'] = 200.0  # Higher cost for missing heavy queries\n",
    "# config['model']['cost_fp'] = 1.0\n",
    "# Example: Change PRD requirements for stricter validation\n",
    "# config['prd_requirements']['target_heavy_recall'] = 0.99  # 99% recall target\n",
    "# config['prd_requirements']['target_fnr'] = 0.01  # 1% FNR target\n",
    "# Example: Change validation samples\n",
    "# config['validation']['onnx_validation_samples'] = 500  # Fewer samples for faster validation\n",
    "# Example: Adjust ONNX export settings\n",
    "# config['validation']['onnx_opset_version'] = 13  # Different opset version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Feature Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T20:02:34.490156Z",
     "iopub.status.busy": "2025-10-17T20:02:34.490020Z",
     "iopub.status.idle": "2025-10-17T20:03:30.485858Z",
     "shell.execute_reply": "2025-10-17T20:03:30.485393Z",
     "shell.execute_reply.started": "2025-10-17T20:02:34.490138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e82de6106d45c88779959a15042f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature datasets from S3...\n",
      "  Base path: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01\n",
      "\n",
      "? Datasets loaded:\n",
      "  Train: 8,782,474 queries\n",
      "  Val:   14,969,526 queries\n",
      "  Test:  15,099,750 queries"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "features_path = config['features']['output_path']\n",
    "date_range = f\"{config['data_loading']['start_date']}_to_{config['data_loading']['end_date']}\"\n",
    "base_path = f\"{features_path}/{date_range}\"\n",
    "\n",
    "train_path = f\"{base_path}/train\"\n",
    "val_path = f\"{base_path}/val\"\n",
    "test_path = f\"{base_path}/test\"\n",
    "\n",
    "print(f\"Loading feature datasets from S3...\")\n",
    "print(f\"  Base path: {base_path}\")\n",
    "\n",
    "# Load datasets\n",
    "train_df = spark.read.parquet(train_path)\n",
    "val_df = spark.read.parquet(val_path)\n",
    "test_df = spark.read.parquet(test_path)\n",
    "\n",
    "print(f\"\\n✅ Datasets loaded:\")\n",
    "print(f\"  Train: {train_df.count():,} queries\")\n",
    "print(f\"  Val:   {val_df.count():,} queries\")\n",
    "print(f\"  Test:  {test_df.count():,} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert to NumPy Arrays\n",
    "\n",
    "Convert Spark DataFrames to NumPy arrays for sklearn/XGBoost training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T20:03:30.486445Z",
     "iopub.status.busy": "2025-10-17T20:03:30.486334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128a1025bea54d13ad58a51dbbb300e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Converting DataFrames to NumPy arrays...\")\n",
    "print(\"This may take 5-10 minutes...\")\n",
    "\n",
    "# Collect features and labels\n",
    "print(\"\\n[1/3] Collecting train data...\")\n",
    "train_data = train_df.select('features', 'is_heavy').collect()\n",
    "X_train = np.array([row['features'] for row in train_data], dtype=np.float32)\n",
    "y_train = np.array([row['is_heavy'] for row in train_data], dtype=np.int32)\n",
    "\n",
    "print(\"[2/3] Collecting val data...\")\n",
    "val_data = val_df.select('features', 'is_heavy').collect()\n",
    "X_val = np.array([row['features'] for row in val_data], dtype=np.float32)\n",
    "y_val = np.array([row['is_heavy'] for row in val_data], dtype=np.int32)\n",
    "\n",
    "print(\"[3/3] Collecting test data...\")\n",
    "test_data = test_df.select('features', 'is_heavy').collect()\n",
    "X_test = np.array([row['features'] for row in test_data], dtype=np.float32)\n",
    "y_test = np.array([row['is_heavy'] for row in test_data], dtype=np.int32)\n",
    "\n",
    "print(f\"\\n✅ Conversion complete:\")\n",
    "print(f\"  X_train shape: {X_train.shape}\")\n",
    "print(f\"  X_val shape:   {X_val.shape}\")\n",
    "print(f\"  X_test shape:  {X_test.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Train: {np.mean(y_train):.2%} heavy\")\n",
    "print(f\"  Val:   {np.mean(y_val):.2%} heavy\")\n",
    "print(f\"  Test:  {np.mean(y_test):.2%} heavy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train XGBoost Model\n",
    "\n",
    "Train with cross-validation and threshold optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING XGBOOST MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(config)\n",
    "\n",
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"This may take 20-30 minutes...\\n\")\n",
    "\n",
    "training_results = trainer.train(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "test_metrics = trainer.evaluate(\n",
    "    model=training_results.model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    threshold=training_results.optimal_threshold\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Test Set Metrics:\")\n",
    "print(f\"  Threshold: {test_metrics['threshold']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  FNR:       {test_metrics['fnr']:.4f}\")\n",
    "print(f\"  FPR:       {test_metrics['fpr']:.4f}\")\n",
    "\n",
    "cm = test_metrics['confusion_matrix']\n",
    "print(f\"\\n📈 Confusion Matrix:\")\n",
    "print(f\"  TN: {cm['tn']:,}  FP: {cm['fp']:,}\")\n",
    "print(f\"  FN: {cm['fn']:,}  TP: {cm['tp']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PRD Compliance Check\n",
    "\n",
    "Validate that model meets PRD requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PRD COMPLIANCE CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize PRD checker\n",
    "prd_checker = PRDChecker(config)\n",
    "\n",
    "# Check compliance\n",
    "prd_report = prd_checker.check_compliance(test_metrics, strict=True)\n",
    "\n",
    "# Generate text report\n",
    "report_text = prd_checker.generate_report_text(prd_report)\n",
    "print(report_text)\n",
    "\n",
    "# Raise error if critical requirements not met\n",
    "if not prd_report.summary['critical_requirements_met']:\n",
    "    print(\"\\n❌ CRITICAL: Model does not meet PRD requirements!\")\n",
    "    print(\"Cannot proceed with ONNX export.\")\n",
    "    raise ValueError(\"Model failed PRD compliance check\")\n",
    "\n",
    "print(\"\\n✅ PRD compliance validated - proceeding with ONNX export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export to ONNX\n",
    "\n",
    "Convert XGBoost model to ONNX format for production inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ONNX EXPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize ONNX validator from config\n",
    "onnx_validator = ONNXValidator(config=config)\n",
    "\n",
    "# Create temporary ONNX file\n",
    "with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmp:\n",
    "    onnx_path = tmp.name\n",
    "\n",
    "# Export to ONNX (opset version from config)\n",
    "input_size = X_train.shape[1]\n",
    "onnx_validator.export_to_onnx(\n",
    "    model=training_results.model,\n",
    "    output_path=onnx_path,\n",
    "    input_size=input_size\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ ONNX export complete: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Validate ONNX Predictions\n",
    "\n",
    "Ensure ONNX predictions match XGBoost predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ONNX VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get validation config\n",
    "validation_config = config.get('validation', {})\n",
    "n_samples = validation_config.get('onnx_validation_samples', 1000)\n",
    "\n",
    "# Validate ONNX\n",
    "onnx_result = onnx_validator.validate_onnx(\n",
    "    xgb_model=training_results.model,\n",
    "    onnx_path=onnx_path,\n",
    "    X_test=X_test,\n",
    "    n_samples=n_samples\n",
    ")\n",
    "\n",
    "# Generate report\n",
    "validation_report = onnx_validator.generate_validation_report(onnx_result)\n",
    "print(validation_report)\n",
    "\n",
    "# Raise error if validation failed\n",
    "if not onnx_result.passed:\n",
    "    print(\"\\n⚠️  WARNING: ONNX validation failed!\")\n",
    "    print(\"ONNX predictions differ significantly from XGBoost.\")\n",
    "    raise ValueError(\"ONNX validation failed\")\n",
    "\n",
    "print(\"✅ ONNX validation passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model Artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSaving model artifacts to S3...\")\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "models_path = config['model']['models_path']\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_version = f\"{date_range}_{timestamp}\"\n",
    "\n",
    "# 1. Save XGBoost model\n",
    "print(\"\\n[1/4] Saving XGBoost model...\")\n",
    "with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as tmp:\n",
    "    trainer.save_model(training_results.model, tmp.name)\n",
    "    xgb_s3_key = f\"{config['s3']['prefix']}/models/xgboost_{model_version}.pkl\"\n",
    "    s3_client.upload_file(tmp.name, config['s3']['bucket'], xgb_s3_key)\n",
    "    xgb_s3_path = f\"s3://{config['s3']['bucket']}/{xgb_s3_key}\"\n",
    "    print(f\"  ✅ Uploaded: {xgb_s3_path}\")\n",
    "\n",
    "# 2. Save ONNX model\n",
    "print(\"[2/4] Saving ONNX model...\")\n",
    "onnx_s3_key = f\"{config['s3']['prefix']}/models/model_{model_version}.onnx\"\n",
    "s3_client.upload_file(onnx_path, config['s3']['bucket'], onnx_s3_key)\n",
    "onnx_s3_path = f\"s3://{config['s3']['bucket']}/{onnx_s3_key}\"\n",
    "print(f\"  ✅ Uploaded: {onnx_s3_path}\")\n",
    "\n",
    "# Also save as \"latest\"\n",
    "latest_onnx_key = f\"{config['s3']['prefix']}/models/model_latest.onnx\"\n",
    "s3_client.copy_object(\n",
    "    Bucket=config['s3']['bucket'],\n",
    "    CopySource={'Bucket': config['s3']['bucket'], 'Key': onnx_s3_key},\n",
    "    Key=latest_onnx_key\n",
    ")\n",
    "print(f\"  ✅ Updated latest: s3://{config['s3']['bucket']}/{latest_onnx_key}\")\n",
    "\n",
    "# 3. Save threshold\n",
    "print(\"[3/4] Saving optimal threshold...\")\n",
    "threshold_data = {\n",
    "    'optimal_threshold': float(training_results.optimal_threshold),\n",
    "    'model_version': model_version,\n",
    "    'timestamp': timestamp\n",
    "}\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tmp:\n",
    "    json.dump(threshold_data, tmp, indent=2)\n",
    "    tmp.flush()\n",
    "    threshold_s3_key = f\"{config['s3']['prefix']}/models/threshold_{model_version}.json\"\n",
    "    s3_client.upload_file(tmp.name, config['s3']['bucket'], threshold_s3_key)\n",
    "    print(f\"  ✅ Uploaded: s3://{config['s3']['bucket']}/{threshold_s3_key}\")\n",
    "\n",
    "# Also save as \"latest\"\n",
    "latest_threshold_key = f\"{config['s3']['prefix']}/models/threshold_latest.json\"\n",
    "s3_client.copy_object(\n",
    "    Bucket=config['s3']['bucket'],\n",
    "    CopySource={'Bucket': config['s3']['bucket'], 'Key': threshold_s3_key},\n",
    "    Key=latest_threshold_key\n",
    ")\n",
    "\n",
    "# 4. Save training metadata\n",
    "print(\"[4/4] Saving training metadata...\")\n",
    "metadata = {\n",
    "    'model_version': model_version,\n",
    "    'timestamp': timestamp,\n",
    "    'date_range': date_range,\n",
    "    'config': {\n",
    "        'n_estimators': config['model']['n_estimators'],\n",
    "        'max_depth': config['model']['max_depth'],\n",
    "        'learning_rate': config['model']['learning_rate'],\n",
    "        'cost_fn': config['model']['cost_fn'],\n",
    "        'cost_fp': config['model']['cost_fp']\n",
    "    },\n",
    "    'dataset_sizes': {\n",
    "        'train': int(X_train.shape[0]),\n",
    "        'val': int(X_val.shape[0]),\n",
    "        'test': int(X_test.shape[0])\n",
    "    },\n",
    "    'cv_results': {\n",
    "        'mean_metrics': training_results.cv_results.mean_metrics,\n",
    "        'best_iteration': training_results.cv_results.best_iteration\n",
    "    },\n",
    "    'optimal_threshold': float(training_results.optimal_threshold),\n",
    "    'test_metrics': test_metrics,\n",
    "    'prd_compliance': prd_checker.export_report_json(prd_report),\n",
    "    'onnx_validation': {\n",
    "        'passed': onnx_result.passed,\n",
    "        'max_difference': onnx_result.max_difference,\n",
    "        'mean_difference': onnx_result.mean_difference,\n",
    "        'mismatch_rate': onnx_result.mismatch_rate\n",
    "    },\n",
    "    's3_paths': {\n",
    "        'xgboost_model': xgb_s3_path,\n",
    "        'onnx_model': onnx_s3_path,\n",
    "        'threshold': f\"s3://{config['s3']['bucket']}/{threshold_s3_key}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tmp:\n",
    "    json.dump(metadata, tmp, indent=2)\n",
    "    tmp.flush()\n",
    "    metadata_s3_key = f\"{config['s3']['prefix']}/metadata/training_{model_version}.json\"\n",
    "    s3_client.upload_file(tmp.name, config['s3']['bucket'], metadata_s3_key)\n",
    "    print(f\"  ✅ Uploaded: s3://{config['s3']['bucket']}/{metadata_s3_key}\")\n",
    "\n",
    "print(\"\\n✅ All artifacts saved to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n📦 Model Version: {model_version}\")\n",
    "print(f\"\\n📊 Cross-Validation Results:\")\n",
    "cv_metrics = training_results.cv_results.mean_metrics\n",
    "cv_std = training_results.cv_results.std_metrics\n",
    "for metric, value in cv_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {cv_std[metric]:.4f}\")\n",
    "\n",
    "print(f\"\\n🎯 Optimal Threshold: {training_results.optimal_threshold:.4f}\")\n",
    "\n",
    "print(f\"\\n📈 Test Set Performance:\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  FNR:       {test_metrics['fnr']:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ PRD Compliance: {prd_report.overall_status.value}\")\n",
    "print(f\"  Critical requirements met: {prd_report.summary['critical_requirements_met']}\")\n",
    "print(f\"  All requirements met: {prd_report.summary['all_requirements_met']}\")\n",
    "\n",
    "print(f\"\\n✅ ONNX Validation: {'PASSED' if onnx_result.passed else 'FAILED'}\")\n",
    "print(f\"  Max difference: {onnx_result.max_difference:.9f}\")\n",
    "print(f\"  Mismatch rate: {onnx_result.mismatch_rate:.2f}%\")\n",
    "\n",
    "print(f\"\\n☁️  S3 Artifacts:\")\n",
    "print(f\"  ONNX model:   {onnx_s3_path}\")\n",
    "print(f\"  XGBoost model: {xgb_s3_path}\")\n",
    "print(f\"  Threshold:     s3://{config['s3']['bucket']}/{threshold_s3_key}\")\n",
    "print(f\"  Metadata:      s3://{config['s3']['bucket']}/{metadata_s3_key}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✅ Next Steps:\")\n",
    "print(\"1. Deploy ONNX model to production inference service\")\n",
    "print(\"2. Configure threshold in service config\")\n",
    "print(\"3. Monitor production metrics\")\n",
    "print(\"4. (Optional) Run notebook 05 for offline replay validation\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
