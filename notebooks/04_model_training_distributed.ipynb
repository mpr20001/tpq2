{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Distributed Model Training\n",
    "\n",
    "**Purpose**: Train XGBoost using distributed Spark (no driver collection)\n",
    "\n",
    "**Pipeline**:\n",
    "1. Load feature datasets as Spark DataFrames\n",
    "2. Prepare Spark ML Vector format\n",
    "3. Train distributed XGBoost (xgboost.spark.SparkXGBClassifier)\n",
    "4. Perform cross-validation with detailed metrics\n",
    "5. Optimize threshold with cost analysis\n",
    "6. Evaluate on test set with confusion matrix\n",
    "7. Check PRD compliance\n",
    "8. Export to ONNX\n",
    "9. Save artifacts to S3\n",
    "\n",
    "**Key Features**:\n",
    "- **No `.collect()`**: All training distributed across Spark cluster\n",
    "- **Detailed analysis**: Cross-validation, cost analysis, confusion matrices\n",
    "- **Scalable**: Handles any dataset size without driver memory limits\n",
    "- **PRD compliance**: Automated requirement validation\n",
    "\n",
    "**CRITICAL DISTRIBUTION STRATEGY**:\n",
    "- Training data: 5:1 ratio (sampled for balanced learning)\n",
    "- Val/Test data: ~36:1 ratio (original production distribution)\n",
    "- scale_pos_weight: Set to training ratio (~5.0), NOT production ratio\n",
    "- Metrics on val/test: Will reflect realistic production performance\n",
    "\n",
    "**Expected Results**:\n",
    "- Precision on val/test will be LOWER (~20-25% vs ~70% on balanced)\n",
    "- Recall should remain high (>95% with proper threshold)\n",
    "- This is EXPECTED and CORRECT for production deployment\n",
    "\n",
    "**Prerequisites**:\n",
    "- Notebook 03 completed (feature datasets available)\n",
    "- xgboost-spark package installed\n",
    "\n",
    "**Duration**: ~30-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Configuration\n",
    "\n",
    "Copy configuration from notebook 00:\n",
    "* Prefer 1 core per executor for barrier stability\n",
    "* Provide headroom: instances > num_workers\n",
    "* Barrier-friendly stability\n",
    "* Give native XGBoost/JNI off-heap room (20G mem -> at least 3G overhead)\n",
    "* Optional: speed re-scheduling on failure\n",
    "*     \"spark.yarn.dist.archives\": \"s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/pyspark_env.tar.gz\",\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:37:46.403134Z",
     "iopub.status.busy": "2025-10-18T04:37:46.403003Z",
     "iopub.status.idle": "2025-10-18T04:37:46.764794Z",
     "shell.execute_reply": "2025-10-18T04:37:46.764407Z",
     "shell.execute_reply.started": "2025-10-18T04:37:46.403120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': './environment/bin/python', 'spark.yarn.dist.archives': 's3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/pyspark_env.tar.gz#environment', 'spark.submit.deployMode': 'cluster', 'spark.dynamicAllocation.enabled': 'false', 'spark.executor.instances': '12', 'spark.speculation': 'false', 'spark.blacklist.enabled': 'true', 'spark.blacklist.application.maxFailedTasksPerExecutor': '1', 'spark.blacklist.application.maxFailedTasksPerNode': '1', 'spark.network.timeout': '600s', 'spark.executor.heartbeatInterval': '60s', 'spark.executor.memoryOverhead': '3072', 'spark.locality.wait': '0s'}, 'driverMemory': '16G', 'executorCores': 1, 'executorMemory': '20G', 'pyFiles': ['s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip', 's3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py'], 'driverCores': 4, 'numExecutors': 12, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3306</td><td>application_1758752217644_219830</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_219830/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-33.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_219830_01_000001/xiao.zhang\">Link</a></td><td>xiao.zhang</td><td></td></tr><tr><td>3350</td><td>application_1758752217644_221612</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_221612/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-33-224.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_221612_01_000016/feifan.jian\">Link</a></td><td>feifan.jian</td><td></td></tr><tr><td>3386</td><td>application_1758752217644_223486</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_223486/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-32-5.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_223486_01_000001/tyerra\">Link</a></td><td>tyerra</td><td></td></tr><tr><td>3391</td><td>application_1758752217644_224113</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_224113/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-114.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_224113_01_000001/mbharti\">Link</a></td><td>mbharti</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"pyFiles\": [\n",
    "    \"s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip\",\n",
    "    \"s3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py\"\n",
    "  ],\n",
    "  \"driverMemory\": \"16G\",\n",
    "  \"driverCores\": 4,\n",
    "\n",
    "  \n",
    "  \"executorMemory\": \"20G\",\n",
    "  \"executorCores\": 1,\n",
    "\n",
    "  \"numExecutors\": 12,\n",
    "  \"conf\": {\n",
    "    \"spark.yarn.dist.archives\": \"s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/pyspark_env.tar.gz#environment\",\n",
    "    \"spark.dynamicAllocation.enabled\": \"false\",\n",
    "    \"spark.executor.instances\": \"12\",\n",
    "\n",
    "    \"spark.speculation\": \"false\",\n",
    "    \"spark.blacklist.enabled\": \"true\",\n",
    "    \"spark.blacklist.application.maxFailedTasksPerExecutor\": \"1\",\n",
    "    \"spark.blacklist.application.maxFailedTasksPerNode\": \"1\",\n",
    "    \"spark.network.timeout\": \"600s\",\n",
    "    \"spark.executor.heartbeatInterval\": \"60s\",\n",
    "\n",
    "    \"spark.executor.memoryOverhead\": \"3072\",\n",
    "\n",
    "    \"spark.locality.wait\": \"0s\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:37:46.765438Z",
     "iopub.status.busy": "2025-10-18T04:37:46.765286Z",
     "iopub.status.idle": "2025-10-18T04:45:50.996816Z",
     "shell.execute_reply": "2025-10-18T04:45:50.996420Z",
     "shell.execute_reply.started": "2025-10-18T04:37:46.765423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3400</td><td>application_1758752217644_225033</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-38-35.us-east-2.compute.internal:20888/proxy/application_1758752217644_225033/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-37-141.us-east-2.compute.internal:8044/node/containerlogs/container_1758752217644_225033_01_000001/pmannem\">Link</a></td><td>pmannem</td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfff4bf04b094814aa69066162d04526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93bf16b1bc1418bab72201fbdf706fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.13 (main, Jul 30 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]\n",
      "PySpark version: 3.5.4-amzn-0\n",
      "? All imports successful\n",
      "? Using distributed XGBoost (no driver collection)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "import onnxruntime\n",
    "import onnxmltools\n",
    "\n",
    "# XGBoost Spark integration\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {spark.version}\")\n",
    "print(\"âœ… All imports successful\")\n",
    "print(\"âœ… Using distributed XGBoost (no driver collection)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:45:50.997499Z",
     "iopub.status.busy": "2025-10-18T04:45:50.997327Z",
     "iopub.status.idle": "2025-10-18T04:45:51.801479Z",
     "shell.execute_reply": "2025-10-18T04:45:51.801014Z",
     "shell.execute_reply.started": "2025-10-18T04:45:50.997484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6858aa341e48cfb2688565b4972333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/config/training_config_latest.yaml\n",
      "? Configuration loaded\n",
      "\n",
      "? Model Configuration:\n",
      "  Algorithm: xgboost\n",
      "  N estimators: 100\n",
      "  Max depth: 6\n",
      "  Learning rate: 0.1\n",
      "  Cost FN:FP = 100.0:1.0\n",
      "\n",
      "? PRD Requirements:\n",
      "  Recall ?0.98\n",
      "  FNR ?0.02\n",
      "  F1 ?0.85\n",
      "  ROC-AUC ?0.9"
     ]
    }
   ],
   "source": [
    "# Download configuration from S3\n",
    "s3_client = boto3.client('s3')\n",
    "s3_bucket = 'uip-datalake-bucket-prod'\n",
    "s3_prefix = 'sf_trino/trino_query_predictor'\n",
    "config_s3_key = f\"{s3_prefix}/config/training_config_latest.yaml\"\n",
    "config_path = '/tmp/training_config.yaml'\n",
    "\n",
    "print(f\"Downloading config: s3://{s3_bucket}/{config_s3_key}\")\n",
    "s3_client.download_file(s3_bucket, config_s3_key, config_path)\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"\\nðŸ“‹ Model Configuration:\")\n",
    "print(f\"  Algorithm: {config['model']['algorithm']}\")\n",
    "print(f\"  N estimators: {config['model']['n_estimators']}\")\n",
    "print(f\"  Max depth: {config['model']['max_depth']}\")\n",
    "print(f\"  Learning rate: {config['model']['learning_rate']}\")\n",
    "print(f\"  Cost FN:FP = {config['model']['cost_fn']}:{config['model']['cost_fp']}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š PRD Requirements:\")\n",
    "print(f\"  Recall â‰¥{config['prd_requirements']['target_heavy_recall']}\")\n",
    "print(f\"  FNR â‰¤{config['prd_requirements']['target_fnr']}\")\n",
    "print(f\"  F1 â‰¥{config['prd_requirements']['target_f1']}\")\n",
    "print(f\"  ROC-AUC â‰¥{config['prd_requirements']['target_roc_auc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Feature Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:45:51.802186Z",
     "iopub.status.busy": "2025-10-18T04:45:51.802039Z",
     "iopub.status.idle": "2025-10-18T04:46:29.503232Z",
     "shell.execute_reply": "2025-10-18T04:46:29.502838Z",
     "shell.execute_reply.started": "2025-10-18T04:45:51.802169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dcf482b85c4bbf96cb677f04833db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature datasets from S3...\n",
      "  Base path: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01\n",
      "\n",
      "? Datasets loaded (as Spark DataFrames):\n",
      "  Train: 8,782,474 queries\n",
      "  Val:   14,969,526 queries\n",
      "  Test:  15,099,750 queries\n",
      "  Features: 345"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "features_path = config['features']['output_path']\n",
    "date_range = f\"{config['data_loading']['start_date']}_to_{config['data_loading']['end_date']}\"\n",
    "base_path = f\"{features_path}/{date_range}\"\n",
    "\n",
    "train_path = f\"{base_path}/train\"\n",
    "val_path = f\"{base_path}/val\"\n",
    "test_path = f\"{base_path}/test\"\n",
    "\n",
    "print(f\"Loading feature datasets from S3...\")\n",
    "print(f\"  Base path: {base_path}\")\n",
    "\n",
    "# Load as Spark DataFrames (NO COLLECT!)\n",
    "train_df = spark.read.parquet(train_path)\n",
    "val_df = spark.read.parquet(val_path)\n",
    "test_df = spark.read.parquet(test_path)\n",
    "\n",
    "# Get counts\n",
    "train_count = train_df.count()\n",
    "val_count = val_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"\\nâœ… Datasets loaded (as Spark DataFrames):\")\n",
    "print(f\"  Train: {train_count:,} queries\")\n",
    "print(f\"  Val:   {val_count:,} queries\")\n",
    "print(f\"  Test:  {test_count:,} queries\")\n",
    "\n",
    "# Check feature dimensions\n",
    "sample = train_df.select('features').first()\n",
    "feature_count = len(sample['features'])\n",
    "print(f\"  Features: {feature_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Spark ML Format\n",
    "\n",
    "**CRITICAL**: Convert to Spark ML Vector format WITHOUT collecting data to driver.\n",
    "\n",
    "This keeps all data distributed across the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:46:29.503882Z",
     "iopub.status.busy": "2025-10-18T04:46:29.503731Z",
     "iopub.status.idle": "2025-10-18T04:46:30.828246Z",
     "shell.execute_reply": "2025-10-18T04:46:30.827852Z",
     "shell.execute_reply.started": "2025-10-18T04:46:29.503867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6154ebae72ec4ff8a0fb0c7296bcb22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Spark ML format (NO .collect()!)...\n",
      "? Spark ML format ready\n",
      "? Data remains distributed (not collected to driver)\n",
      "\n",
      "DataFrame schema:\n",
      "root\n",
      " |-- features_vec: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "print(\"Preparing Spark ML format (NO .collect()!)...\")\n",
    "\n",
    "# Convert array column to Spark ML Vector\n",
    "train_df = train_df.withColumn(\"features_vec\", array_to_vector(F.col(\"features\")))\n",
    "val_df = val_df.withColumn(\"features_vec\", array_to_vector(F.col(\"features\")))\n",
    "test_df = test_df.withColumn(\"features_vec\", array_to_vector(F.col(\"features\")))\n",
    "\n",
    "# Rename label column for Spark ML\n",
    "train_df = train_df.withColumnRenamed(\"is_heavy\", \"label\")\n",
    "val_df = val_df.withColumnRenamed(\"is_heavy\", \"label\")\n",
    "test_df = test_df.withColumnRenamed(\"is_heavy\", \"label\")\n",
    "\n",
    "# Cache for reuse\n",
    "train_df.cache()\n",
    "val_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "print(\"âœ… Spark ML format ready\")\n",
    "print(\"âœ… Data remains distributed (not collected to driver)\")\n",
    "\n",
    "# Verify schema\n",
    "print(\"\\nDataFrame schema:\")\n",
    "train_df.select(\"features_vec\", \"label\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:46:30.828901Z",
     "iopub.status.busy": "2025-10-18T04:46:30.828745Z",
     "iopub.status.idle": "2025-10-18T04:47:28.736115Z",
     "shell.execute_reply": "2025-10-18T04:47:28.735642Z",
     "shell.execute_reply.started": "2025-10-18T04:46:30.828885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2350f3faf643ebb5e80d0f98bdfa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating class weights...\n",
      "\n",
      "? Class Distribution:\n",
      "  Small (0): 7,318,751 (83.33%)\n",
      "  Heavy (1): 1,463,723 (16.67%)\n",
      "  Ratio: 5.00:1 (small:heavy)\n",
      "\n",
      "? XGBoost scale_pos_weight: 5.0001"
     ]
    }
   ],
   "source": [
    "print(\"Calculating class weights...\")\n",
    "\n",
    "# Get class distribution (distributed count)\n",
    "class_dist = train_df.groupBy(\"label\").count().orderBy(\"label\").collect()\n",
    "\n",
    "small_count = class_dist[0]['count']\n",
    "heavy_count = class_dist[1]['count']\n",
    "total_count = small_count + heavy_count\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost\n",
    "scale_pos_weight = small_count / heavy_count\n",
    "\n",
    "print(f\"\\nâœ… Class Distribution:\")\n",
    "print(f\"  Small (0): {small_count:,} ({small_count/total_count*100:.2f}%)\")\n",
    "print(f\"  Heavy (1): {heavy_count:,} ({heavy_count/total_count*100:.2f}%)\")\n",
    "print(f\"  Ratio: {scale_pos_weight:.2f}:1 (small:heavy)\")\n",
    "print(f\"\\nâœ… XGBoost scale_pos_weight: {scale_pos_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:47:28.736828Z",
     "iopub.status.busy": "2025-10-18T04:47:28.736674Z",
     "iopub.status.idle": "2025-10-18T04:49:29.212758Z",
     "shell.execute_reply": "2025-10-18T04:49:29.212340Z",
     "shell.execute_reply.started": "2025-10-18T04:47:28.736812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e860470383c47d4bf34fc5624a97af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying val/test distributions match production...\n",
      "\n",
      "Distribution Verification:\n",
      "  Train: 5.0:1 (sampled for training)\n",
      "  Val:   48.2:1 (production distribution)\n",
      "  Test:  25.0:1 (production distribution)\n",
      "\n",
      "  WARNING: Val ratio 48.2:1 differs from expected 36.0:1\n",
      "  WARNING: Test ratio 25.0:1 differs from expected 36.0:1\n",
      "\n",
      "  Verified: Val/Test use production distribution for realistic evaluation"
     ]
    }
   ],
   "source": [
    "print(\"Verifying val/test distributions match production...\")\n",
    "\n",
    "# Get val distribution\n",
    "val_class_dist = val_df.groupBy(\"label\").count().orderBy(\"label\").collect()\n",
    "val_small = val_class_dist[0]['count']\n",
    "val_heavy = val_class_dist[1]['count']\n",
    "val_ratio = val_small / val_heavy\n",
    "\n",
    "# Get test distribution\n",
    "test_class_dist = test_df.groupBy(\"label\").count().orderBy(\"label\").collect()\n",
    "test_small = test_class_dist[0]['count']\n",
    "test_heavy = test_class_dist[1]['count']\n",
    "test_ratio = test_small / test_heavy\n",
    "\n",
    "print(f\"\\nDistribution Verification:\")\n",
    "print(f\"  Train: {scale_pos_weight:.1f}:1 (sampled for training)\")\n",
    "print(f\"  Val:   {val_ratio:.1f}:1 (production distribution)\")\n",
    "print(f\"  Test:  {test_ratio:.1f}:1 (production distribution)\")\n",
    "\n",
    "# Verify val/test are significantly different from training\n",
    "expected_production_ratio = 36.0\n",
    "tolerance = 10.0  # Allow +/- 10\n",
    "\n",
    "if abs(val_ratio - expected_production_ratio) > tolerance:\n",
    "    print(f\"\\n  WARNING: Val ratio {val_ratio:.1f}:1 differs from expected {expected_production_ratio:.1f}:1\")\n",
    "\n",
    "if abs(test_ratio - expected_production_ratio) > tolerance:\n",
    "    print(f\"  WARNING: Test ratio {test_ratio:.1f}:1 differs from expected {expected_production_ratio:.1f}:1\")\n",
    "\n",
    "if abs(val_ratio - scale_pos_weight) < 2:\n",
    "    print(f\"\\n  ERROR: Val distribution ({val_ratio:.1f}:1) too similar to training ({scale_pos_weight:.1f}:1)!\")\n",
    "    print(f\"  This indicates incorrect pipeline - val/test should use original distribution\")\n",
    "    raise ValueError(\"Val/Test distributions appear to be sampled instead of original!\")\n",
    "\n",
    "print(\"\\n  Verified: Val/Test use production distribution for realistic evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Distributed XGBoost Model\n",
    "\n",
    "Train using `SparkXGBClassifier` which distributes training across Spark executors.\n",
    "\n",
    "No data collection to driver - training happens on the cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:49:29.213437Z",
     "iopub.status.busy": "2025-10-18T04:49:29.213272Z",
     "iopub.status.idle": "2025-10-18T04:49:29.268278Z",
     "shell.execute_reply": "2025-10-18T04:49:29.267917Z",
     "shell.execute_reply.started": "2025-10-18T04:49:29.213420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da022a4af064b5fa5a900b99a749ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def healthy_executor_count(spark):\n",
    "    sc = spark.sparkContext\n",
    "    # 1) Preferred: JVM StatusTracker array (no toArray)\n",
    "    try:\n",
    "        jinfos = sc._jsc.sc().statusTracker().getExecutorInfos()  # Java array\n",
    "        n = 0\n",
    "        for i in range(jinfos.length()):\n",
    "            if jinfos(i).executorId() != \"driver\":\n",
    "                n += 1\n",
    "        if n > 0:\n",
    "            return n\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) Fallback: size of executor memory status map (includes driver)\n",
    "    try:\n",
    "        memStatus = sc._jsc.sc().getExecutorMemoryStatus()\n",
    "        # memStatus.size() includes driver entry; subtract 1 defensively\n",
    "        return max(0, memStatus.size() - 1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 3) Last resort: configured instances\n",
    "    try:\n",
    "        return int(spark.conf.get(\"spark.executor.instances\"))\n",
    "    except Exception:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:49:29.268850Z",
     "iopub.status.busy": "2025-10-18T04:49:29.268716Z",
     "iopub.status.idle": "2025-10-18T04:52:32.253814Z",
     "shell.execute_reply": "2025-10-18T04:52:32.253353Z",
     "shell.execute_reply.started": "2025-10-18T04:49:29.268837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8024af9a3e840c3979ae90d4f4bbcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING DISTRIBUTED XGBOOST\n",
      "======================================================================\n",
      "Using num_workers=10\n",
      "\n",
      "Training configuration:\n",
      "  Workers: 10 (fixed)\n",
      "  Estimators: 100\n",
      "  Max depth: 6\n",
      "  Learning rate: 0.1\n",
      "  Scale pos weight: 5.0001 (TRAINING distribution)\n",
      "\n",
      "IMPORTANT: Using scale_pos_weight=5.0 (training 5:1)\n",
      "           Val/test use ~48.2:1 (production) for evaluation\n",
      "           This is CORRECT - learn from balanced, evaluate on realistic\n",
      "\n",
      "Training on 8,782,474 samples...\n",
      "Note: Training without early stopping due to Spark XGBoost limitation\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE\n",
      "======================================================================\n",
      "Model trained using distributed XGBoost\n",
      "No driver memory collection - all training distributed"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING DISTRIBUTED XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "nw = max(2, min(10, healthy_executor_count(spark) - 1))\n",
    "print(f\"Using num_workers={nw}\")\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "train_df = train_df.repartition(nw).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "_ = train_df.count()  # materialize before barrier stage\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "classifier = SparkXGBClassifier(\n",
    "    num_workers=nw,\n",
    "    features_col=\"features_vec\",\n",
    "    label_col=\"label\",\n",
    "    prediction_col=\"prediction\",\n",
    "    probability_col=\"probability\",\n",
    "    raw_prediction_col=\"rawPrediction\",\n",
    "    n_estimators=config['model']['n_estimators'],\n",
    "    max_depth=config['model']['max_depth'],\n",
    "    learning_rate=config['model']['learning_rate'],\n",
    "    subsample=config['model'].get('subsample', 0.8),\n",
    "    colsample_bytree=config['model'].get('colsample_bytree', 0.8),\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric=\"aucpr\",\n",
    "    random_state=42,\n",
    "    verbosity=1,\n",
    "    timeout_request_workers=180  # give cluster time to allocate\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Workers: {nw} (fixed)\")\n",
    "print(f\"  Estimators: {config['model']['n_estimators']}\")\n",
    "print(f\"  Max depth: {config['model']['max_depth']}\")\n",
    "print(f\"  Learning rate: {config['model']['learning_rate']}\")\n",
    "print(f\"  Scale pos weight: {scale_pos_weight:.4f} (TRAINING distribution)\")\n",
    "\n",
    "print(f\"\\nIMPORTANT: Using scale_pos_weight={scale_pos_weight:.1f} (training 5:1)\")\n",
    "print(f\"           Val/test use ~{val_ratio:.1f}:1 (production) for evaluation\")\n",
    "print(f\"           This is CORRECT - learn from balanced, evaluate on realistic\")\n",
    "\n",
    "print(f\"\\nTraining on {train_count:,} samples...\")\n",
    "print(\"Note: Training without early stopping due to Spark XGBoost limitation\")\n",
    "\n",
    "# Train model on training dataset only\n",
    "model = classifier.fit(train_df.select(\"features_vec\", \"label\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Model trained using distributed XGBoost\")\n",
    "print(\"No driver memory collection - all training distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validation Set Analysis\n",
    "\n",
    "Evaluate model performance on validation set with detailed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:52:32.254528Z",
     "iopub.status.busy": "2025-10-18T04:52:32.254356Z",
     "iopub.status.idle": "2025-10-18T04:53:30.133774Z",
     "shell.execute_reply": "2025-10-18T04:53:30.133253Z",
     "shell.execute_reply.started": "2025-10-18T04:52:32.254512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6aad2575d949f7a9319847f6e57a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION SET ANALYSIS (PRODUCTION DISTRIBUTION)\n",
      "======================================================================\n",
      "\n",
      "IMPORTANT: Validation set uses PRODUCTION distribution (~48.2:1)\n",
      "           Training used SAMPLED distribution (5.0:1)\n",
      "           Metrics reflect realistic production performance\n",
      "======================================================================\n",
      "\n",
      "Validation Performance (Default Threshold 0.5):\n",
      "  Samples:   14,969,526\n",
      "  Recall:    0.9112\n",
      "  Precision: 0.2424\n",
      "  F1-Score:  0.3829\n",
      "  ROC-AUC:   0.9378\n",
      "  FNR:       0.0888\n",
      "  FPR:       0.0590\n",
      "\n",
      "Confusion Matrix (Validation):\n",
      "                 Predicted\n",
      "               Small   Heavy\n",
      "Actual Small   13,799,483    865,994\n",
      "       Heavy   26,987     277,062\n",
      "\n",
      "NOTE: Precision is lower on production distribution (expected)\n",
      "      More small queries means more false positives in absolute terms\n",
      "      This reflects realistic production trade-offs"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"VALIDATION SET ANALYSIS (PRODUCTION DISTRIBUTION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nIMPORTANT: Validation set uses PRODUCTION distribution (~{val_ratio:.1f}:1)\")\n",
    "print(f\"           Training used SAMPLED distribution ({scale_pos_weight:.1f}:1)\")\n",
    "print(f\"           Metrics reflect realistic production performance\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Transform validation set (distributed)\n",
    "val_predictions = model.transform(val_df)\n",
    "\n",
    "# Evaluate with Spark ML evaluators\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "roc_auc = binary_evaluator.evaluate(val_predictions)\n",
    "\n",
    "# Get confusion matrix elements (small aggregation)\n",
    "val_metrics_df = val_predictions.groupBy(\"label\", \"prediction\").count()\n",
    "val_metrics_list = val_metrics_df.collect()\n",
    "\n",
    "# Parse confusion matrix\n",
    "tn = fp = fn = tp = 0\n",
    "for row in val_metrics_list:\n",
    "    if row['label'] == 0 and row['prediction'] == 0.0:\n",
    "        tn = row['count']\n",
    "    elif row['label'] == 0 and row['prediction'] == 1.0:\n",
    "        fp = row['count']\n",
    "    elif row['label'] == 1 and row['prediction'] == 0.0:\n",
    "        fn = row['count']\n",
    "    elif row['label'] == 1 and row['prediction'] == 1.0:\n",
    "        tp = row['count']\n",
    "\n",
    "# Calculate metrics\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(f\"\\nValidation Performance (Default Threshold 0.5):\")\n",
    "print(f\"  Samples:   {val_count:,}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"  FNR:       {fnr:.4f}\")\n",
    "print(f\"  FPR:       {fpr:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix (Validation):\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"               Small   Heavy\")\n",
    "print(f\"Actual Small   {tn:,}    {fp:,}\")\n",
    "print(f\"       Heavy   {fn:,}     {tp:,}\")\n",
    "\n",
    "print(f\"\\nNOTE: Precision is lower on production distribution (expected)\")\n",
    "print(f\"      More small queries means more false positives in absolute terms\")\n",
    "print(f\"      This reflects realistic production trade-offs\")\n",
    "\n",
    "# Store for later\n",
    "val_metrics = {\n",
    "    'recall': recall, 'precision': precision, 'f1': f1,\n",
    "    'roc_auc': roc_auc, 'fnr': fnr, 'fpr': fpr,\n",
    "    'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:53:30.134707Z",
     "iopub.status.busy": "2025-10-18T04:53:30.134474Z",
     "iopub.status.idle": "2025-10-18T04:54:13.875232Z",
     "shell.execute_reply": "2025-10-18T04:54:13.874828Z",
     "shell.execute_reply.started": "2025-10-18T04:53:30.134687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d91d365d684b7e9e9a8e465842cba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION SET ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Validation Performance (Default Threshold 0.5):\n",
      "  Samples:   14,969,526\n",
      "  Recall:    0.9112\n",
      "  Precision: 0.2424\n",
      "  F1-Score:  0.3829\n",
      "  ROC-AUC:   0.9378\n",
      "  FNR:       0.0888\n",
      "  FPR:       0.0590\n",
      "\n",
      "Confusion Matrix (Validation):\n",
      "                 Predicted\n",
      "               Small   Heavy\n",
      "Actual Small   13,799,483    865,994\n",
      "       Heavy   26,987     277,062"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"VALIDATION SET ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Transform validation set (distributed)\n",
    "val_predictions = model.transform(val_df)\n",
    "\n",
    "# Evaluate with Spark ML evaluators\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "roc_auc = binary_evaluator.evaluate(val_predictions)\n",
    "\n",
    "# Get confusion matrix elements (small aggregation)\n",
    "val_metrics_df = val_predictions.groupBy(\"label\", \"prediction\").count()\n",
    "val_metrics_list = val_metrics_df.collect()\n",
    "\n",
    "# Parse confusion matrix\n",
    "tn = fp = fn = tp = 0\n",
    "for row in val_metrics_list:\n",
    "    if row['label'] == 0 and row['prediction'] == 0.0:\n",
    "        tn = row['count']\n",
    "    elif row['label'] == 0 and row['prediction'] == 1.0:\n",
    "        fp = row['count']\n",
    "    elif row['label'] == 1 and row['prediction'] == 0.0:\n",
    "        fn = row['count']\n",
    "    elif row['label'] == 1 and row['prediction'] == 1.0:\n",
    "        tp = row['count']\n",
    "\n",
    "# Calculate metrics\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(f\"\\nValidation Performance (Default Threshold 0.5):\")\n",
    "print(f\"  Samples:   {val_count:,}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"  FNR:       {fnr:.4f}\")\n",
    "print(f\"  FPR:       {fpr:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix (Validation):\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"               Small   Heavy\")\n",
    "print(f\"Actual Small   {tn:,}    {fp:,}\")\n",
    "print(f\"       Heavy   {fn:,}     {tp:,}\")\n",
    "\n",
    "# Store for later\n",
    "val_metrics = {\n",
    "    'recall': recall, 'precision': precision, 'f1': f1,\n",
    "    'roc_auc': roc_auc, 'fnr': fnr, 'fpr': fpr,\n",
    "    'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Threshold Optimization with Cost Analysis\n",
    "\n",
    "Optimize classification threshold using cost-sensitive approach:\n",
    "- **False Negative Cost**: 100 (missing heavy query causes system issues)\n",
    "- **False Positive Cost**: 1 (routing small query to heavy cluster wastes resources)\n",
    "\n",
    "For this step, we sample a small subset for threshold optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:54:13.875930Z",
     "iopub.status.busy": "2025-10-18T04:54:13.875785Z",
     "iopub.status.idle": "2025-10-18T04:54:35.383677Z",
     "shell.execute_reply": "2025-10-18T04:54:35.383229Z",
     "shell.execute_reply.started": "2025-10-18T04:54:13.875916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecf560c48e64e73b8c23ccec2d9c973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "THRESHOLD OPTIMIZATION WITH COST ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Cost Configuration:\n",
      "  False Negative Cost: 100 (missing heavy query)\n",
      "  False Positive Cost: 1 (routing small query to heavy cluster)\n",
      "  Ratio: 100:1\n",
      "\n",
      "Sampling 0.7% of validation set for threshold search...\n",
      "Sampled 100,087 validation examples\n",
      "\n",
      "Searching for optimal threshold...\n",
      "\n",
      "? Optimal Threshold Found: 0.200\n",
      "\n",
      "Optimal Threshold Metrics (on validation sample):\n",
      "  Recall:     0.9120\n",
      "  Precision:  0.2173\n",
      "  F1-Score:   0.3509\n",
      "  Total Cost: 24,579\n",
      "\n",
      "Cost Breakdown:\n",
      "  FN: 179 ? 100 = 17,900\n",
      "  FP: 6679 ? 1 = 6,679\n",
      "\n",
      "Comparison to Default (?0.50) Threshold:\n",
      "  Cost reduction: 5.2%\n",
      "  Recall change: +1.1%\n",
      "  Precision change: -2.2%\n",
      "\n",
      "Threshold Sensitivity Analysis (+/-0.05 around 0.200):\n",
      "  Threshold  Recall  Precision  F1-Score     Cost\n",
      "  0.160      0.912   0.211      0.343       24,826\n",
      "  0.180      0.912   0.216      0.349       24,641\n",
      "  0.200 *    0.912   0.217      0.351       24,579\n",
      "  0.220      0.910   0.218      0.352       24,834\n",
      "  0.240      0.909   0.220      0.354       24,956"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# THRESHOLD OPTIMIZATION WITH COST ANALYSIS (robust formatting)\n",
    "# ======================================================================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"THRESHOLD OPTIMIZATION WITH COST ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Cost parameters ---\n",
    "cost_fn = int(config['model']['cost_fn'])  # e.g., 100\n",
    "cost_fp = int(config['model']['cost_fp'])  # e.g., 1\n",
    "\n",
    "print(f\"\\nCost Configuration:\")\n",
    "print(f\"  False Negative Cost: {cost_fn} (missing heavy query)\")\n",
    "print(f\"  False Positive Cost: {cost_fp} (routing small query to heavy cluster)\")\n",
    "print(f\"  Ratio: {cost_fn}:{cost_fp}\")\n",
    "\n",
    "# --- Sample validation set for threshold optimization (cap to ~100k rows) ---\n",
    "try:\n",
    "    _val_count = val_count\n",
    "except NameError:\n",
    "    _val_count = val_predictions.count()\n",
    "\n",
    "target = 100_000\n",
    "sample_fraction = min(target / max(_val_count, 1), 1.0)\n",
    "print(f\"\\nSampling {sample_fraction*100:.1f}% of validation set for threshold search...\")\n",
    "\n",
    "val_sample = val_predictions.sample(sample_fraction, seed=42)\n",
    "\n",
    "# --- Extract positive class probability & label ---\n",
    "prob_arr = vector_to_array(F.col(\"probability\"))\n",
    "val_probs_df = (\n",
    "    val_sample\n",
    "    .select(\n",
    "        prob_arr.getItem(1).alias(\"prob_heavy\"),            # p(class=1)\n",
    "        F.col(\"label\").cast(\"int\").alias(\"label\")\n",
    "    )\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "print(f\"Sampled {len(val_probs_df):,} validation examples\")\n",
    "if len(val_probs_df) == 0:\n",
    "    raise RuntimeError(\"Validation sample is empty; check val_predictions pipeline and sampling.\")\n",
    "\n",
    "# --- Search thresholds ---\n",
    "thresholds = np.arange(0.10, 0.90, 0.02)\n",
    "threshold_results = []\n",
    "\n",
    "print(\"\\nSearching for optimal threshold...\")\n",
    "labels_np = val_probs_df['label'].to_numpy(dtype=np.int32)\n",
    "probs_np  = val_probs_df['prob_heavy'].to_numpy(dtype=np.float64)\n",
    "\n",
    "for thr in thresholds:\n",
    "    preds = (probs_np >= thr).astype(np.int32)\n",
    "\n",
    "    tp = int(((preds == 1) & (labels_np == 1)).sum())\n",
    "    fp = int(((preds == 1) & (labels_np == 0)).sum())\n",
    "    fn = int(((preds == 0) & (labels_np == 1)).sum())\n",
    "    tn = int(((preds == 0) & (labels_np == 0)).sum())\n",
    "\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    cost = fn * cost_fn + fp * cost_fp  # may be float in pandas; treat as numeric\n",
    "\n",
    "    threshold_results.append({\n",
    "        \"threshold\": float(thr),\n",
    "        \"recall\": float(recall),\n",
    "        \"precision\": float(precision),\n",
    "        \"f1\": float(f1),\n",
    "        \"cost\": float(cost),   # keep as float to avoid dtype surprises\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "# --- Best threshold by minimum cost ---\n",
    "best_idx = int(results_df[\"cost\"].idxmin())\n",
    "best = results_df.iloc[best_idx]\n",
    "best_threshold = float(best[\"threshold\"])\n",
    "\n",
    "print(f\"\\nâœ… Optimal Threshold Found: {best_threshold:.3f}\")\n",
    "print(f\"\\nOptimal Threshold Metrics (on validation sample):\")\n",
    "print(f\"  Recall:     {best['recall']:.4f}\")\n",
    "print(f\"  Precision:  {best['precision']:.4f}\")\n",
    "print(f\"  F1-Score:   {best['f1']:.4f}\")\n",
    "print(f\"  Total Cost: {best['cost']:,.0f}\")  # float-friendly formatting\n",
    "\n",
    "# --- Cost breakdown ---\n",
    "print(f\"\\nCost Breakdown:\")\n",
    "print(f\"  FN: {best['fn']:.0f} Ã— {cost_fn} = {best['fn'] * cost_fn:,.0f}\")\n",
    "print(f\"  FP: {best['fp']:.0f} Ã— {cost_fp} = {best['fp'] * cost_fp:,.0f}\")\n",
    "\n",
    "# --- Compare to default threshold (â‰ˆ0.5): use nearest available threshold ---\n",
    "nearest_05_idx = int((results_df[\"threshold\"] - 0.5).abs().idxmin())\n",
    "default = results_df.iloc[nearest_05_idx]\n",
    "\n",
    "print(f\"\\nComparison to Default (â‰ˆ0.50) Threshold:\")\n",
    "if default[\"cost\"] > 0:\n",
    "    reduction = (default[\"cost\"] - best[\"cost\"]) / default[\"cost\"] * 100.0\n",
    "else:\n",
    "    reduction = 0.0\n",
    "print(f\"  Cost reduction: {reduction:.1f}%\")\n",
    "print(f\"  Recall change: {(best['recall'] - default['recall']) * 100:+.1f}%\")\n",
    "print(f\"  Precision change: {(best['precision'] - default['precision']) * 100:+.1f}%\")\n",
    "\n",
    "# --- Sensitivity window around best threshold ---\n",
    "window = 0.05\n",
    "nearby = results_df[\n",
    "    (results_df[\"threshold\"] >= best_threshold - window) &\n",
    "    (results_df[\"threshold\"] <= best_threshold + window)\n",
    "].sort_values(\"threshold\")\n",
    "\n",
    "print(f\"\\nThreshold Sensitivity Analysis (+/-{window:.2f} around {best_threshold:.3f}):\")\n",
    "print(\"  Threshold  Recall  Precision  F1-Score     Cost\")\n",
    "for _, row in nearby.iterrows():\n",
    "    mark = \" *\" if abs(row[\"threshold\"] - best_threshold) < 1e-9 else \"  \"\n",
    "    # Use float-friendly thousands formatting (no ':d')\n",
    "    print(f\"  {row['threshold']:.3f}{mark}    {row['recall']:.3f}   {row['precision']:.3f}      {row['f1']:.3f}   {row['cost']:>10,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Set Evaluation\n",
    "\n",
    "Evaluate final model on held-out test set using optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:54:35.384472Z",
     "iopub.status.busy": "2025-10-18T04:54:35.384293Z",
     "iopub.status.idle": "2025-10-18T04:55:32.007515Z",
     "shell.execute_reply": "2025-10-18T04:55:32.007101Z",
     "shell.execute_reply.started": "2025-10-18T04:54:35.384456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c497f2df5f40639305a81ca0a32a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST SET EVALUATION (PRODUCTION DISTRIBUTION)\n",
      "======================================================================\n",
      "\n",
      "IMPORTANT: Test set uses PRODUCTION distribution (~25.0:1)\n",
      "           Metrics reflect realistic production performance\n",
      "======================================================================\n",
      "\n",
      "Test Set Performance (Threshold: 0.200):\n",
      "  Samples:   15,099,750\n",
      "  Recall:    0.6012\n",
      "  Precision: 0.2386\n",
      "  F1-Score:  0.3417\n",
      "  ROC-AUC:   0.7409\n",
      "  FNR:       0.3988\n",
      "  FPR:       0.0768\n",
      "\n",
      "Test Confusion Matrix:\n",
      "                 Predicted\n",
      "               Small       Heavy\n",
      "Actual Small   13,402,534    1,115,620\n",
      "       Heavy   231,915     349,681\n",
      "\n",
      "Test Set Cost: 24,307,120\n",
      "  FN Cost: 231,915 ? 100 = 23,191,500\n",
      "  FP Cost: 1,115,620 ? 1 = 1,115,620\n",
      "\n",
      "PRODUCTION REALITY CHECK:\n",
      "  - Test uses 25.0:1 distribution (production)\n",
      "  - Precision 23.9% reflects real false positive rate\n",
      "  - In production: expect 1,115,620 false positives per 15,099,750 queries\n",
      "  - This is REALISTIC and EXPECTED"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# TEST SET EVALUATION (PRODUCTION DISTRIBUTION) â€” VectorUDT-safe\n",
    "# ======================================================================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST SET EVALUATION (PRODUCTION DISTRIBUTION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nIMPORTANT: Test set uses PRODUCTION distribution (~{test_ratio:.1f}:1)\")\n",
    "print(f\"           Metrics reflect realistic production performance\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1) Transform test set\n",
    "test_predictions = model.transform(test_df)\n",
    "\n",
    "# 2) Extract p(class=1) from VectorUDT probability\n",
    "#    Spark binary classifier outputs [p(class0), p(class1)]\n",
    "prob_arr_col = vector_to_array(F.col(\"probability\"))\n",
    "test_predictions = test_predictions.withColumn(\"prob_heavy\", prob_arr_col.getItem(1))\n",
    "\n",
    "# 3) Apply optimal threshold\n",
    "test_predictions = test_predictions.withColumn(\n",
    "    \"prediction_optimal\",\n",
    "    F.when(F.col(\"prob_heavy\") >= F.lit(best_threshold), F.lit(1.0)).otherwise(F.lit(0.0))\n",
    ")\n",
    "\n",
    "# 4) Confusion matrix counts\n",
    "#    Ensure label is numeric 0/1\n",
    "cm_df = (\n",
    "    test_predictions\n",
    "    .withColumn(\"label_i\", F.col(\"label\").cast(\"int\"))\n",
    "    .groupBy(\"label_i\", \"prediction_optimal\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Pull counts into local vars\n",
    "tn_test = fp_test = fn_test = tp_test = 0\n",
    "for r in cm_df.collect():\n",
    "    lbl = int(r[\"label_i\"])\n",
    "    pred = float(r[\"prediction_optimal\"])\n",
    "    c = int(r[\"count\"])\n",
    "    if lbl == 0 and pred == 0.0: tn_test = c\n",
    "    elif lbl == 0 and pred == 1.0: fp_test = c\n",
    "    elif lbl == 1 and pred == 0.0: fn_test = c\n",
    "    elif lbl == 1 and pred == 1.0: tp_test = c\n",
    "\n",
    "# 5) ROC-AUC on model's default score (unchanged by our threshold)\n",
    "#    Assumes you created binary_evaluator earlier with rawPredictionCol=\"rawPrediction\" or probabilityCol=\"probability\"\n",
    "test_roc_auc = binary_evaluator.evaluate(test_predictions)\n",
    "\n",
    "# 6) Metrics\n",
    "recall_test    = tp_test / (tp_test + fn_test) if (tp_test + fn_test) > 0 else 0.0\n",
    "precision_test = tp_test / (tp_test + fp_test) if (tp_test + fp_test) > 0 else 0.0\n",
    "f1_test        = (2 * precision_test * recall_test / (precision_test + recall_test)) if (precision_test + recall_test) > 0 else 0.0\n",
    "fnr_test       = fn_test / (fn_test + tp_test) if (fn_test + tp_test) > 0 else 0.0\n",
    "fpr_test       = fp_test / (fp_test + tn_test) if (fp_test + tn_test) > 0 else 0.0\n",
    "\n",
    "# 7) Pretty print\n",
    "print(f\"\\nTest Set Performance (Threshold: {best_threshold:.3f}):\")\n",
    "print(f\"  Samples:   {test_count:,}\")\n",
    "print(f\"  Recall:    {recall_test:.4f}\")\n",
    "print(f\"  Precision: {precision_test:.4f}\")\n",
    "print(f\"  F1-Score:  {f1_test:.4f}\")\n",
    "print(f\"  ROC-AUC:   {test_roc_auc:.4f}\")\n",
    "print(f\"  FNR:       {fnr_test:.4f}\")\n",
    "print(f\"  FPR:       {fpr_test:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Confusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"               Small       Heavy\")\n",
    "print(f\"Actual Small   {tn_test:,}    {fp_test:,}\")\n",
    "print(f\"       Heavy   {fn_test:,}     {tp_test:,}\")\n",
    "\n",
    "# 8) Cost using your config weights\n",
    "#    Assumes cost_fn and cost_fp already defined (ints)\n",
    "test_cost = fn_test * cost_fn + fp_test * cost_fp\n",
    "print(f\"\\nTest Set Cost: {test_cost:,}\")\n",
    "print(f\"  FN Cost: {fn_test:,} Ã— {cost_fn} = {fn_test * cost_fn:,}\")\n",
    "print(f\"  FP Cost: {fp_test:,} Ã— {cost_fp} = {fp_test * cost_fp:,}\")\n",
    "\n",
    "print(f\"\\nPRODUCTION REALITY CHECK:\")\n",
    "print(f\"  - Test uses {test_ratio:.1f}:1 distribution (production)\")\n",
    "print(f\"  - Precision {precision_test:.1%} reflects real false positive rate\")\n",
    "print(f\"  - In production: expect {fp_test:,} false positives per {test_count:,} queries\")\n",
    "print(f\"  - This is REALISTIC and EXPECTED\")\n",
    "\n",
    "# 9) Persist metrics for later use\n",
    "test_metrics = {\n",
    "    \"threshold\": float(best_threshold),\n",
    "    \"recall\": float(recall_test),\n",
    "    \"precision\": float(precision_test),\n",
    "    \"f1\": float(f1_test),\n",
    "    \"roc_auc\": float(test_roc_auc),\n",
    "    \"fnr\": float(fnr_test),\n",
    "    \"fpr\": float(fpr_test),\n",
    "    \"tn\": int(tn_test), \"fp\": int(fp_test), \"fn\": int(fn_test), \"tp\": int(tp_test),\n",
    "    \"cost\": int(test_cost)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. PRD Compliance Check\n",
    "\n",
    "Validate model against Product Requirements Document specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:55:32.008171Z",
     "iopub.status.busy": "2025-10-18T04:55:32.008018Z",
     "iopub.status.idle": "2025-10-18T04:55:32.068912Z",
     "shell.execute_reply": "2025-10-18T04:55:32.068511Z",
     "shell.execute_reply.started": "2025-10-18T04:55:32.008156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add2e5ccdda3422881200e5473e90cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRD COMPLIANCE CHECK\n",
      "======================================================================\n",
      "\n",
      "Requirement Analysis:\n",
      "  Heavy Recall ?0.98: ? FAIL (0.6012)\n",
      "  FNR ?0.02: ? FAIL (0.3988)\n",
      "  F1-Score ?0.85: ? FAIL (0.3417)\n",
      "  ROC-AUC ?0.9: ? FAIL (0.7409)\n",
      "\n",
      "======================================================================\n",
      "? PRD REQUIREMENTS NOT MET\n",
      "??  Model requires review before deployment\n",
      "\n",
      "Considerations:\n",
      "  - Hyperparameter tuning\n",
      "  - Feature engineering improvements\n",
      "  - Threshold adjustment\n",
      "  - Training data augmentation\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PRD COMPLIANCE CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check each requirement\n",
    "prd_results = {\n",
    "    'heavy_recall': recall_test >= config['prd_requirements']['target_heavy_recall'],\n",
    "    'max_fnr': fnr_test <= config['prd_requirements']['target_fnr'],\n",
    "    'f1_score': f1_test >= config['prd_requirements']['target_f1'],\n",
    "    'roc_auc': test_roc_auc >= config['prd_requirements']['target_roc_auc']\n",
    "}\n",
    "\n",
    "print(f\"\\nRequirement Analysis:\")\n",
    "print(f\"  Heavy Recall â‰¥{config['prd_requirements']['target_heavy_recall']}: \", end=\"\")\n",
    "print(f\"{'âœ… PASS' if prd_results['heavy_recall'] else 'âŒ FAIL'} ({recall_test:.4f})\")\n",
    "\n",
    "print(f\"  FNR â‰¤{config['prd_requirements']['target_fnr']}: \", end=\"\")\n",
    "print(f\"{'âœ… PASS' if prd_results['max_fnr'] else 'âŒ FAIL'} ({fnr_test:.4f})\")\n",
    "\n",
    "print(f\"  F1-Score â‰¥{config['prd_requirements']['target_f1']}: \", end=\"\")\n",
    "print(f\"{'âœ… PASS' if prd_results['f1_score'] else 'âŒ FAIL'} ({f1_test:.4f})\")\n",
    "\n",
    "print(f\"  ROC-AUC â‰¥{config['prd_requirements']['target_roc_auc']}: \", end=\"\")\n",
    "print(f\"{'âœ… PASS' if prd_results['roc_auc'] else 'âŒ FAIL'} ({test_roc_auc:.4f})\")\n",
    "\n",
    "all_pass = all(prd_results.values())\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if all_pass:\n",
    "    print(\"âœ… ALL PRD REQUIREMENTS MET\")\n",
    "    print(\"âœ… Model ready for production deployment\")\n",
    "else:\n",
    "    print(\"âŒ PRD REQUIREMENTS NOT MET\")\n",
    "    print(\"âš ï¸  Model requires review before deployment\")\n",
    "    print(\"\\nConsiderations:\")\n",
    "    print(\"  - Hyperparameter tuning\")\n",
    "    print(\"  - Feature engineering improvements\")\n",
    "    print(\"  - Threshold adjustment\")\n",
    "    print(\"  - Training data augmentation\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export to ONNX\n",
    "\n",
    "Convert model to ONNX format for production inference.\n",
    "\n",
    "Note: For distributed XGBoost, we extract the underlying booster and convert to sklearn format first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:55:32.069548Z",
     "iopub.status.busy": "2025-10-18T04:55:32.069402Z",
     "iopub.status.idle": "2025-10-18T04:55:39.449365Z",
     "shell.execute_reply": "2025-10-18T04:55:39.448946Z",
     "shell.execute_reply.started": "2025-10-18T04:55:32.069535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717de41eea614011ac474dd666b8e4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ONNX EXPORT\n",
      "======================================================================\n",
      "\n",
      "Extracting sklearn-compatible model from Spark XGBoost...\n",
      "Sampled 956 test examples for ONNX validation\n",
      "\n",
      "Exporting to ONNX format...\n",
      "  Input size: 345 features\n",
      "  Output path: /tmp/model_v20251018_045539.onnx\n",
      "\n",
      "? ONNX export failed: No module named 'onnxconverter_common'\n",
      "\n",
      "Note: Install onnxmltools for ONNX export:\n",
      "  pip install onnxmltools onnxruntime"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ONNX EXPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nExtracting sklearn-compatible model from Spark XGBoost...\")\n",
    "\n",
    "# Get underlying XGBoost booster\n",
    "xgb_model = model.get_booster()\n",
    "\n",
    "# Sample small test set for ONNX validation\n",
    "onnx_sample_size = min(1000, test_count)\n",
    "test_sample = test_df.sample(onnx_sample_size / test_count, seed=42).select(\"features\").toPandas()\n",
    "X_sample = np.vstack(test_sample['features'].values).astype(np.float32)\n",
    "\n",
    "print(f\"Sampled {X_sample.shape[0]:,} test examples for ONNX validation\")\n",
    "\n",
    "# Define ONNX path\n",
    "model_version = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "onnx_path = f\"/tmp/model_v{model_version}.onnx\"\n",
    "\n",
    "print(f\"\\nExporting to ONNX format...\")\n",
    "print(f\"  Input size: {X_sample.shape[1]:,} features\")\n",
    "print(f\"  Output path: {onnx_path}\")\n",
    "\n",
    "# Export using onnxmltools\n",
    "try:\n",
    "    import onnxmltools\n",
    "    from onnxmltools.convert import convert_xgboost\n",
    "    from onnxconverter_common import FloatTensorType\n",
    "    \n",
    "    # Define input type\n",
    "    initial_type = [('float_input', FloatTensorType([None, X_sample.shape[1]]))]\n",
    "    \n",
    "    # Convert to ONNX\n",
    "    onnx_model = convert_xgboost(xgb_model, initial_types=initial_type)\n",
    "    \n",
    "    # Save ONNX model\n",
    "    with open(onnx_path, \"wb\") as f:\n",
    "        f.write(onnx_model.SerializeToString())\n",
    "    \n",
    "    print(f\"\\nâœ… ONNX export successful\")\n",
    "    \n",
    "    # Validate ONNX predictions\n",
    "    import onnxruntime as rt\n",
    "    sess = rt.InferenceSession(onnx_path)\n",
    "    \n",
    "    # Get ONNX predictions\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    label_name = sess.get_outputs()[0].name\n",
    "    onnx_pred = sess.run([label_name], {input_name: X_sample})[0]\n",
    "    \n",
    "    # Get XGBoost predictions\n",
    "    import xgboost as xgb\n",
    "    dmatrix = xgb.DMatrix(X_sample)\n",
    "    xgb_pred = xgb_model.predict(dmatrix)\n",
    "    \n",
    "    # Compare\n",
    "    max_diff = np.max(np.abs(onnx_pred.flatten() - xgb_pred))\n",
    "    mean_diff = np.mean(np.abs(onnx_pred.flatten() - xgb_pred))\n",
    "    \n",
    "    print(f\"\\nONNX Validation:\")\n",
    "    print(f\"  Samples tested: {X_sample.shape[0]:,}\")\n",
    "    print(f\"  Max difference: {max_diff:.9f}\")\n",
    "    print(f\"  Mean difference: {mean_diff:.9f}\")\n",
    "    \n",
    "    if max_diff < 1e-5:\n",
    "        print(f\"  Status: âœ… PASS (predictions match)\")\n",
    "        onnx_export_success = True\n",
    "    else:\n",
    "        print(f\"  Status: âš ï¸  WARNING (predictions differ)\")\n",
    "        onnx_export_success = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ONNX export failed: {e}\")\n",
    "    onnx_export_success = False\n",
    "    print(\"\\nNote: Install onnxmltools for ONNX export:\")\n",
    "    print(\"  pip install onnxmltools onnxruntime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:55:39.450053Z",
     "iopub.status.busy": "2025-10-18T04:55:39.449894Z",
     "iopub.status.idle": "2025-10-18T04:55:39.514523Z",
     "shell.execute_reply": "2025-10-18T04:55:39.514055Z",
     "shell.execute_reply.started": "2025-10-18T04:55:39.450038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7293460e63e2415eab5a632904e2a6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import onnxruntime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model Artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:55:39.515319Z",
     "iopub.status.busy": "2025-10-18T04:55:39.515155Z",
     "iopub.status.idle": "2025-10-18T04:55:39.802450Z",
     "shell.execute_reply": "2025-10-18T04:55:39.802016Z",
     "shell.execute_reply.started": "2025-10-18T04:55:39.515305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e43c50583b4fdc9b3fabbaf0a7da5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'best_metrics' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'best_metrics' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_summary = f\"\"\"\n",
    "{'='*70}\n",
    "DISTRIBUTED MODEL TRAINING SUMMARY\n",
    "{'='*70}\n",
    "\n",
    "Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Model Version: {model_version}\n",
    "\n",
    "ARCHITECTURE:\n",
    "- Algorithm: XGBoost (Distributed Spark)\n",
    "- Training Approach: NO driver collection (fully distributed)\n",
    "- Features: {feature_count:,}\n",
    "- Training samples: {train_count:,}\n",
    "- Validation samples: {val_count:,}\n",
    "- Test samples: {test_count:,}\n",
    "\n",
    "CRITICAL: CLASS DISTRIBUTION STRATEGY\n",
    "{'='*70}\n",
    "Training Distribution:     {scale_pos_weight:.1f}:1 (Small:Heavy) - SAMPLED\n",
    "Validation Distribution:   {val_ratio:.1f}:1 (Small:Heavy) - PRODUCTION\n",
    "Test Distribution:         {test_ratio:.1f}:1 (Small:Heavy) - PRODUCTION\n",
    "\n",
    "Strategy: Train on balanced (5:1), evaluate on realistic (~36:1)\n",
    "Purpose:  Learn from balanced examples, measure realistic performance\n",
    "Result:   Metrics reflect actual production trade-offs\n",
    "\n",
    "VALIDATION PERFORMANCE (Default Threshold 0.5):\n",
    "- Recall:    {val_metrics['recall']:.4f}\n",
    "- Precision: {val_metrics['precision']:.4f} (lower due to production distribution)\n",
    "- F1-Score:  {val_metrics['f1']:.4f}\n",
    "- ROC-AUC:   {val_metrics['roc_auc']:.4f}\n",
    "- FNR:       {val_metrics['fnr']:.4f}\n",
    "\n",
    "THRESHOLD OPTIMIZATION:\n",
    "- Optimal Threshold: {best_threshold:.3f}\n",
    "- Cost Function: {cost_fn}:{cost_fp} (FN:FP ratio)\n",
    "- Validation Cost: {best_metrics['cost']:,}\n",
    "- Test Cost: {test_cost:,}\n",
    "\n",
    "TEST SET PERFORMANCE (Optimal Threshold - PRODUCTION DISTRIBUTION):\n",
    "- Recall:    {recall_test:.4f}\n",
    "- Precision: {precision_test:.4f}\n",
    "- F1-Score:  {f1_test:.4f}\n",
    "- ROC-AUC:   {test_roc_auc:.4f}\n",
    "- FNR:       {fnr_test:.4f}\n",
    "- FPR:       {fpr_test:.4f}\n",
    "\n",
    "CONFUSION MATRIX (Test Set - Production {test_ratio:.1f}:1):\n",
    "                 Predicted\n",
    "               Small   Heavy\n",
    "Actual Small   {tn_test:,}    {fp_test:,}\n",
    "       Heavy   {fn_test:,}     {tp_test:,}\n",
    "\n",
    "PRD COMPLIANCE (on Production Distribution):\n",
    "- Heavy Recall â‰¥{config['prd_requirements']['target_heavy_recall']}: {'PASS' if prd_results['heavy_recall'] else 'FAIL'} ({recall_test:.4f})\n",
    "- FNR â‰¤{config['prd_requirements']['target_fnr']}: {'PASS' if prd_results['max_fnr'] else 'FAIL'} ({fnr_test:.4f})\n",
    "- F1-Score â‰¥{config['prd_requirements']['target_f1']}: {'PASS' if prd_results['f1_score'] else 'FAIL'} ({f1_test:.4f})\n",
    "- ROC-AUC â‰¥{config['prd_requirements']['target_roc_auc']}: {'PASS' if prd_results['roc_auc'] else 'FAIL'} ({test_roc_auc:.4f})\n",
    "\n",
    "Overall: {'ALL REQUIREMENTS MET' if all_pass else 'REQUIREMENTS NOT MET'}\n",
    "\n",
    "PRODUCTION REALITY:\n",
    "- False Positive Rate: {fpr_test:.2%}\n",
    "- Expected FP per 1M queries: {int(fpr_test * 1_000_000):,}\n",
    "- False Negative Rate: {fnr_test:.2%}\n",
    "- Expected FN per 1M queries: {int(fnr_test * 1_000_000):,}\n",
    "\n",
    "This is REALISTIC - metrics reflect 36:1 production distribution\n",
    "\n",
    "MODEL EXPORT:\n",
    "- ONNX Export: {'PASS' if onnx_export_success else 'FAIL'}\n",
    "- XGBoost Model: {xgb_s3_path}\n",
    "- ONNX Model: {onnx_s3_path if onnx_export_success else 'N/A'}\n",
    "\n",
    "KEY ACHIEVEMENTS:\n",
    "- Distributed training (no driver memory bottleneck)\n",
    "- Proper class imbalance handling (train 5:1, eval 36:1)\n",
    "- Realistic evaluation metrics (production distribution)\n",
    "- Cost-based threshold optimization\n",
    "- Comprehensive confusion matrices\n",
    "- Automated PRD compliance checking\n",
    "- Production-ready artifacts saved to S3\n",
    "\n",
    "STATUS: {'READY FOR DEPLOYMENT' if (all_pass and onnx_export_success) else 'REVIEW REQUIRED'}\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(training_summary)\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "if all_pass and onnx_export_success:\n",
    "    print(\"1. Deploy ONNX model to production service\")\n",
    "    print(f\"2. Configure threshold: {best_threshold:.3f}\")\n",
    "    print(\"3. Monitor production metrics\")\n",
    "    print(f\"4. Expect precision ~{precision_test:.1%} (matches test set)\")\n",
    "    print(f\"5. Expect recall ~{recall_test:.1%} (matches test set)\")\n",
    "else:\n",
    "    print(\"1. Review model performance issues\")\n",
    "    print(\"2. Consider hyperparameter tuning or feature engineering\")\n",
    "    print(\"3. Re-train and validate before deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISTRIBUTED MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T04:55:39.803268Z",
     "iopub.status.busy": "2025-10-18T04:55:39.803057Z",
     "iopub.status.idle": "2025-10-18T04:55:39.859468Z",
     "shell.execute_reply": "2025-10-18T04:55:39.859039Z",
     "shell.execute_reply.started": "2025-10-18T04:55:39.803254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8606dc187d4bfcb4327029ebf54c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'best_metrics' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'best_metrics' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_summary = f\"\"\"\n",
    "{'='*70}\n",
    "DISTRIBUTED MODEL TRAINING SUMMARY\n",
    "{'='*70}\n",
    "\n",
    "Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Model Version: {model_version}\n",
    "\n",
    "ARCHITECTURE:\n",
    "- Algorithm: XGBoost (Distributed Spark)\n",
    "- Training Approach: NO driver collection (fully distributed)\n",
    "- Features: {feature_count:,}\n",
    "- Training samples: {train_count:,}\n",
    "- Validation samples: {val_count:,}\n",
    "- Test samples: {test_count:,}\n",
    "- Class ratio: {scale_pos_weight:.2f}:1 (small:heavy)\n",
    "\n",
    "VALIDATION PERFORMANCE (Default Threshold 0.5):\n",
    "- Recall:    {val_metrics['recall']:.4f}\n",
    "- Precision: {val_metrics['precision']:.4f}\n",
    "- F1-Score:  {val_metrics['f1']:.4f}\n",
    "- ROC-AUC:   {val_metrics['roc_auc']:.4f}\n",
    "- FNR:       {val_metrics['fnr']:.4f}\n",
    "\n",
    "THRESHOLD OPTIMIZATION:\n",
    "- Optimal Threshold: {best_threshold:.3f}\n",
    "- Cost Function: {cost_fn}:{cost_fp} (FN:FP ratio)\n",
    "- Validation Cost: {best_metrics['cost']:,}\n",
    "- Test Cost: {test_cost:,}\n",
    "\n",
    "TEST SET PERFORMANCE (Optimal Threshold):\n",
    "- Recall:    {recall_test:.4f}\n",
    "- Precision: {precision_test:.4f}\n",
    "- F1-Score:  {f1_test:.4f}\n",
    "- ROC-AUC:   {test_roc_auc:.4f}\n",
    "- FNR:       {fnr_test:.4f}\n",
    "- FPR:       {fpr_test:.4f}\n",
    "\n",
    "CONFUSION MATRIX (Test Set):\n",
    "                 Predicted\n",
    "               Small   Heavy\n",
    "Actual Small   {tn_test:,}    {fp_test:,}\n",
    "       Heavy   {fn_test:,}     {tp_test:,}\n",
    "\n",
    "PRD COMPLIANCE:\n",
    "- Heavy Recall â‰¥{config['prd_requirements']['target_heavy_recall']}: {'âœ… PASS' if prd_results['heavy_recall'] else 'âŒ FAIL'} ({recall_test:.4f})\n",
    "- FNR â‰¤{config['prd_requirements']['target_fnr']}: {'âœ… PASS' if prd_results['max_fnr'] else 'âŒ FAIL'} ({fnr_test:.4f})\n",
    "- F1-Score â‰¥{config['prd_requirements']['target_f1']}: {'âœ… PASS' if prd_results['f1_score'] else 'âŒ FAIL'} ({f1_test:.4f})\n",
    "- ROC-AUC â‰¥{config['prd_requirements']['target_roc_auc']}: {'âœ… PASS' if prd_results['roc_auc'] else 'âŒ FAIL'} ({test_roc_auc:.4f})\n",
    "\n",
    "Overall: {'âœ… ALL REQUIREMENTS MET' if all_pass else 'âŒ REQUIREMENTS NOT MET'}\n",
    "\n",
    "MODEL EXPORT:\n",
    "- ONNX Export: {'âœ… PASS' if onnx_export_success else 'âŒ FAIL'}\n",
    "- XGBoost Model: {xgb_s3_path}\n",
    "- ONNX Model: {onnx_s3_path if onnx_export_success else 'N/A'}\n",
    "\n",
    "KEY ACHIEVEMENTS:\n",
    "âœ… Distributed training (no driver memory bottleneck)\n",
    "âœ… Detailed validation analysis\n",
    "âœ… Cost-based threshold optimization\n",
    "âœ… Comprehensive confusion matrices\n",
    "âœ… Automated PRD compliance checking\n",
    "âœ… Production-ready artifacts saved to S3\n",
    "\n",
    "STATUS: {'âœ… READY FOR DEPLOYMENT' if (all_pass and onnx_export_success) else 'âš ï¸  REVIEW REQUIRED'}\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(training_summary)\n",
    "\n",
    "print(\"\\nâœ… Next Steps:\")\n",
    "if all_pass and onnx_export_success:\n",
    "    print(\"1. Deploy ONNX model to production service\")\n",
    "    print(f\"2. Configure threshold: {best_threshold:.3f}\")\n",
    "    print(\"3. Monitor production metrics\")\n",
    "else:\n",
    "    print(\"1. Review model performance issues\")\n",
    "    print(\"2. Consider hyperparameter tuning or feature engineering\")\n",
    "    print(\"3. Re-train and validate before deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISTRIBUTED MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
