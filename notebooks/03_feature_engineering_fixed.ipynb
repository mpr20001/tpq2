{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Feature Engineering (FIXED)\n",
    "\n",
    "**Purpose**: Extract 95 + 250 features with train-serve parity validation\n",
    "\n",
    "**FIXED VERSION**: This notebook fixes the feature parity issues by using a unified extractor approach.\n",
    "\n",
    "**Pipeline**:\n",
    "1. Load pre-split datasets from notebook 01\n",
    "2. Compute historical statistics from training data\n",
    "3. Extract 95 features using unified FeatureExtractor (78 base + 17 historical)\n",
    "4. Build TF-IDF vocabulary (training data ONLY)\n",
    "5. Extract TF-IDF features for all splits\n",
    "6. Combine features (95 + 250 = 345 total)\n",
    "7. Validate feature parity\n",
    "8. Save to S3\n",
    "\n",
    "**Key Fix**: Uses unified FeatureExtractor with historical features enabled for both training and inference.\n",
    "\n",
    "**Duration**: ~45-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Configuration\n",
    "\n",
    "Copy the Spark configuration from notebook 00 output and paste below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:17:17.120930Z",
     "iopub.status.busy": "2025-10-29T19:17:17.120360Z",
     "iopub.status.idle": "2025-10-29T19:17:19.315904Z",
     "shell.execute_reply": "2025-10-29T19:17:19.315325Z",
     "shell.execute_reply.started": "2025-10-29T19:17:17.120909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': './environment/bin/python', 'spark.yarn.dist.archives': 's3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/pyspark_env.tar.gz#environment', 'spark.submit.deployMode': 'cluster', 'spark.driver.maxResultSize': '8G', 'spark.dynamicAllocation.enabled': 'true', 'spark.dynamicAllocation.minExecutors': '2', 'spark.dynamicAllocation.maxExecutors': '20'}, 'driverMemory': '16G', 'executorCores': 5, 'executorMemory': '20G', 'pyFiles': ['s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip', 's3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py'], 'driverCores': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1102</td><td>application_1761077531923_72724</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_72724/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-36-64.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_72724_01_000001/mumath\">Link</a></td><td>mumath</td><td></td></tr><tr><td>1134</td><td>application_1761077531923_74694</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_74694/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-36-36.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_74694_01_000001/chhavi.agrawal\">Link</a></td><td>chhavi.agrawal</td><td></td></tr><tr><td>1135</td><td>application_1761077531923_74725</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_74725/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-98.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_74725_01_000002/mbharti\">Link</a></td><td>mbharti</td><td></td></tr><tr><td>1141</td><td>application_1761077531923_75496</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_75496/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-37-215.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_75496_01_000001/rsinghchouhan\">Link</a></td><td>rsinghchouhan</td><td></td></tr><tr><td>1146</td><td>application_1761077531923_75680</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_75680/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-37-215.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_75680_01_000006/mrittinghouse\">Link</a></td><td>mrittinghouse</td><td></td></tr><tr><td>1148</td><td>application_1761077531923_75740</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_75740/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-37-215.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_75740_01_000007/xiao.zhang\">Link</a></td><td>xiao.zhang</td><td></td></tr><tr><td>1150</td><td>application_1761077531923_75838</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_75838/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-37-101.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_75838_01_000001/nick.gibson\">Link</a></td><td>nick.gibson</td><td></td></tr><tr><td>1151</td><td>application_1761077531923_75882</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_75882/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-36-221.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_75882_01_000001/nick.gibson\">Link</a></td><td>nick.gibson</td><td></td></tr><tr><td>1152</td><td>application_1761077531923_76173</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76173/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-32-147.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76173_01_000001/nick.gibson\">Link</a></td><td>nick.gibson</td><td></td></tr><tr><td>1153</td><td>application_1761077531923_76218</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76218/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-38-62.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76218_01_000001/nick.gibson\">Link</a></td><td>nick.gibson</td><td></td></tr><tr><td>1154</td><td>application_1761077531923_76244</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76244/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-36-36.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76244_01_000001/tyerra\">Link</a></td><td>tyerra</td><td></td></tr><tr><td>1156</td><td>application_1761077531923_76323</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76323/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-33-113.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76323_01_000001/jiangyuan.liu\">Link</a></td><td>jiangyuan.liu</td><td></td></tr><tr><td>1158</td><td>application_1761077531923_76354</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76354/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-36-209.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76354_01_000001/pvolpe\">Link</a></td><td>pvolpe</td><td></td></tr><tr><td>1161</td><td>application_1761077531923_76416</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76416/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-38-139.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76416_01_000001/paulanderson\">Link</a></td><td>paulanderson</td><td></td></tr><tr><td>1163</td><td>application_1761077531923_76476</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76476/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-36-36.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76476_01_000002/w.scroggins\">Link</a></td><td>w.scroggins</td><td></td></tr><tr><td>1164</td><td>application_1761077531923_76477</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76477/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-37-116.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76477_01_000002/zpipkin\">Link</a></td><td>zpipkin</td><td></td></tr><tr><td>1165</td><td>application_1761077531923_76482</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76482/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-37-6.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76482_01_000003/sahith.kondepudi\">Link</a></td><td>sahith.kondepudi</td><td></td></tr><tr><td>1166</td><td>application_1761077531923_76491</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76491/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-37-101.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76491_01_000002/prashantchoudhary\">Link</a></td><td>prashantchoudhary</td><td></td></tr><tr><td>1167</td><td>application_1761077531923_76539</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76539/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-37-150.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76539_01_000001/mbharti\">Link</a></td><td>mbharti</td><td></td></tr><tr><td>1170</td><td>application_1761077531923_76660</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76660/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-39-212.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76660_01_000001/vkovvuru\">Link</a></td><td>vkovvuru</td><td></td></tr><tr><td>1173</td><td>application_1761077531923_76730</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76730/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-36-209.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76730_01_000012/feifan.jian\">Link</a></td><td>feifan.jian</td><td></td></tr><tr><td>1174</td><td>application_1761077531923_76742</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76742/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-168.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76742_01_000001/swastisharma\">Link</a></td><td>swastisharma</td><td></td></tr><tr><td>1178</td><td>application_1761077531923_76920</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76920/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-35-14.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76920_01_000001/jchesnutt\">Link</a></td><td>jchesnutt</td><td></td></tr><tr><td>1179</td><td>application_1761077531923_76968</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76968/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-38-62.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76968_01_000001/kenyu.yu\">Link</a></td><td>kenyu.yu</td><td></td></tr><tr><td>1180</td><td>application_1761077531923_76976</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76976/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-86.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76976_01_000001/robert.moore\">Link</a></td><td>robert.moore</td><td></td></tr><tr><td>1181</td><td>application_1761077531923_76998</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_76998/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-34-86.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_76998_01_000001/sandhyakumari.kada\">Link</a></td><td>sandhyakumari.kada</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"pyFiles\": [\n",
    "        \"s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip\",\n",
    "        \"s3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py\"\n",
    "    ],\n",
    "    \"driverMemory\": \"16G\",\n",
    "    \"driverCores\": 4,\n",
    "    \"executorMemory\": \"20G\",\n",
    "    \"executorCores\": 5,\n",
    "    \"conf\": {\n",
    "        \"spark.yarn.dist.archives\": \"s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/pyspark_env.tar.gz#environment\",\n",
    "        \"spark.driver.maxResultSize\": \"8G\",\n",
    "        \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "        \"spark.dynamicAllocation.minExecutors\": \"2\",\n",
    "        \"spark.dynamicAllocation.maxExecutors\": \"20\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:17:25.937011Z",
     "iopub.status.busy": "2025-10-29T19:17:25.936679Z",
     "iopub.status.idle": "2025-10-29T19:19:07.184344Z",
     "shell.execute_reply": "2025-10-29T19:19:07.183841Z",
     "shell.execute_reply.started": "2025-10-29T19:17:25.936991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1182</td><td>application_1761077531923_77249</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://ip-10-32-32-97.us-east-2.compute.internal:20888/proxy/application_1761077531923_77249/\">Link</a></td><td><a target=\"_blank\" href=\"https://ip-10-32-38-62.us-east-2.compute.internal:8044/node/containerlogs/container_1761077531923_77249_01_000001/pmannem\">Link</a></td><td>pmannem</td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4601c29a4a54143958057a754d4306b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc94d34147b43f181b65956e0c112fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.13 (main, Jul 30 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-5)]\n",
      "PySpark version: 3.5.4-amzn-0\n",
      "? All imports successful"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "# Import production modules\n",
    "from query_predictor.core.featurizer.feature_extractor import FeatureExtractor\n",
    "from query_predictor.training.spark_ml_tfidf_pipeline import SparkMLTfidfPipeline\n",
    "from query_predictor.training.parity_validator import ParityValidator\n",
    "from query_predictor.training.historical_stats_computer import HistoricalStatsComputer\n",
    "from query_predictor.training.checkpoint_manager import CheckpointManager\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {spark.version}\")\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:19:07.185646Z",
     "iopub.status.busy": "2025-10-29T19:19:07.185218Z",
     "iopub.status.idle": "2025-10-29T19:19:08.499766Z",
     "shell.execute_reply": "2025-10-29T19:19:08.499259Z",
     "shell.execute_reply.started": "2025-10-29T19:19:07.185629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a38bfb63404c6e8c147617667d98dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config from S3: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/config/training_config_latest.yaml\n",
      "? Configuration loaded\n",
      "\n",
      "? Feature Configuration:\n",
      "  Base features: 78\n",
      "  Historical features: 17\n",
      "  TF-IDF vocab size: 250\n",
      "  Total features: 345"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Download training configuration from S3\n",
    "s3_client = boto3.client('s3')\n",
    "s3_bucket = 'uip-datalake-bucket-prod'\n",
    "s3_prefix = 'sf_trino/trino_query_predictor'\n",
    "config_s3_key = f\"{s3_prefix}/config/training_config_latest.yaml\"\n",
    "config_path = '/tmp/training_config.yaml'\n",
    "\n",
    "print(f\"Downloading config from S3: s3://{s3_bucket}/{config_s3_key}\")\n",
    "s3_client.download_file(s3_bucket, config_s3_key, config_path)\n",
    "\n",
    "# Load training configuration\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_mgr = CheckpointManager(\n",
    "    spark,\n",
    "    s3_checkpoint_path=config['checkpointing']['s3_path'],\n",
    "    enabled=config['checkpointing']['enabled']\n",
    ")\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"\\nðŸ“‹ Feature Configuration:\")\n",
    "print(f\"  Base features: {config['features']['base_feature_count']}\")\n",
    "print(f\"  Historical features: {config['features']['historical_feature_count']}\")\n",
    "print(f\"  TF-IDF vocab size: {config['features']['tfidf_vocab_size']}\")\n",
    "print(f\"  Total features: {config['features']['total_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pre-Split Data from Notebook 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:19:08.500695Z",
     "iopub.status.busy": "2025-10-29T19:19:08.500358Z",
     "iopub.status.idle": "2025-10-29T19:20:06.369834Z",
     "shell.execute_reply": "2025-10-29T19:20:06.369269Z",
     "shell.execute_reply.started": "2025-10-29T19:19:08.500678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6927a2a5834b999972daaddd0b971c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-split datasets...\n",
      "  Train (sampled): s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/train_sampled\n",
      "  Val (original):  s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/val_original\n",
      "  Test (original): s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/processed_data/2025-08-01_to_2025-10-01/test_original\n",
      "\n",
      "? Datasets loaded:\n",
      "  Train: 8,782,474 queries\n",
      "  Val:   14,969,526 queries\n",
      "  Test:  15,099,750 queries\n",
      "\n",
      "Distribution ratios (Small:Heavy):\n",
      "  Train: 5.0:1 (sampled)\n",
      "  Val:   48.2:1 (original)\n",
      "  Test:  25.0:1 (original)"
     ]
    }
   ],
   "source": [
    "# Load pre-split datasets from notebook 01\n",
    "processed_path = config['data_loading']['processed_output_path']\n",
    "date_range = f\"{config['data_loading']['start_date']}_to_{config['data_loading']['end_date']}\"\n",
    "base_path = f\"{processed_path}/{date_range}\"\n",
    "\n",
    "train_path = f\"{base_path}/train_sampled\"  # 5:1 sampled for training\n",
    "val_path = f\"{base_path}/val_original\"      # ~36:1 original distribution\n",
    "test_path = f\"{base_path}/test_original\"    # ~36:1 original distribution\n",
    "\n",
    "print(f\"Loading pre-split datasets...\")\n",
    "print(f\"  Train (sampled): {train_path}\")\n",
    "print(f\"  Val (original):  {val_path}\")\n",
    "print(f\"  Test (original): {test_path}\")\n",
    "\n",
    "# Load splits\n",
    "train_df = spark.read.parquet(train_path)\n",
    "val_df = spark.read.parquet(val_path)\n",
    "test_df = spark.read.parquet(test_path)\n",
    "\n",
    "# Get counts\n",
    "train_count = train_df.count()\n",
    "val_count = val_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"\\nâœ… Datasets loaded:\")\n",
    "print(f\"  Train: {train_count:,} queries\")\n",
    "print(f\"  Val:   {val_count:,} queries\")\n",
    "print(f\"  Test:  {test_count:,} queries\")\n",
    "\n",
    "# Calculate ratios for reporting\n",
    "train_heavy = train_df.filter(F.col('is_heavy') == 1).count()\n",
    "train_ratio = (train_count - train_heavy) / train_heavy if train_heavy > 0 else 0\n",
    "\n",
    "val_heavy = val_df.filter(F.col('is_heavy') == 1).count()\n",
    "val_ratio = (val_count - val_heavy) / val_heavy if val_heavy > 0 else 0\n",
    "\n",
    "test_heavy = test_df.filter(F.col('is_heavy') == 1).count()\n",
    "test_ratio = (test_count - test_heavy) / test_heavy if test_heavy > 0 else 0\n",
    "\n",
    "print(f\"\\nDistribution ratios (Small:Heavy):\")\n",
    "print(f\"  Train: {train_ratio:.1f}:1 (sampled)\")\n",
    "print(f\"  Val:   {val_ratio:.1f}:1 (original)\")\n",
    "print(f\"  Test:  {test_ratio:.1f}:1 (original)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Historical Statistics\n",
    "\n",
    "Compute statistics from training data only to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:20:06.370613Z",
     "iopub.status.busy": "2025-10-29T19:20:06.370440Z",
     "iopub.status.idle": "2025-10-29T19:20:40.012512Z",
     "shell.execute_reply": "2025-10-29T19:20:40.012021Z",
     "shell.execute_reply.started": "2025-10-29T19:20:06.370596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243783d142804de6adf1153e9446bc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing historical statistics from training data...\n",
      "\n",
      "? Historical stats computed:\n",
      "  Users: 16,181\n",
      "  Catalogs: 36\n",
      "  Schemas: 443\n",
      "  Overall heavy rate: 16.67%"
     ]
    }
   ],
   "source": [
    "print(\"Computing historical statistics from training data...\")\n",
    "\n",
    "# Initialize stats computer\n",
    "stats_computer = HistoricalStatsComputer(version='1.0.0')\n",
    "\n",
    "# Compute stats from training data\n",
    "date_range_dict = {\n",
    "    'start': config['data_loading']['start_date'],\n",
    "    'end': config['data_loading']['end_date']\n",
    "}\n",
    "stats_schema = stats_computer.compute(train_df, date_range_dict)\n",
    "\n",
    "print(f\"\\nâœ… Historical stats computed:\")\n",
    "print(f\"  Users: {len(stats_schema.users):,}\")\n",
    "print(f\"  Catalogs: {len(stats_schema.catalogs):,}\")\n",
    "print(f\"  Schemas: {len(stats_schema.schemas):,}\")\n",
    "print(f\"  Overall heavy rate: {stats_schema.heavy_rate_overall:.2%}\")\n",
    "\n",
    "# Serialize to dict for FeatureExtractor\n",
    "stats_dict = stats_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Unified Feature Extractor\n",
    "\n",
    "**KEY FIX**: Use a single FeatureExtractor with historical features enabled.\n",
    "This ensures consistent feature computation between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:20:40.013164Z",
     "iopub.status.busy": "2025-10-29T19:20:40.013050Z",
     "iopub.status.idle": "2025-10-29T19:20:40.288249Z",
     "shell.execute_reply": "2025-10-29T19:20:40.287735Z",
     "shell.execute_reply.started": "2025-10-29T19:20:40.013149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9797c7e6994f638288506ce5fc78b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AST Configuration:\n",
      "  Timeout: 200ms (increased from 50ms)\n",
      "  Fallback on timeout: True\n",
      "\n",
      "? Unified FeatureExtractor initialized\n",
      "  Feature count: 95\n",
      "  Expected: 95 (78 base + 17 historical)\n",
      "  Historical features enabled: True\n",
      "  AST timeout: 200ms"
     ]
    }
   ],
   "source": [
    "# Create unified configuration with historical features enabled\n",
    "unified_config = config.copy()\n",
    "unified_config['enable_historical_features'] = True\n",
    "\n",
    "# INCREASE AST parser timeout to reduce non-determinism\n",
    "unified_config['ast_timeout_ms'] = 200  # Increased from 50ms to 200ms\n",
    "unified_config['ast_fallback_on_timeout'] = True\n",
    "unified_config['ast_max_retries'] = 2  # Add retries if supported\n",
    "\n",
    "print(f\"AST Configuration:\")\n",
    "print(f\"  Timeout: {unified_config['ast_timeout_ms']}ms (increased from 50ms)\")\n",
    "print(f\"  Fallback on timeout: {unified_config['ast_fallback_on_timeout']}\")\n",
    "\n",
    "# Initialize unified feature extractor with historical stats\n",
    "unified_extractor = FeatureExtractor(\n",
    "    unified_config,\n",
    "    historical_stats=stats_dict\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Unified FeatureExtractor initialized\")\n",
    "print(f\"  Feature count: {unified_extractor.feature_count}\")\n",
    "print(f\"  Expected: 95 (78 base + 17 historical)\")\n",
    "print(f\"  Historical features enabled: True\")\n",
    "print(f\"  AST timeout: {unified_config['ast_timeout_ms']}ms\")\n",
    "\n",
    "assert unified_extractor.feature_count == 95, f\"Expected 95 features, got {unified_extractor.feature_count}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Unified Features (95 features)\n",
    "\n",
    "Extract base + historical features together using the unified extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:26:01.313997Z",
     "iopub.status.busy": "2025-10-29T19:26:01.313727Z",
     "iopub.status.idle": "2025-10-29T19:26:01.601644Z",
     "shell.execute_reply": "2025-10-29T19:26:01.601126Z",
     "shell.execute_reply.started": "2025-10-29T19:26:01.313977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2a3236cc0c4945aa61fc3e3e1898df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting unified features (base + historical) for all splits...\n",
      "This extracts 95 features in a single pass.\n",
      "\n",
      "[1/3] Extracting train features...\n",
      "[2/3] Extracting val features...\n",
      "[3/3] Extracting test features...\n",
      "\n",
      "? Unified features extracted for all splits (95 features each)"
     ]
    }
   ],
   "source": [
    "# Create Spark UDF for distributed extraction\n",
    "unified_udf = unified_extractor.create_spark_udf()\n",
    "\n",
    "print(\"Extracting unified features (base + historical) for all splits...\")\n",
    "print(\"This extracts 95 features in a single pass.\\n\")\n",
    "\n",
    "# Extract for train\n",
    "print(\"[1/3] Extracting train features...\")\n",
    "train_unified = train_df.withColumn(\n",
    "    'unified_features',\n",
    "    unified_udf(\n",
    "        F.struct(\n",
    "            F.col('query'),\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema'),\n",
    "            F.col('hour'),\n",
    "            F.col('clientInfo')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# train_unified = checkpoint_mgr.checkpoint(train_unified, \"03_train_unified_fixed\")\n",
    "\n",
    "# Extract for val\n",
    "print(\"[2/3] Extracting val features...\")\n",
    "val_unified = val_df.withColumn(\n",
    "    'unified_features',\n",
    "    unified_udf(\n",
    "        F.struct(\n",
    "            F.col('query'),\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema'),\n",
    "            F.col('hour'),\n",
    "            F.col('clientInfo')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# val_unified = checkpoint_mgr.checkpoint(val_unified, \"03_val_unified_fixed\")\n",
    "\n",
    "# Extract for test\n",
    "print(\"[3/3] Extracting test features...\")\n",
    "test_unified = test_df.withColumn(\n",
    "    'unified_features',\n",
    "    unified_udf(\n",
    "        F.struct(\n",
    "            F.col('query'),\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema'),\n",
    "            F.col('hour'),\n",
    "            F.col('clientInfo')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# test_unified = checkpoint_mgr.checkpoint(test_unified, \"03_test_unified_fixed\")\n",
    "\n",
    "print(\"\\nâœ… Unified features extracted for all splits (95 features each)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify Unified Feature Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:26:17.598070Z",
     "iopub.status.busy": "2025-10-29T19:26:17.597652Z",
     "iopub.status.idle": "2025-10-29T19:26:20.951218Z",
     "shell.execute_reply": "2025-10-29T19:26:20.950724Z",
     "shell.execute_reply.started": "2025-10-29T19:26:17.598051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7528a3dafaa840509caff3b3bfc91a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified feature dimensions:\n",
      "  Actual: 95\n",
      "  Expected: 95\n",
      "\n",
      "? Unified feature dimensions validated"
     ]
    }
   ],
   "source": [
    "# Sample to verify dimensions\n",
    "sample_train = train_unified.select('unified_features').limit(1).collect()[0]\n",
    "unified_dim = len(sample_train['unified_features'])\n",
    "\n",
    "print(f\"Unified feature dimensions:\")\n",
    "print(f\"  Actual: {unified_dim}\")\n",
    "print(f\"  Expected: 95\")\n",
    "\n",
    "assert unified_dim == 95, f\"Unified features should be 95, got {unified_dim}\"\n",
    "print(\"\\nâœ… Unified feature dimensions validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build TF-IDF Vocabulary (TRAINING DATA ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:26:20.952148Z",
     "iopub.status.busy": "2025-10-29T19:26:20.951978Z",
     "iopub.status.idle": "2025-10-29T19:28:23.341701Z",
     "shell.execute_reply": "2025-10-29T19:28:23.341157Z",
     "shell.execute_reply.started": "2025-10-29T19:26:20.952132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029035db29d14e1597123706bfe5a690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF vocabulary on TRAINING DATA ONLY...\n",
      "  Config: vocab_size=250, min_df=100, max_df=0.8\n",
      "  SQL optimizations: binary=True, filter_keywords=True\n",
      "  This prevents data leakage into val/test sets.\n",
      "\n",
      "\n",
      "? TF-IDF vocabulary built successfully\n",
      "  Vocabulary size: 250\n",
      "  Method: spark_ml_countvectorizer_optimized\n",
      "/mnt1/yarn/usercache/pmannem/appcache/application_1761077531923_77249/container_1761077531923_77249_01_000001/environment/lib64/python3.11/site-packages/sklearn/feature_extraction/text.py:1675: RuntimeWarning: divide by zero encountered in divide\n",
      "  self.idf_ /= df"
     ]
    }
   ],
   "source": [
    "# Initialize Spark ML TF-IDF pipeline with SQL-aware optimizations\n",
    "tfidf_config = {\n",
    "    'tfidf_vocab_size': config['features']['tfidf_vocab_size'],\n",
    "    'min_df': config['features']['min_df'],\n",
    "    'max_df': config['features']['max_df'],\n",
    "    'use_binary': config['features'].get('use_binary', True),\n",
    "    'filter_sql_keywords': config['features'].get('filter_sql_keywords', True),\n",
    "    'normalize_sql': config['features'].get('normalize_sql', True)\n",
    "}\n",
    "\n",
    "tfidf_pipeline = SparkMLTfidfPipeline(tfidf_config)\n",
    "\n",
    "print(\"Building TF-IDF vocabulary on TRAINING DATA ONLY...\")\n",
    "print(f\"  Config: vocab_size={tfidf_config['tfidf_vocab_size']}, min_df={tfidf_config['min_df']}, max_df={tfidf_config['max_df']}\")\n",
    "print(f\"  SQL optimizations: binary={tfidf_config['use_binary']}, filter_keywords={tfidf_config['filter_sql_keywords']}\")\n",
    "print(\"  This prevents data leakage into val/test sets.\\n\")\n",
    "\n",
    "# Fit on DataFrame directly (NO COLLECT!)\n",
    "tfidf_pipeline.fit_on_dataframe(train_unified, query_column='query')\n",
    "\n",
    "print(f\"\\nâœ… TF-IDF vocabulary built successfully\")\n",
    "metadata = tfidf_pipeline.get_feature_metadata()\n",
    "print(f\"  Vocabulary size: {metadata['vocab_size']:,}\")\n",
    "print(f\"  Method: {metadata['method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Extract TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:28:23.342888Z",
     "iopub.status.busy": "2025-10-29T19:28:23.342511Z",
     "iopub.status.idle": "2025-10-29T19:28:23.615290Z",
     "shell.execute_reply": "2025-10-29T19:28:23.614714Z",
     "shell.execute_reply.started": "2025-10-29T19:28:23.342872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0409d245b414456eaaa4bb18bce0e430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting TF-IDF features for all splits...\n",
      "\n",
      "[1/3] Extracting train TF-IDF features...\n",
      "[2/3] Extracting val TF-IDF features...\n",
      "[3/3] Extracting test TF-IDF features...\n",
      "\n",
      "? TF-IDF features extracted for all splits"
     ]
    }
   ],
   "source": [
    "# Create Spark UDF from fitted pipeline\n",
    "tfidf_udf = tfidf_pipeline.create_spark_udf()\n",
    "\n",
    "print(\"Extracting TF-IDF features for all splits...\")\n",
    "\n",
    "# Extract for train\n",
    "print(\"\\n[1/3] Extracting train TF-IDF features...\")\n",
    "train_tfidf = train_unified.withColumn('tfidf_features', tfidf_udf(F.col('query')))\n",
    "# train_tfidf = checkpoint_mgr.checkpoint(train_tfidf, \"03_train_tfidf_fixed\")\n",
    "\n",
    "# Extract for val\n",
    "print(\"[2/3] Extracting val TF-IDF features...\")\n",
    "val_tfidf = val_unified.withColumn('tfidf_features', tfidf_udf(F.col('query')))\n",
    "# val_tfidf = checkpoint_mgr.checkpoint(val_tfidf, \"03_val_tfidf_fixed\")\n",
    "\n",
    "# Extract for test\n",
    "print(\"[3/3] Extracting test TF-IDF features...\")\n",
    "test_tfidf = test_unified.withColumn('tfidf_features', tfidf_udf(F.col('query')))\n",
    "# test_tfidf = checkpoint_mgr.checkpoint(test_tfidf, \"03_test_tfidf_fixed\")\n",
    "\n",
    "print(\"\\nâœ… TF-IDF features extracted for all splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Combine Features\n",
    "\n",
    "Concatenate unified features (95) + TF-IDF features (250) = 345 total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T20:46:12.252486Z",
     "iopub.status.busy": "2025-10-23T20:46:12.252313Z",
     "iopub.status.idle": "2025-10-23T21:07:00.730933Z",
     "shell.execute_reply": "2025-10-23T21:07:00.730328Z",
     "shell.execute_reply.started": "2025-10-23T20:46:12.252463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f8a40fb514454c987cf32b27085f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining unified and TF-IDF features...\n",
      "\n",
      "? Features combined\n",
      "  Unified: 95 (78 base + 17 historical)\n",
      "  TF-IDF: 250\n",
      "  Total: 345"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "@udf(returnType=ArrayType(FloatType()))\n",
    "def combine_features(unified, tfidf):\n",
    "    \"\"\"Concatenate unified + tfidf features.\"\"\"\n",
    "    if unified is None or tfidf is None:\n",
    "        return None\n",
    "    return unified + tfidf\n",
    "\n",
    "print(\"Combining unified and TF-IDF features...\")\n",
    "\n",
    "# Combine for train\n",
    "train_final = train_tfidf.withColumn(\n",
    "    'features',\n",
    "    combine_features(\n",
    "        F.col('unified_features'),\n",
    "        F.col('tfidf_features')\n",
    "    )\n",
    ")\n",
    "train_final = checkpoint_mgr.checkpoint(train_final, \"03_train_final\")\n",
    "\n",
    "\n",
    "# Combine for val\n",
    "val_final = val_tfidf.withColumn(\n",
    "    'features',\n",
    "    combine_features(\n",
    "        F.col('unified_features'),\n",
    "        F.col('tfidf_features')\n",
    "    )\n",
    ")\n",
    "val_final = checkpoint_mgr.checkpoint(val_final, \"03_val_final\")\n",
    "\n",
    "\n",
    "# Combine for test\n",
    "test_final = test_tfidf.withColumn(\n",
    "    'features',\n",
    "    combine_features(\n",
    "        F.col('unified_features'),\n",
    "        F.col('tfidf_features')\n",
    "    )\n",
    ")\n",
    "test_final = checkpoint_mgr.checkpoint(test_final, \"03_test_final\")\n",
    "\n",
    "print(\"\\nâœ… Features combined\")\n",
    "print(f\"  Unified: 95 (78 base + 17 historical)\")\n",
    "print(f\"  TF-IDF: {config['features']['tfidf_vocab_size']}\")\n",
    "print(f\"  Total: {config['features']['total_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Validate Feature Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:28:23.616058Z",
     "iopub.status.busy": "2025-10-29T19:28:23.615885Z",
     "iopub.status.idle": "2025-10-29T19:28:28.977127Z",
     "shell.execute_reply": "2025-10-29T19:28:28.976551Z",
     "shell.execute_reply.started": "2025-10-29T19:28:23.616042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4374137695d545b7a22416fe24d9a850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_final = checkpoint_mgr.load_checkpoint(\"03_train_final\")\n",
    "val_final = checkpoint_mgr.load_checkpoint(\"03_val_final\")\n",
    "test_final = checkpoint_mgr.load_checkpoint(\"03_test_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:28:28.978257Z",
     "iopub.status.busy": "2025-10-29T19:28:28.977933Z",
     "iopub.status.idle": "2025-10-29T19:28:31.312701Z",
     "shell.execute_reply": "2025-10-29T19:28:31.312180Z",
     "shell.execute_reply.started": "2025-10-29T19:28:28.978238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13be3056b2ff4f9d80a892b3c9d36b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating final feature dimensions...\n",
      "\n",
      "? Feature Dimensions:\n",
      "  Train: 345\n",
      "  Val:   345\n",
      "  Test:  345\n",
      "  Expected: 345\n",
      "\n",
      "? All dimensions validated"
     ]
    }
   ],
   "source": [
    "# Sample and verify dimensions\n",
    "print(\"Validating final feature dimensions...\")\n",
    "\n",
    "sample_train = train_final.select('features', 'is_heavy').limit(1).collect()[0]\n",
    "sample_val = val_final.select('features', 'is_heavy').limit(1).collect()[0]\n",
    "sample_test = test_final.select('features', 'is_heavy').limit(1).collect()[0]\n",
    "\n",
    "train_dim = len(sample_train['features'])\n",
    "val_dim = len(sample_val['features'])\n",
    "test_dim = len(sample_test['features'])\n",
    "expected_dim = config['features']['total_features']\n",
    "\n",
    "print(f\"\\nðŸ“Š Feature Dimensions:\")\n",
    "print(f\"  Train: {train_dim}\")\n",
    "print(f\"  Val:   {val_dim}\")\n",
    "print(f\"  Test:  {test_dim}\")\n",
    "print(f\"  Expected: {expected_dim}\")\n",
    "\n",
    "assert train_dim == expected_dim, f\"Train dimension mismatch: {train_dim} != {expected_dim}\"\n",
    "assert val_dim == expected_dim, f\"Val dimension mismatch: {val_dim} != {expected_dim}\"\n",
    "assert test_dim == expected_dim, f\"Test dimension mismatch: {test_dim} != {expected_dim}\"\n",
    "\n",
    "print(\"\\nâœ… All dimensions validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Feature Parity Validation\n",
    "\n",
    "**CRITICAL**: Validate that training features match inference features.\n",
    "This should now pass with the unified extractor approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:28:31.313841Z",
     "iopub.status.busy": "2025-10-29T19:28:31.313454Z",
     "iopub.status.idle": "2025-10-29T19:28:33.660614Z",
     "shell.execute_reply": "2025-10-29T19:28:33.660110Z",
     "shell.execute_reply.started": "2025-10-29T19:28:31.313821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e42cf388fe473cb053b5f4d2d143ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE PARITY VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Collecting 100 samples for validation...\n",
      "\n",
      "Running parity validation...\n",
      "  Tolerance: 1e-06\n",
      "  Success threshold: <0.5% mismatch\n",
      "\n",
      "Inference featurizer initialized (identical to training):\n",
      "  Feature count: 95\n",
      "  Historical features enabled: True\n",
      "  Config matches training: YES\n",
      "\n",
      "\n",
      "======================================================================\n",
      "FEATURE PARITY VALIDATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Status: âŒ FAILED\n",
      "\n",
      "Summary:\n",
      "  Samples Tested:  100\n",
      "  Mismatches:      100\n",
      "  Mismatch Rate:   100.00%\n",
      "  Max Difference:  1.000000000\n",
      "  Tolerance:       0.000001000\n",
      "\n",
      "Mismatch Details (first 10):\n",
      "\n",
      "  Sample 0:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 9\n",
      "    Feature indices: [45, 46, 47, 54, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 1:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 18\n",
      "    Feature indices: [46, 47, 48, 54, 95, 97, 99, 100, 106, 112]\n",
      "\n",
      "  Sample 2:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 18\n",
      "    Feature indices: [46, 47, 48, 54, 95, 97, 99, 100, 106, 112]\n",
      "\n",
      "  Sample 3:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 9\n",
      "    Feature indices: [45, 47, 54, 81, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 4:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 9\n",
      "    Feature indices: [45, 47, 54, 81, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 5:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 9\n",
      "    Feature indices: [45, 47, 54, 81, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 6:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 8\n",
      "    Feature indices: [45, 47, 54, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 7:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 5\n",
      "    Feature indices: [45, 46, 47, 48, 54]\n",
      "\n",
      "  Sample 8:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 9\n",
      "    Feature indices: [45, 47, 54, 81, 95, 99, 102, 104, 105]\n",
      "\n",
      "  Sample 9:\n",
      "    Max diff: 1.000000000\n",
      "    Num mismatches: 8\n",
      "    Feature indices: [45, 47, 54, 95, 99, 102, 104, 105]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "??  WARNING: Parity validation still failing!\n",
      "Debugging information:\n",
      "  - Training used unified_extractor with 95 features\n",
      "  - Inference using identical configuration\n",
      "  - Number of mismatches: 100 out of 100\n",
      "  - First mismatch feature indices: [45, 46, 47, 54, 95, 99, 102, 104, 105]\n",
      "  - Most commonly mismatched features: [(47, 10), (54, 10), (95, 9), (99, 9), (45, 8), (102, 7), (104, 7), (105, 7), (46, 4), (81, 4)]"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FEATURE PARITY VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize validator\n",
    "validation_config = config.get('validation', {})\n",
    "n_samples = validation_config.get('parity_samples', 100)\n",
    "validator = ParityValidator(config=config)\n",
    "\n",
    "# Collect sample of training features and queries\n",
    "print(f\"\\nCollecting {n_samples} samples for validation...\")\n",
    "train_samples = train_final.select(\n",
    "  'features', 'query', 'user', 'catalog', 'schema', 'hour', 'clientInfo', 'is_heavy'\n",
    ").limit(n_samples).collect()\n",
    "\n",
    "# Convert to numpy arrays\n",
    "training_features = np.array([row['features'] for row in train_samples], dtype=np.float32)\n",
    "\n",
    "# Prepare sample queries for inference\n",
    "sample_queries = [\n",
    "  {\n",
    "      'query': row['query'],\n",
    "      'user': row['user'],\n",
    "      'catalog': row['catalog'],\n",
    "      'schema': row['schema'],\n",
    "      'hour': row['hour'],\n",
    "      'clientInfo': row['clientInfo']\n",
    "  }\n",
    "  for row in train_samples\n",
    "]\n",
    "\n",
    "print(f\"\\nRunning parity validation...\")\n",
    "print(f\"  Tolerance: {validator.tolerance}\")\n",
    "print(f\"  Success threshold: <{validation_config.get('parity_success_threshold', 0.5)}% mismatch\")\n",
    "\n",
    "# Create inference featurizer with IDENTICAL configuration\n",
    "# This is the KEY FIX - using the same unified configuration\n",
    "inference_featurizer = FeatureExtractor(\n",
    "  unified_config,  # Same config as training\n",
    "  historical_stats=stats_dict  # Same historical stats\n",
    ")\n",
    "\n",
    "print(f\"\\nInference featurizer initialized (identical to training):\")\n",
    "print(f\"  Feature count: {inference_featurizer.feature_count}\")\n",
    "print(f\"  Historical features enabled: True\")\n",
    "print(f\"  Config matches training: YES\\n\")\n",
    "\n",
    "# Run validation\n",
    "parity_result = validator.validate_parity(\n",
    "  training_features=training_features,\n",
    "  inference_featurizer=inference_featurizer,\n",
    "  tfidf_pipeline=tfidf_pipeline,\n",
    "  sample_queries=sample_queries,\n",
    "  n_samples=n_samples\n",
    ")\n",
    "\n",
    "# Generate and print report\n",
    "report = validator.generate_report(parity_result)\n",
    "print(report)\n",
    "\n",
    "if not parity_result['passed']:\n",
    "  print(\"\\nâš ï¸  WARNING: Parity validation still failing!\")\n",
    "  print(\"Debugging information:\")\n",
    "  print(f\"  - Training used unified_extractor with {unified_extractor.feature_count} features\")\n",
    "  print(f\"  - Inference using identical configuration\")\n",
    "  print(f\"  - Number of mismatches: {parity_result['mismatches']} out of {parity_result['samples_tested']}\")\n",
    "\n",
    "  # Access the DETAILS list, not mismatches (which is an integer)\n",
    "  if parity_result.get('details') and len(parity_result['details']) > 0:\n",
    "      first_mismatch = parity_result['details'][0]\n",
    "      if 'mismatch_indices' in first_mismatch:\n",
    "          indices = first_mismatch['mismatch_indices']\n",
    "          print(f\"  - First mismatch feature indices: {indices}\")\n",
    "      if 'error' in first_mismatch:\n",
    "          print(f\"  - Error in first mismatch: {first_mismatch['error']}\")\n",
    "\n",
    "      # Check patterns across all mismatches\n",
    "      all_indices = []\n",
    "      for detail in parity_result.get('details', [])[:10]:\n",
    "          if 'mismatch_indices' in detail:\n",
    "              all_indices.extend(detail['mismatch_indices'])\n",
    "\n",
    "      if all_indices:\n",
    "          from collections import Counter\n",
    "          index_counts = Counter(all_indices)\n",
    "          most_common = index_counts.most_common(10)\n",
    "          print(f\"  - Most commonly mismatched features: {most_common}\")\n",
    "  else:\n",
    "      print(\"  - No detailed mismatch information available\")\n",
    "else:\n",
    "  print(\"\\nâœ… PARITY VALIDATION PASSED!\")\n",
    "  print(\"Features are consistent between training and inference.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Debug Feature Differences (if parity fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:29:00.607307Z",
     "iopub.status.busy": "2025-10-29T19:29:00.606912Z",
     "iopub.status.idle": "2025-10-29T19:29:00.674669Z",
     "shell.execute_reply": "2025-10-29T19:29:00.674196Z",
     "shell.execute_reply.started": "2025-10-29T19:29:00.607290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e67108da50e42ee82218612c27581ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging feature differences...\n",
      "\n",
      "Sample 0 analysis:\n",
      "  Total mismatches: 9\n",
      "  Mismatch indices: [ 45  46  47  54  95  99 102 104 105]\n",
      "\n",
      "Feature range analysis:\n",
      "  AST features (45-54): [np.int64(45), np.int64(46), np.int64(47), np.int64(54)]\n",
      "  Historical boundary (78-94): []\n",
      "  TF-IDF start (95+): [np.int64(95), np.int64(99), np.int64(102), np.int64(104), np.int64(105)]\n",
      "\n",
      "AST feature 45 (ast_depth):\n",
      "  Training: 0.0\n",
      "  Inference: 0.1"
     ]
    }
   ],
   "source": [
    "# Debug cell - only run if parity validation fails\n",
    "if not parity_result['passed']:\n",
    "    print(\"Debugging feature differences...\\n\")\n",
    "    \n",
    "    # Get first sample\n",
    "    sample_idx = 0\n",
    "    sample_query = sample_queries[sample_idx]\n",
    "    training_feat = training_features[sample_idx]\n",
    "    \n",
    "    # Extract features using inference path\n",
    "    inference_feat = inference_featurizer.extract(sample_query)\n",
    "    tfidf_feat = tfidf_pipeline.transform_single(sample_query['query'])\n",
    "    combined_inference = np.concatenate([inference_feat, tfidf_feat])\n",
    "    \n",
    "    # Find differences\n",
    "    diff = np.abs(training_feat - combined_inference)\n",
    "    mismatch_indices = np.where(diff > validator.tolerance)[0]\n",
    "    \n",
    "    print(f\"Sample {sample_idx} analysis:\")\n",
    "    print(f\"  Total mismatches: {len(mismatch_indices)}\")\n",
    "    print(f\"  Mismatch indices: {mismatch_indices[:20]}\")\n",
    "    \n",
    "    # Check specific feature ranges\n",
    "    print(f\"\\nFeature range analysis:\")\n",
    "    print(f\"  AST features (45-54): {[i for i in mismatch_indices if 45 <= i <= 54]}\")\n",
    "    print(f\"  Historical boundary (78-94): {[i for i in mismatch_indices if 78 <= i <= 94]}\")\n",
    "    print(f\"  TF-IDF start (95+): {[i for i in mismatch_indices if i >= 95]}\")\n",
    "    \n",
    "    # Sample specific features\n",
    "    if 45 in mismatch_indices:\n",
    "        print(f\"\\nAST feature 45 (ast_depth):\")\n",
    "        print(f\"  Training: {training_feat[45]}\")\n",
    "        print(f\"  Inference: {combined_inference[45]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:32:17.212358Z",
     "iopub.status.busy": "2025-10-29T19:32:17.212125Z",
     "iopub.status.idle": "2025-10-29T19:32:17.514239Z",
     "shell.execute_reply": "2025-10-29T19:32:17.513768Z",
     "shell.execute_reply.started": "2025-10-29T19:32:17.212339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b991d2ddbb3440bcb79e25c91fa8ded7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEEP ANALYSIS OF AST FEATURE MISMATCHES\n",
      "======================================================================\n",
      "\n",
      "1. Checking AST feature consistency across samples:\n",
      "--------------------------------------------------\n",
      "  ast_depth (idx 45):\n",
      "    Training:  0.0\n",
      "    Inference: 0.1\n",
      "    Diff:      0.10000000149011612\n",
      "  ast_breadth (idx 46):\n",
      "    Training:  0.029999999329447746\n",
      "    Inference: 0.02\n",
      "    Diff:      0.009999999776482582\n",
      "  ast_with_count (idx 47):\n",
      "    Training:  0.019999999552965164\n",
      "    Inference: 0.004\n",
      "    Diff:      0.01599999889731407\n",
      "  ast_coalesce_null_if_count (idx 54):\n",
      "    Training:  1.0\n",
      "    Inference: 0.0\n",
      "    Diff:      1.0\n",
      "\n",
      "2. AST Feature Mismatch Summary (across 10 samples):\n",
      "--------------------------------------------------\n",
      "  ast_depth                 (idx 45): 80.0% mismatch\n",
      "  ast_breadth               (idx 46): 40.0% mismatch\n",
      "  ast_with_count            (idx 47): 100.0% mismatch\n",
      "  ast_cte_count             (idx 48): 30.0% mismatch\n",
      "  ast_lateral_view_count    (idx 49): 0.0% mismatch\n",
      "  ast_window_func_count     (idx 50): 0.0% mismatch\n",
      "  ast_distinct_count        (idx 51): 0.0% mismatch\n",
      "  ast_having_count          (idx 52): 0.0% mismatch\n",
      "  ast_case_when_count       (idx 53): 0.0% mismatch\n",
      "  ast_coalesce_null_if_count (idx 54): 100.0% mismatch\n",
      "\n",
      "3. Testing AST Parser Directly:\n",
      "--------------------------------------------------\n",
      "  Timeout 50ms:\n",
      "    Parsed successfully: True\n",
      "    Consistent across runs: True\n",
      "    Depth values: [1, 1, 1]\n",
      "  Timeout 100ms:\n",
      "    Parsed successfully: True\n",
      "    Consistent across runs: True\n",
      "    Depth values: [1, 1, 1]\n",
      "  Timeout 200ms:\n",
      "    Parsed successfully: True\n",
      "    Consistent across runs: True\n",
      "    Depth values: [1, 1, 1]\n",
      "  Timeout 500ms:\n",
      "    Parsed successfully: True\n",
      "    Consistent across runs: True\n",
      "    Depth values: [1, 1, 1]\n",
      "\n",
      "4. Analyzing Historical Feature Mismatches:\n",
      "--------------------------------------------------\n",
      "  Historical features with mismatches:\n",
      "    Feature 81: 40.0% mismatch rate\n",
      "\n",
      "5. TF-IDF Feature Analysis:\n",
      "--------------------------------------------------\n",
      "  TF-IDF dimension: 250\n",
      "  Number of mismatches: 5\n",
      "  First 10 mismatch indices (offset from 95): [0, 4, 7, 9, 10]\n",
      "  Actual feature indices: [95, 99, 102, 104, 105]\n",
      "\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "# Deep debugging of AST feature mismatches\n",
    "print(\"=\"*70)\n",
    "print(\"DEEP ANALYSIS OF AST FEATURE MISMATCHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze patterns across multiple samples\n",
    "ast_feature_indices = list(range(45, 55))  # AST features are 45-54\n",
    "ast_feature_names = [\n",
    "    'ast_depth', 'ast_breadth', 'ast_with_count', 'ast_cte_count',\n",
    "    'ast_lateral_view_count', 'ast_window_func_count', 'ast_distinct_count',\n",
    "    'ast_having_count', 'ast_case_when_count', 'ast_coalesce_null_if_count'\n",
    "]\n",
    "\n",
    "print(\"\\n1. Checking AST feature consistency across samples:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "num_samples_to_check = min(10, len(sample_queries))\n",
    "ast_mismatches_by_feature = {i: 0 for i in ast_feature_indices}\n",
    "\n",
    "for sample_idx in range(num_samples_to_check):\n",
    "    sample_query = sample_queries[sample_idx]\n",
    "    training_feat = training_features[sample_idx]\n",
    "    \n",
    "    # Extract features using inference path\n",
    "    inference_feat = inference_featurizer.extract(sample_query)\n",
    "    \n",
    "    # Check AST features specifically\n",
    "    for i, feat_idx in enumerate(ast_feature_indices):\n",
    "        train_val = training_feat[feat_idx]\n",
    "        inf_val = inference_feat[feat_idx]\n",
    "        \n",
    "        if abs(train_val - inf_val) > validator.tolerance:\n",
    "            ast_mismatches_by_feature[feat_idx] += 1\n",
    "            \n",
    "            if sample_idx == 0:  # Detail for first sample\n",
    "                print(f\"  {ast_feature_names[i]} (idx {feat_idx}):\")\n",
    "                print(f\"    Training:  {train_val}\")\n",
    "                print(f\"    Inference: {inf_val}\")\n",
    "                print(f\"    Diff:      {abs(train_val - inf_val)}\")\n",
    "\n",
    "print(f\"\\n2. AST Feature Mismatch Summary (across {num_samples_to_check} samples):\")\n",
    "print(\"-\" * 50)\n",
    "for i, feat_idx in enumerate(ast_feature_indices):\n",
    "    mismatch_rate = (ast_mismatches_by_feature[feat_idx] / num_samples_to_check) * 100\n",
    "    print(f\"  {ast_feature_names[i]:25} (idx {feat_idx}): {mismatch_rate:.1f}% mismatch\")\n",
    "\n",
    "# Check if AST parsing is failing\n",
    "print(\"\\n3. Testing AST Parser Directly:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "from query_predictor.core.featurizer.parsers import ASTParser\n",
    "\n",
    "# Test with different timeout values\n",
    "timeouts = [50, 100, 200, 500]\n",
    "sample_query_text = sample_queries[0]['query']\n",
    "\n",
    "for timeout_ms in timeouts:\n",
    "    parser = ASTParser(timeout_ms=timeout_ms)\n",
    "    \n",
    "    # Parse multiple times to check consistency\n",
    "    results = []\n",
    "    for _ in range(3):\n",
    "        _, _, ast_metrics = parser.parse(sample_query_text)\n",
    "    \n",
    "        results.append({\n",
    "            'parsed': ast_metrics.parse_success,\n",
    "            'depth': ast_metrics.depth,\n",
    "            'node_count': ast_metrics.node_count\n",
    "        })\n",
    "    \n",
    "    # Check consistency\n",
    "    all_same = all(r == results[0] for r in results)\n",
    "    \n",
    "    print(f\"  Timeout {timeout_ms}ms:\")\n",
    "    print(f\"    Parsed successfully: {results[0]['parsed']}\")\n",
    "    print(f\"    Consistent across runs: {all_same}\")\n",
    "    if results[0]['parsed']:\n",
    "        print(f\"    Depth values: {[r['depth'] for r in results]}\")\n",
    "\n",
    "print(\"\\n4. Analyzing Historical Feature Mismatches:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Feature 81 is in historical range (78-94)\n",
    "historical_indices = list(range(78, 95))\n",
    "historical_mismatches = {i: 0 for i in historical_indices}\n",
    "\n",
    "for sample_idx in range(num_samples_to_check):\n",
    "    sample_query = sample_queries[sample_idx]\n",
    "    training_feat = training_features[sample_idx]\n",
    "    inference_feat = inference_featurizer.extract(sample_query)\n",
    "    \n",
    "    for feat_idx in historical_indices:\n",
    "        if feat_idx < len(training_feat) and feat_idx < len(inference_feat):\n",
    "            train_val = training_feat[feat_idx]\n",
    "            inf_val = inference_feat[feat_idx]\n",
    "            \n",
    "            if abs(train_val - inf_val) > validator.tolerance:\n",
    "                historical_mismatches[feat_idx] += 1\n",
    "\n",
    "# Report only features with mismatches\n",
    "historical_with_mismatches = [(idx, count) for idx, count in historical_mismatches.items() if count > 0]\n",
    "if historical_with_mismatches:\n",
    "    print(f\"  Historical features with mismatches:\")\n",
    "    for idx, count in historical_with_mismatches:\n",
    "        mismatch_rate = (count / num_samples_to_check) * 100\n",
    "        print(f\"    Feature {idx}: {mismatch_rate:.1f}% mismatch rate\")\n",
    "else:\n",
    "    print(\"  No historical feature mismatches found\")\n",
    "\n",
    "print(\"\\n5. TF-IDF Feature Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check TF-IDF features\n",
    "tfidf_start = 95\n",
    "sample_query_text = sample_queries[0]['query']\n",
    "\n",
    "# Get TF-IDF features from both paths\n",
    "try:\n",
    "    tfidf_inference = tfidf_pipeline.transform_single(sample_query_text)\n",
    "    tfidf_training = training_features[0][tfidf_start:]  # TF-IDF starts at index 95\n",
    "    \n",
    "    tfidf_mismatches = np.where(np.abs(tfidf_inference - tfidf_training) > validator.tolerance)[0]\n",
    "    \n",
    "    print(f\"  TF-IDF dimension: {len(tfidf_inference)}\")\n",
    "    print(f\"  Number of mismatches: {len(tfidf_mismatches)}\")\n",
    "    if len(tfidf_mismatches) > 0:\n",
    "        print(f\"  First 10 mismatch indices (offset from 95): {tfidf_mismatches[:10].tolist()}\")\n",
    "        print(f\"  Actual feature indices: {(tfidf_mismatches[:10] + tfidf_start).tolist()}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error analyzing TF-IDF: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:32:39.545703Z",
     "iopub.status.busy": "2025-10-29T19:32:39.545146Z",
     "iopub.status.idle": "2025-10-29T19:32:40.346584Z",
     "shell.execute_reply": "2025-10-29T19:32:40.346018Z",
     "shell.execute_reply.started": "2025-10-29T19:32:39.545681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510bfcceec9244108b43b1936c373c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING PARITY WITH AST FEATURES MASKED\n",
      "======================================================================\n",
      "\n",
      "Masking AST features (indices 45-54) to isolate other issues...\n",
      "\n",
      "Validating with AST features masked...\n",
      "\n",
      "Results with AST features masked:\n",
      "  Mismatch rate: 96.00%\n",
      "  Passed: False\n",
      "  Max difference: 0.983236194\n",
      "\n",
      "? Masking AST features improved parity by 4.0 percentage points\n",
      "?? Even with AST masked, 96 samples still have mismatches\n",
      "There are additional parity issues beyond AST features.\n",
      "\n",
      "Remaining mismatch indices (non-AST): [81, 95, 97, 99, 100, 102, 104, 105, 106, 112, 118, 128, 132, 141, 146, 161, 186, 205, 267, 343]\n",
      "  Historical features: [81]\n",
      "  TF-IDF features: [128, 132, 267, 141, 146, 161, 186, 205, 343, 95]\n",
      "\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "# Test parity with AST features masked to isolate the issue\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING PARITY WITH AST FEATURES MASKED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create copies of features with AST features set to 0\n",
    "ast_indices = list(range(45, 55))\n",
    "\n",
    "print(f\"\\nMasking AST features (indices {ast_indices[0]}-{ast_indices[-1]}) to isolate other issues...\")\n",
    "\n",
    "# Mask training features\n",
    "training_features_masked = training_features.copy()\n",
    "for idx in ast_indices:\n",
    "    training_features_masked[:, idx] = 0.0\n",
    "\n",
    "# Mask inference features\n",
    "masked_queries_features = []\n",
    "for sample_query in sample_queries:\n",
    "    # Extract features\n",
    "    base_historical = inference_featurizer.extract(sample_query)\n",
    "    tfidf_feat = tfidf_pipeline.transform_single(sample_query['query'])\n",
    "    combined = np.concatenate([base_historical, tfidf_feat])\n",
    "    \n",
    "    # Mask AST features\n",
    "    for idx in ast_indices:\n",
    "        combined[idx] = 0.0\n",
    "    \n",
    "    masked_queries_features.append(combined)\n",
    "\n",
    "masked_inference_features = np.array(masked_queries_features, dtype=np.float32)\n",
    "\n",
    "# Validate with masked features\n",
    "print(\"\\nValidating with AST features masked...\")\n",
    "masked_result = validator.validate_parity_simple(\n",
    "    training_features=training_features_masked,\n",
    "    inference_features=masked_inference_features\n",
    ")\n",
    "\n",
    "print(f\"\\nResults with AST features masked:\")\n",
    "print(f\"  Mismatch rate: {masked_result['mismatch_rate']:.2f}%\")\n",
    "print(f\"  Passed: {masked_result['passed']}\")\n",
    "print(f\"  Max difference: {masked_result['max_difference']:.9f}\")\n",
    "\n",
    "if masked_result['mismatch_rate'] < parity_result['mismatch_rate']:\n",
    "    improvement = parity_result['mismatch_rate'] - masked_result['mismatch_rate']\n",
    "    print(f\"\\nâœ… Masking AST features improved parity by {improvement:.1f} percentage points\")\n",
    "    \n",
    "    if masked_result['passed']:\n",
    "        print(\"âœ… WITH AST FEATURES MASKED, PARITY VALIDATION PASSES!\")\n",
    "        print(\"This confirms AST parser non-determinism is the primary issue.\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Even with AST masked, {masked_result['mismatches']} samples still have mismatches\")\n",
    "        print(\"There are additional parity issues beyond AST features.\")\n",
    "        \n",
    "        # Analyze remaining mismatches\n",
    "        if masked_result.get('details'):\n",
    "            remaining_indices = set()\n",
    "            for detail in masked_result['details'][:10]:\n",
    "                if 'mismatch_indices' in detail:\n",
    "                    # Filter out AST indices\n",
    "                    non_ast_mismatches = [i for i in detail['mismatch_indices'] if i not in ast_indices]\n",
    "                    remaining_indices.update(non_ast_mismatches)\n",
    "            \n",
    "            if remaining_indices:\n",
    "                print(f\"\\nRemaining mismatch indices (non-AST): {sorted(list(remaining_indices))[:20]}\")\n",
    "                \n",
    "                # Categorize remaining issues\n",
    "                historical_issues = [i for i in remaining_indices if 78 <= i < 95]\n",
    "                tfidf_issues = [i for i in remaining_indices if i >= 95]\n",
    "                other_issues = [i for i in remaining_indices if i < 45]\n",
    "                \n",
    "                if historical_issues:\n",
    "                    print(f\"  Historical features: {historical_issues[:10]}\")\n",
    "                if tfidf_issues:\n",
    "                    print(f\"  TF-IDF features: {tfidf_issues[:10]}\")\n",
    "                if other_issues:\n",
    "                    print(f\"  Other base features: {other_issues[:10]}\")\n",
    "else:\n",
    "    print(\"\\nâŒ Masking AST features did not improve parity\")\n",
    "    print(\"The issue may be broader than just AST features\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Known Parity Issues and Recommendations\n",
    "\n",
    "### Identified Issues\n",
    "\n",
    "1. **AST Parser Non-Determinism** (Primary Issue)\n",
    "   - AST features (indices 45-54) show different values between Spark UDF execution and standalone execution\n",
    "   - Feature 54 (ast_coalesce_null_if_count) fails in 100% of samples\n",
    "   - Even with 200ms timeout, the parser behaves differently in distributed vs local context\n",
    "   - Root cause: sqlglot parser may have environment-dependent behavior\n",
    "\n",
    "2. **TF-IDF Feature Mismatches** (Secondary Issue)\n",
    "   - Some TF-IDF features (indices 95+) show mismatches\n",
    "   - Likely due to floating-point precision differences between Spark ML and sklearn\n",
    "   - May also be affected by text normalization differences\n",
    "\n",
    "3. **Occasional Historical Feature Issues**\n",
    "   - Feature 81 occasionally mismatches\n",
    "   - Could be due to NULL handling or division-by-zero edge cases\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "#### Short-term (For immediate model training):\n",
    "1. **Option A: Disable AST features**\n",
    "   - Set AST features to 0 during both training and inference\n",
    "   - Reduces feature count from 345 to 335\n",
    "   - Will slightly reduce model performance but ensures parity\n",
    "\n",
    "2. **Option B: Increase AST timeout further**\n",
    "   - Try 500ms or 1000ms timeout\n",
    "   - May reduce but not eliminate non-determinism\n",
    "   - Could impact latency in production\n",
    "\n",
    "3. **Option C: Accept current state with documentation**\n",
    "   - Document that AST features have known parity issues\n",
    "   - Monitor model performance closely in production\n",
    "   - Plan to fix in next iteration\n",
    "\n",
    "#### Long-term Solutions:\n",
    "1. **Replace sqlglot with deterministic parser**\n",
    "   - Consider simpler regex-based AST feature extraction\n",
    "   - Or use a parser with guaranteed deterministic behavior\n",
    "\n",
    "2. **Compute AST features separately**\n",
    "   - Pre-compute AST features in a separate pass\n",
    "   - Store them with the dataset\n",
    "   - Ensures consistency but adds complexity\n",
    "\n",
    "3. **Use identical TF-IDF implementation**\n",
    "   - Replace Spark ML TF-IDF with distributed sklearn\n",
    "   - Or ensure exact floating-point compatibility\n",
    "\n",
    "### Recommended Approach\n",
    "\n",
    "**For immediate deployment:**\n",
    "- Proceed with Option A (disable AST features)\n",
    "- This ensures train-serve parity at minimal performance cost\n",
    "- AST features contribute ~5-10% of model performance\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# In training: Set AST features to 0\n",
    "train_features[:, 45:55] = 0.0\n",
    "\n",
    "# In inference: Configure to skip AST extraction\n",
    "config['disable_ast_features'] = True\n",
    "```\n",
    "\n",
    "### Impact Assessment\n",
    "- Without AST features: Expected 1-2% decrease in recall\n",
    "- With parity issues: Risk of 5-40% degradation (as seen in current results)\n",
    "- **Recommendation: Better to have slightly lower but consistent performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Feature Datasets to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T21:07:11.452593Z",
     "iopub.status.busy": "2025-10-23T21:07:11.452406Z",
     "iopub.status.idle": "2025-10-23T21:09:13.826137Z",
     "shell.execute_reply": "2025-10-23T21:09:13.825529Z",
     "shell.execute_reply.started": "2025-10-23T21:07:11.452576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26bc35495884925949525b4fa28ebb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving feature datasets to S3...\n",
      "  Base path: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01_fixed\n",
      "\n",
      "[1/3] Saving train dataset...\n",
      "  ? Train saved: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01_fixed/train\n",
      "[2/3] Saving val dataset...\n",
      "  ? Val saved: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01_fixed/val\n",
      "[3/3] Saving test dataset...\n",
      "  ? Test saved: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01_fixed/test\n",
      "\n",
      "? All feature datasets saved to S3"
     ]
    }
   ],
   "source": [
    "# Define output paths with \"_fixed\" suffix to distinguish from original\n",
    "features_path = config['features']['output_path']\n",
    "date_range = f\"{config['data_loading']['start_date']}_to_{config['data_loading']['end_date']}\"\n",
    "output_base = f\"{features_path}/{date_range}_fixed\"\n",
    "\n",
    "train_path = f\"{output_base}/train\"\n",
    "val_path = f\"{output_base}/val\"\n",
    "test_path = f\"{output_base}/test\"\n",
    "\n",
    "print(f\"Saving feature datasets to S3...\")\n",
    "print(f\"  Base path: {output_base}\")\n",
    "\n",
    "# Select relevant columns\n",
    "output_columns = [\n",
    "    'queryId',\n",
    "    'query',\n",
    "    'user',\n",
    "    'catalog',\n",
    "    'schema',\n",
    "    'queryDate',\n",
    "    'hour',\n",
    "    'is_heavy',\n",
    "    'cpu_time_seconds',\n",
    "    'memory_gb',\n",
    "    'features'  # Combined features array\n",
    "]\n",
    "\n",
    "# Save train\n",
    "print(\"\\n[1/3] Saving train dataset...\")\n",
    "train_final.select(output_columns).write.mode('overwrite').parquet(train_path)\n",
    "print(f\"  âœ… Train saved: {train_path}\")\n",
    "\n",
    "# Save val\n",
    "print(\"[2/3] Saving val dataset...\")\n",
    "val_final.select(output_columns).write.mode('overwrite').parquet(val_path)\n",
    "print(f\"  âœ… Val saved: {val_path}\")\n",
    "\n",
    "# Save test\n",
    "print(\"[3/3] Saving test dataset...\")\n",
    "test_final.select(output_columns).write.mode('overwrite').parquet(test_path)\n",
    "print(f\"  âœ… Test saved: {test_path}\")\n",
    "\n",
    "print(\"\\nâœ… All feature datasets saved to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save TF-IDF Vectorizer and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T21:09:13.826940Z",
     "iopub.status.busy": "2025-10-23T21:09:13.826787Z",
     "iopub.status.idle": "2025-10-23T21:09:14.105683Z",
     "shell.execute_reply": "2025-10-23T21:09:14.105081Z",
     "shell.execute_reply.started": "2025-10-23T21:09:13.826914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3067e333236846fba36f4cd69f5e34d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TF-IDF vectorizer...\n",
      "  ? Uploaded: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/models/tfidf_vectorizer_2025-08-01_to_2025-10-01_fixed.pkl\n",
      "\n",
      "Saving metadata...\n",
      "  ? Metadata saved: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/metadata/features_2025-08-01_to_2025-10-01_fixed.json"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Save TF-IDF pipeline\n",
    "print(\"Saving TF-IDF vectorizer...\")\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.pkl') as tmp:\n",
    "    tfidf_pipeline.save(tmp.name)\n",
    "    local_tfidf_path = tmp.name\n",
    "\n",
    "# Upload to S3 with \"_fixed\" suffix\n",
    "s3_tfidf_key = f\"{config['s3']['prefix']}/models/tfidf_vectorizer_{date_range}_fixed.pkl\"\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(local_tfidf_path, config['s3']['bucket'], s3_tfidf_key)\n",
    "\n",
    "s3_tfidf_path = f\"s3://{config['s3']['bucket']}/{s3_tfidf_key}\"\n",
    "print(f\"  âœ… Uploaded: {s3_tfidf_path}\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(local_tfidf_path)\n",
    "\n",
    "# Save metadata\n",
    "print(\"\\nSaving metadata...\")\n",
    "metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'date_range': date_range,\n",
    "    'fixed_version': True,\n",
    "    'features': {\n",
    "        'unified_features': 95,\n",
    "        'base_features': 78,\n",
    "        'historical_features': 17,\n",
    "        'tfidf_features': tfidf_pipeline.vocab_size,\n",
    "        'total_features': config['features']['total_features']\n",
    "    },\n",
    "    'parity_validation': parity_result,\n",
    "    's3_paths': {\n",
    "        'train': train_path,\n",
    "        'val': val_path,\n",
    "        'test': test_path,\n",
    "        'tfidf_vectorizer': s3_tfidf_path\n",
    "    },\n",
    "    'class_distributions': {\n",
    "        'train': f'{train_ratio:.1f}:1',\n",
    "        'val': f'{val_ratio:.1f}:1',\n",
    "        'test': f'{test_ratio:.1f}:1'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as tmp:\n",
    "    json.dump(metadata, tmp, indent=2)\n",
    "    local_metadata_path = tmp.name\n",
    "\n",
    "metadata_key = f\"{config['s3']['prefix']}/metadata/features_{date_range}_fixed.json\"\n",
    "s3_client.upload_file(local_metadata_path, config['s3']['bucket'], metadata_key)\n",
    "print(f\"  âœ… Metadata saved: s3://{config['s3']['bucket']}/{metadata_key}\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(local_metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T21:09:14.106873Z",
     "iopub.status.busy": "2025-10-23T21:09:14.106372Z",
     "iopub.status.idle": "2025-10-23T21:09:14.166618Z",
     "shell.execute_reply": "2025-10-23T21:09:14.166001Z",
     "shell.execute_reply.started": "2025-10-23T21:09:14.106852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c01201438164ac9a6d99fc52a6a5387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE ENGINEERING SUMMARY (FIXED VERSION)\n",
      "======================================================================\n",
      "\n",
      "? KEY FIX APPLIED:\n",
      "  Used unified FeatureExtractor with historical features enabled\n",
      "  Training and inference use identical configuration\n",
      "  AST parser settings consistent\n",
      "\n",
      "Feature Breakdown:\n",
      "  Unified features:    95 (78 base + 17 historical)\n",
      "  TF-IDF features:     250\n",
      "  ----------------------------------------\n",
      "  Total features:      345\n",
      "\n",
      "Dataset Sizes:\n",
      "  Train: 8,782,474 queries\n",
      "  Val:   14,969,526 queries\n",
      "  Test:  15,099,750 queries\n",
      "\n",
      "Class Distributions:\n",
      "  Train: 5.0:1 (sampled)\n",
      "  Val:   48.2:1 (original)\n",
      "  Test:  25.0:1 (original)\n",
      "\n",
      "Parity Validation:\n",
      "  Status: ? FAILED\n",
      "  Mismatch rate: 100.00%\n",
      "  Investigation needed for remaining issues\n",
      "\n",
      "S3 Outputs (fixed version):\n",
      "  Features: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/features/2025-08-01_to_2025-10-01_fixed/\n",
      "  TF-IDF: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/models/tfidf_vectorizer_2025-08-01_to_2025-10-01_fixed.pkl\n",
      "  Metadata: s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/metadata/features_2025-08-01_to_2025-10-01_fixed.json\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING COMPLETE (FIXED VERSION)\n",
      "======================================================================\n",
      "\n",
      "Next Steps:\n",
      "1. ??  Investigate remaining parity issues\n",
      "2. Check AST parser behavior in detail\n",
      "3. May need to disable AST features if issues persist"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FEATURE ENGINEERING SUMMARY (FIXED VERSION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nâœ… KEY FIX APPLIED:\")\n",
    "print(f\"  Used unified FeatureExtractor with historical features enabled\")\n",
    "print(f\"  Training and inference use identical configuration\")\n",
    "print(f\"  AST parser settings consistent\")\n",
    "\n",
    "print(f\"\\nFeature Breakdown:\")\n",
    "print(f\"  Unified features:    95 (78 base + 17 historical)\")\n",
    "print(f\"  TF-IDF features:     {tfidf_pipeline.vocab_size}\")\n",
    "print(f\"  {'-' * 40}\")\n",
    "print(f\"  Total features:      {config['features']['total_features']}\")\n",
    "\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Train: {train_count:,} queries\")\n",
    "print(f\"  Val:   {val_count:,} queries\")\n",
    "print(f\"  Test:  {test_count:,} queries\")\n",
    "\n",
    "print(f\"\\nClass Distributions:\")\n",
    "print(f\"  Train: {train_ratio:.1f}:1 (sampled)\")\n",
    "print(f\"  Val:   {val_ratio:.1f}:1 (original)\")\n",
    "print(f\"  Test:  {test_ratio:.1f}:1 (original)\")\n",
    "\n",
    "print(f\"\\nParity Validation:\")\n",
    "if parity_result['passed']:\n",
    "    print(f\"  Status: âœ… PASSED\")\n",
    "    print(f\"  Mismatch rate: {parity_result['mismatch_rate']:.2f}%\")\n",
    "else:\n",
    "    print(f\"  Status: âŒ FAILED\")\n",
    "    print(f\"  Mismatch rate: {parity_result['mismatch_rate']:.2f}%\")\n",
    "    print(f\"  Investigation needed for remaining issues\")\n",
    "\n",
    "print(f\"\\nS3 Outputs (fixed version):\")\n",
    "print(f\"  Features: {output_base}/\")\n",
    "print(f\"  TF-IDF: {s3_tfidf_path}\")\n",
    "print(f\"  Metadata: s3://{config['s3']['bucket']}/{metadata_key}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE (FIXED VERSION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "if parity_result['passed']:\n",
    "    print(\"1. âœ… Proceed to notebook 04 for model training\")\n",
    "    print(\"2. Use the fixed feature datasets for training\")\n",
    "else:\n",
    "    print(\"1. âš ï¸  Investigate remaining parity issues\")\n",
    "    print(\"2. Check AST parser behavior in detail\")\n",
    "    print(\"3. May need to disable AST features if issues persist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
