{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Feature Engineering (FIXED)\n",
    "\n",
    "**Purpose**: Extract 95 + 250 features with train-serve parity validation\n",
    "\n",
    "**FIXED VERSION**: This notebook fixes the feature parity issues by using a unified extractor approach.\n",
    "\n",
    "**Pipeline**:\n",
    "1. Load pre-split datasets from notebook 01\n",
    "2. Compute historical statistics from training data\n",
    "3. Extract 95 features using unified FeatureExtractor (78 base + 17 historical)\n",
    "4. Build TF-IDF vocabulary (training data ONLY)\n",
    "5. Extract TF-IDF features for all splits\n",
    "6. Combine features (95 + 250 = 345 total)\n",
    "7. Validate feature parity\n",
    "8. Save to S3\n",
    "\n",
    "**Key Fix**: Uses unified FeatureExtractor with historical features enabled for both training and inference.\n",
    "\n",
    "**Duration**: ~45-60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Configuration\n",
    "\n",
    "Copy the Spark configuration from notebook 00 output and paste below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"pyFiles\": [\n",
    "        \"s3://uip-datalake-bucket-prod/sf_trino/trino_query_predictor/code/query_predictor_latest.zip\",\n",
    "        \"s3://uipds-108043591022/dataintelligence-dev/di-airflow-prod/dags/common/utils/ParseArgs.py\"\n",
    "    ],\n",
    "    \"driverMemory\": \"16G\",\n",
    "    \"driverCores\": 4,\n",
    "    \"executorMemory\": \"20G\",\n",
    "    \"executorCores\": 5,\n",
    "    \"conf\": {\n",
    "        \"spark.driver.maxResultSize\": \"8G\",\n",
    "        \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "        \"spark.dynamicAllocation.minExecutors\": \"2\",\n",
    "        \"spark.dynamicAllocation.maxExecutors\": \"20\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "# Import production modules\n",
    "from query_predictor.core.featurizer.feature_extractor import FeatureExtractor\n",
    "from query_predictor.training.spark_ml_tfidf_pipeline import SparkMLTfidfPipeline\n",
    "from query_predictor.training.parity_validator import ParityValidator\n",
    "from query_predictor.training.historical_stats_computer import HistoricalStatsComputer\n",
    "from query_predictor.training.checkpoint_manager import CheckpointManager\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PySpark version: {spark.version}\")\n",
    "print(\"✅ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Download training configuration from S3\n",
    "s3_client = boto3.client('s3')\n",
    "s3_bucket = 'uip-datalake-bucket-prod'\n",
    "s3_prefix = 'sf_trino/trino_query_predictor'\n",
    "config_s3_key = f\"{s3_prefix}/config/training_config_latest.yaml\"\n",
    "config_path = '/tmp/training_config.yaml'\n",
    "\n",
    "print(f\"Downloading config from S3: s3://{s3_bucket}/{config_s3_key}\")\n",
    "s3_client.download_file(s3_bucket, config_s3_key, config_path)\n",
    "\n",
    "# Load training configuration\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_mgr = CheckpointManager(\n",
    "    spark,\n",
    "    s3_checkpoint_path=config['checkpointing']['s3_path'],\n",
    "    enabled=config['checkpointing']['enabled']\n",
    ")\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"\\n📋 Feature Configuration:\")\n",
    "print(f\"  Base features: {config['features']['base_feature_count']}\")\n",
    "print(f\"  Historical features: {config['features']['historical_feature_count']}\")\n",
    "print(f\"  TF-IDF vocab size: {config['features']['tfidf_vocab_size']}\")\n",
    "print(f\"  Total features: {config['features']['total_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pre-Split Data from Notebook 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-split datasets from notebook 01\n",
    "processed_path = config['data_loading']['processed_output_path']\n",
    "date_range = f\"{config['data_loading']['start_date']}_to_{config['data_loading']['end_date']}\"\n",
    "base_path = f\"{processed_path}/{date_range}\"\n",
    "\n",
    "train_path = f\"{base_path}/train_sampled\"  # 5:1 sampled for training\n",
    "val_path = f\"{base_path}/val_original\"      # ~36:1 original distribution\n",
    "test_path = f\"{base_path}/test_original\"    # ~36:1 original distribution\n",
    "\n",
    "print(f\"Loading pre-split datasets...\")\n",
    "print(f\"  Train (sampled): {train_path}\")\n",
    "print(f\"  Val (original):  {val_path}\")\n",
    "print(f\"  Test (original): {test_path}\")\n",
    "\n",
    "# Load splits\n",
    "train_df = spark.read.parquet(train_path)\n",
    "val_df = spark.read.parquet(val_path)\n",
    "test_df = spark.read.parquet(test_path)\n",
    "\n",
    "# Get counts\n",
    "train_count = train_df.count()\n",
    "val_count = val_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"\\n✅ Datasets loaded:\")\n",
    "print(f\"  Train: {train_count:,} queries\")\n",
    "print(f\"  Val:   {val_count:,} queries\")\n",
    "print(f\"  Test:  {test_count:,} queries\")\n",
    "\n",
    "# Calculate ratios for reporting\n",
    "train_heavy = train_df.filter(F.col('is_heavy') == 1).count()\n",
    "train_ratio = (train_count - train_heavy) / train_heavy if train_heavy > 0 else 0\n",
    "\n",
    "val_heavy = val_df.filter(F.col('is_heavy') == 1).count()\n",
    "val_ratio = (val_count - val_heavy) / val_heavy if val_heavy > 0 else 0\n",
    "\n",
    "test_heavy = test_df.filter(F.col('is_heavy') == 1).count()\n",
    "test_ratio = (test_count - test_heavy) / test_heavy if test_heavy > 0 else 0\n",
    "\n",
    "print(f\"\\nDistribution ratios (Small:Heavy):\")\n",
    "print(f\"  Train: {train_ratio:.1f}:1 (sampled)\")\n",
    "print(f\"  Val:   {val_ratio:.1f}:1 (original)\")\n",
    "print(f\"  Test:  {test_ratio:.1f}:1 (original)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Historical Statistics\n",
    "\n",
    "Compute statistics from training data only to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing historical statistics from training data...\")\n",
    "\n",
    "# Initialize stats computer\n",
    "stats_computer = HistoricalStatsComputer(version='1.0.0')\n",
    "\n",
    "# Compute stats from training data\n",
    "date_range_dict = {\n",
    "    'start': config['data_loading']['start_date'],\n",
    "    'end': config['data_loading']['end_date']\n",
    "}\n",
    "stats_schema = stats_computer.compute(train_df, date_range_dict)\n",
    "\n",
    "print(f\"\\n✅ Historical stats computed:\")\n",
    "print(f\"  Users: {len(stats_schema.users):,}\")\n",
    "print(f\"  Catalogs: {len(stats_schema.catalogs):,}\")\n",
    "print(f\"  Schemas: {len(stats_schema.schemas):,}\")\n",
    "print(f\"  Overall heavy rate: {stats_schema.heavy_rate_overall:.2%}\")\n",
    "\n",
    "# Serialize to dict for FeatureExtractor\n",
    "stats_dict = stats_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Unified Feature Extractor\n",
    "\n",
    "**KEY FIX**: Use a single FeatureExtractor with historical features enabled.\n",
    "This ensures consistent feature computation between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unified configuration with historical features enabled\n",
    "unified_config = config.copy()\n",
    "unified_config['enable_historical_features'] = True\n",
    "\n",
    "# Ensure consistent AST parser settings\n",
    "unified_config['ast_timeout_ms'] = 50\n",
    "unified_config['ast_fallback_on_timeout'] = True\n",
    "\n",
    "# Initialize unified feature extractor with historical stats\n",
    "unified_extractor = FeatureExtractor(\n",
    "    unified_config,\n",
    "    historical_stats=stats_dict\n",
    ")\n",
    "\n",
    "print(\"✅ Unified FeatureExtractor initialized\")\n",
    "print(f\"  Feature count: {unified_extractor.feature_count}\")\n",
    "print(f\"  Expected: 95 (78 base + 17 historical)\")\n",
    "print(f\"  Historical features enabled: True\")\n",
    "print(f\"  AST timeout: {unified_config['ast_timeout_ms']}ms\")\n",
    "\n",
    "assert unified_extractor.feature_count == 95, f\"Expected 95 features, got {unified_extractor.feature_count}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Unified Features (95 features)\n",
    "\n",
    "Extract base + historical features together using the unified extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark UDF for distributed extraction\n",
    "unified_udf = unified_extractor.create_spark_udf()\n",
    "\n",
    "print(\"Extracting unified features (base + historical) for all splits...\")\n",
    "print(\"This extracts 95 features in a single pass.\\n\")\n",
    "\n",
    "# Extract for train\n",
    "print(\"[1/3] Extracting train features...\")\n",
    "train_unified = train_df.withColumn(\n",
    "    'unified_features',\n",
    "    unified_udf(\n",
    "        F.struct(\n",
    "            F.col('query'),\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema'),\n",
    "            F.col('hour'),\n",
    "            F.col('clientInfo')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# train_unified = checkpoint_mgr.checkpoint(train_unified, \"03_train_unified_fixed\")\n",
    "\n",
    "# Extract for val\n",
    "print(\"[2/3] Extracting val features...\")\n",
    "val_unified = val_df.withColumn(\n",
    "    'unified_features',\n",
    "    unified_udf(\n",
    "        F.struct(\n",
    "            F.col('query'),\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema'),\n",
    "            F.col('hour'),\n",
    "            F.col('clientInfo')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# val_unified = checkpoint_mgr.checkpoint(val_unified, \"03_val_unified_fixed\")\n",
    "\n",
    "# Extract for test\n",
    "print(\"[3/3] Extracting test features...\")\n",
    "test_unified = test_df.withColumn(\n",
    "    'unified_features',\n",
    "    unified_udf(\n",
    "        F.struct(\n",
    "            F.col('query'),\n",
    "            F.col('user'),\n",
    "            F.col('catalog'),\n",
    "            F.col('schema'),\n",
    "            F.col('hour'),\n",
    "            F.col('clientInfo')\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# test_unified = checkpoint_mgr.checkpoint(test_unified, \"03_test_unified_fixed\")\n",
    "\n",
    "print(\"\\n✅ Unified features extracted for all splits (95 features each)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify Unified Feature Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample to verify dimensions\n",
    "sample_train = train_unified.select('unified_features').limit(1).collect()[0]\n",
    "unified_dim = len(sample_train['unified_features'])\n",
    "\n",
    "print(f\"Unified feature dimensions:\")\n",
    "print(f\"  Actual: {unified_dim}\")\n",
    "print(f\"  Expected: 95\")\n",
    "\n",
    "assert unified_dim == 95, f\"Unified features should be 95, got {unified_dim}\"\n",
    "print(\"\\n✅ Unified feature dimensions validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build TF-IDF Vocabulary (TRAINING DATA ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark ML TF-IDF pipeline with SQL-aware optimizations\n",
    "tfidf_config = {\n",
    "    'tfidf_vocab_size': config['features']['tfidf_vocab_size'],\n",
    "    'min_df': config['features']['min_df'],\n",
    "    'max_df': config['features']['max_df'],\n",
    "    'use_binary': config['features'].get('use_binary', True),\n",
    "    'filter_sql_keywords': config['features'].get('filter_sql_keywords', True),\n",
    "    'normalize_sql': config['features'].get('normalize_sql', True)\n",
    "}\n",
    "\n",
    "tfidf_pipeline = SparkMLTfidfPipeline(tfidf_config)\n",
    "\n",
    "print(\"Building TF-IDF vocabulary on TRAINING DATA ONLY...\")\n",
    "print(f\"  Config: vocab_size={tfidf_config['tfidf_vocab_size']}, min_df={tfidf_config['min_df']}, max_df={tfidf_config['max_df']}\")\n",
    "print(f\"  SQL optimizations: binary={tfidf_config['use_binary']}, filter_keywords={tfidf_config['filter_sql_keywords']}\")\n",
    "print(\"  This prevents data leakage into val/test sets.\\n\")\n",
    "\n",
    "# Fit on DataFrame directly (NO COLLECT!)\n",
    "tfidf_pipeline.fit_on_dataframe(train_unified, query_column='query')\n",
    "\n",
    "print(f\"\\n✅ TF-IDF vocabulary built successfully\")\n",
    "metadata = tfidf_pipeline.get_feature_metadata()\n",
    "print(f\"  Vocabulary size: {metadata['vocab_size']:,}\")\n",
    "print(f\"  Method: {metadata['method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Extract TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark UDF from fitted pipeline\n",
    "tfidf_udf = tfidf_pipeline.create_spark_udf()\n",
    "\n",
    "print(\"Extracting TF-IDF features for all splits...\")\n",
    "\n",
    "# Extract for train\n",
    "print(\"\\n[1/3] Extracting train TF-IDF features...\")\n",
    "train_tfidf = train_unified.withColumn('tfidf_features', tfidf_udf(F.col('query')))\n",
    "# train_tfidf = checkpoint_mgr.checkpoint(train_tfidf, \"03_train_tfidf_fixed\")\n",
    "\n",
    "# Extract for val\n",
    "print(\"[2/3] Extracting val TF-IDF features...\")\n",
    "val_tfidf = val_unified.withColumn('tfidf_features', tfidf_udf(F.col('query')))\n",
    "# val_tfidf = checkpoint_mgr.checkpoint(val_tfidf, \"03_val_tfidf_fixed\")\n",
    "\n",
    "# Extract for test\n",
    "print(\"[3/3] Extracting test TF-IDF features...\")\n",
    "test_tfidf = test_unified.withColumn('tfidf_features', tfidf_udf(F.col('query')))\n",
    "# test_tfidf = checkpoint_mgr.checkpoint(test_tfidf, \"03_test_tfidf_fixed\")\n",
    "\n",
    "print(\"\\n✅ TF-IDF features extracted for all splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Combine Features\n",
    "\n",
    "Concatenate unified features (95) + TF-IDF features (250) = 345 total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "@udf(returnType=ArrayType(FloatType()))\n",
    "def combine_features(unified, tfidf):\n",
    "    \"\"\"Concatenate unified + tfidf features.\"\"\"\n",
    "    if unified is None or tfidf is None:\n",
    "        return None\n",
    "    return unified + tfidf\n",
    "\n",
    "print(\"Combining unified and TF-IDF features...\")\n",
    "\n",
    "# Combine for train\n",
    "train_final = train_tfidf.withColumn(\n",
    "    'features',\n",
    "    combine_features(\n",
    "        F.col('unified_features'),\n",
    "        F.col('tfidf_features')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine for val\n",
    "val_final = val_tfidf.withColumn(\n",
    "    'features',\n",
    "    combine_features(\n",
    "        F.col('unified_features'),\n",
    "        F.col('tfidf_features')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine for test\n",
    "test_final = test_tfidf.withColumn(\n",
    "    'features',\n",
    "    combine_features(\n",
    "        F.col('unified_features'),\n",
    "        F.col('tfidf_features')\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Features combined\")\n",
    "print(f\"  Unified: 95 (78 base + 17 historical)\")\n",
    "print(f\"  TF-IDF: {config['features']['tfidf_vocab_size']}\")\n",
    "print(f\"  Total: {config['features']['total_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Validate Feature Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and verify dimensions\n",
    "print(\"Validating final feature dimensions...\")\n",
    "\n",
    "sample_train = train_final.select('features', 'is_heavy').limit(1).collect()[0]\n",
    "sample_val = val_final.select('features', 'is_heavy').limit(1).collect()[0]\n",
    "sample_test = test_final.select('features', 'is_heavy').limit(1).collect()[0]\n",
    "\n",
    "train_dim = len(sample_train['features'])\n",
    "val_dim = len(sample_val['features'])\n",
    "test_dim = len(sample_test['features'])\n",
    "expected_dim = config['features']['total_features']\n",
    "\n",
    "print(f\"\\n📊 Feature Dimensions:\")\n",
    "print(f\"  Train: {train_dim}\")\n",
    "print(f\"  Val:   {val_dim}\")\n",
    "print(f\"  Test:  {test_dim}\")\n",
    "print(f\"  Expected: {expected_dim}\")\n",
    "\n",
    "assert train_dim == expected_dim, f\"Train dimension mismatch: {train_dim} != {expected_dim}\"\n",
    "assert val_dim == expected_dim, f\"Val dimension mismatch: {val_dim} != {expected_dim}\"\n",
    "assert test_dim == expected_dim, f\"Test dimension mismatch: {test_dim} != {expected_dim}\"\n",
    "\n",
    "print(\"\\n✅ All dimensions validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Feature Parity Validation\n",
    "\n",
    "**CRITICAL**: Validate that training features match inference features.\n",
    "This should now pass with the unified extractor approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FEATURE PARITY VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize validator\n",
    "validation_config = config.get('validation', {})\n",
    "n_samples = validation_config.get('parity_samples', 100)\n",
    "validator = ParityValidator(config=config)\n",
    "\n",
    "# Collect sample of training features and queries\n",
    "print(f\"\\nCollecting {n_samples} samples for validation...\")\n",
    "train_samples = train_final.select(\n",
    "    'features', 'query', 'user', 'catalog', 'schema', 'hour', 'clientInfo', 'is_heavy'\n",
    ").limit(n_samples).collect()\n",
    "\n",
    "# Convert to numpy arrays\n",
    "training_features = np.array([row['features'] for row in train_samples], dtype=np.float32)\n",
    "\n",
    "# Prepare sample queries for inference\n",
    "sample_queries = [\n",
    "    {\n",
    "        'query': row['query'],\n",
    "        'user': row['user'],\n",
    "        'catalog': row['catalog'],\n",
    "        'schema': row['schema'],\n",
    "        'hour': row['hour'],\n",
    "        'clientInfo': row['clientInfo']\n",
    "    }\n",
    "    for row in train_samples\n",
    "]\n",
    "\n",
    "print(f\"\\nRunning parity validation...\")\n",
    "print(f\"  Tolerance: {validator.tolerance}\")\n",
    "print(f\"  Success threshold: <{validation_config.get('parity_success_threshold', 0.5)}% mismatch\")\n",
    "\n",
    "# Create inference featurizer with IDENTICAL configuration\n",
    "# This is the KEY FIX - using the same unified configuration\n",
    "inference_featurizer = FeatureExtractor(\n",
    "    unified_config,  # Same config as training\n",
    "    historical_stats=stats_dict  # Same historical stats\n",
    ")\n",
    "\n",
    "print(f\"\\nInference featurizer initialized (identical to training):\")\n",
    "print(f\"  Feature count: {inference_featurizer.feature_count}\")\n",
    "print(f\"  Historical features enabled: True\")\n",
    "print(f\"  Config matches training: YES\\n\")\n",
    "\n",
    "# Run validation\n",
    "parity_result = validator.validate_parity(\n",
    "    training_features=training_features,\n",
    "    inference_featurizer=inference_featurizer,\n",
    "    tfidf_pipeline=tfidf_pipeline,\n",
    "    sample_queries=sample_queries,\n",
    "    n_samples=n_samples\n",
    ")\n",
    "\n",
    "# Generate and print report\n",
    "report = validator.generate_report(parity_result)\n",
    "print(report)\n",
    "\n",
    "if not parity_result['passed']:\n",
    "    print(\"\\n⚠️  WARNING: Parity validation still failing!\")\n",
    "    print(\"Debugging information:\")\n",
    "    print(f\"  - Training used unified_extractor with {unified_extractor.feature_count} features\")\n",
    "    print(f\"  - Inference using identical configuration\")\n",
    "    print(f\"  - Mismatched features: {parity_result['mismatches'][0]['indices'][:10] if parity_result['mismatches'] else 'N/A'}\")\n",
    "    # Don't raise error - allow notebook to continue for debugging\n",
    "else:\n",
    "    print(\"\\n✅ PARITY VALIDATION PASSED!\")\n",
    "    print(\"Features are consistent between training and inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Debug Feature Differences (if parity fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug cell - only run if parity validation fails\n",
    "if not parity_result['passed']:\n",
    "    print(\"Debugging feature differences...\\n\")\n",
    "    \n",
    "    # Get first sample\n",
    "    sample_idx = 0\n",
    "    sample_query = sample_queries[sample_idx]\n",
    "    training_feat = training_features[sample_idx]\n",
    "    \n",
    "    # Extract features using inference path\n",
    "    inference_feat = inference_featurizer.extract(sample_query)\n",
    "    tfidf_feat = tfidf_pipeline.transform_single(sample_query['query'])\n",
    "    combined_inference = np.concatenate([inference_feat, tfidf_feat])\n",
    "    \n",
    "    # Find differences\n",
    "    diff = np.abs(training_feat - combined_inference)\n",
    "    mismatch_indices = np.where(diff > validator.tolerance)[0]\n",
    "    \n",
    "    print(f\"Sample {sample_idx} analysis:\")\n",
    "    print(f\"  Total mismatches: {len(mismatch_indices)}\")\n",
    "    print(f\"  Mismatch indices: {mismatch_indices[:20]}\")\n",
    "    \n",
    "    # Check specific feature ranges\n",
    "    print(f\"\\nFeature range analysis:\")\n",
    "    print(f\"  AST features (45-54): {[i for i in mismatch_indices if 45 <= i <= 54]}\")\n",
    "    print(f\"  Historical boundary (78-94): {[i for i in mismatch_indices if 78 <= i <= 94]}\")\n",
    "    print(f\"  TF-IDF start (95+): {[i for i in mismatch_indices if i >= 95]}\")\n",
    "    \n",
    "    # Sample specific features\n",
    "    if 45 in mismatch_indices:\n",
    "        print(f\"\\nAST feature 45 (ast_depth):\")\n",
    "        print(f\"  Training: {training_feat[45]}\")\n",
    "        print(f\"  Inference: {combined_inference[45]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Feature Datasets to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output paths with \"_fixed\" suffix to distinguish from original\n",
    "features_path = config['features']['output_path']\n",
    "date_range = f\"{config['data_loading']['start_date']}_to_{config['data_loading']['end_date']}\"\n",
    "output_base = f\"{features_path}/{date_range}_fixed\"\n",
    "\n",
    "train_path = f\"{output_base}/train\"\n",
    "val_path = f\"{output_base}/val\"\n",
    "test_path = f\"{output_base}/test\"\n",
    "\n",
    "print(f\"Saving feature datasets to S3...\")\n",
    "print(f\"  Base path: {output_base}\")\n",
    "\n",
    "# Select relevant columns\n",
    "output_columns = [\n",
    "    'queryId',\n",
    "    'query',\n",
    "    'user',\n",
    "    'catalog',\n",
    "    'schema',\n",
    "    'queryDate',\n",
    "    'hour',\n",
    "    'is_heavy',\n",
    "    'cpu_time_seconds',\n",
    "    'memory_gb',\n",
    "    'features'  # Combined features array\n",
    "]\n",
    "\n",
    "# Save train\n",
    "print(\"\\n[1/3] Saving train dataset...\")\n",
    "train_final.select(output_columns).write.mode('overwrite').parquet(train_path)\n",
    "print(f\"  ✅ Train saved: {train_path}\")\n",
    "\n",
    "# Save val\n",
    "print(\"[2/3] Saving val dataset...\")\n",
    "val_final.select(output_columns).write.mode('overwrite').parquet(val_path)\n",
    "print(f\"  ✅ Val saved: {val_path}\")\n",
    "\n",
    "# Save test\n",
    "print(\"[3/3] Saving test dataset...\")\n",
    "test_final.select(output_columns).write.mode('overwrite').parquet(test_path)\n",
    "print(f\"  ✅ Test saved: {test_path}\")\n",
    "\n",
    "print(\"\\n✅ All feature datasets saved to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save TF-IDF Vectorizer and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Save TF-IDF pipeline\n",
    "print(\"Saving TF-IDF vectorizer...\")\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.pkl') as tmp:\n",
    "    tfidf_pipeline.save(tmp.name)\n",
    "    local_tfidf_path = tmp.name\n",
    "\n",
    "# Upload to S3 with \"_fixed\" suffix\n",
    "s3_tfidf_key = f\"{config['s3']['prefix']}/models/tfidf_vectorizer_{date_range}_fixed.pkl\"\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(local_tfidf_path, config['s3']['bucket'], s3_tfidf_key)\n",
    "\n",
    "s3_tfidf_path = f\"s3://{config['s3']['bucket']}/{s3_tfidf_key}\"\n",
    "print(f\"  ✅ Uploaded: {s3_tfidf_path}\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(local_tfidf_path)\n",
    "\n",
    "# Save metadata\n",
    "print(\"\\nSaving metadata...\")\n",
    "metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'date_range': date_range,\n",
    "    'fixed_version': True,\n",
    "    'features': {\n",
    "        'unified_features': 95,\n",
    "        'base_features': 78,\n",
    "        'historical_features': 17,\n",
    "        'tfidf_features': tfidf_pipeline.vocab_size,\n",
    "        'total_features': config['features']['total_features']\n",
    "    },\n",
    "    'parity_validation': parity_result,\n",
    "    's3_paths': {\n",
    "        'train': train_path,\n",
    "        'val': val_path,\n",
    "        'test': test_path,\n",
    "        'tfidf_vectorizer': s3_tfidf_path\n",
    "    },\n",
    "    'class_distributions': {\n",
    "        'train': f'{train_ratio:.1f}:1',\n",
    "        'val': f'{val_ratio:.1f}:1',\n",
    "        'test': f'{test_ratio:.1f}:1'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json') as tmp:\n",
    "    json.dump(metadata, tmp, indent=2)\n",
    "    local_metadata_path = tmp.name\n",
    "\n",
    "metadata_key = f\"{config['s3']['prefix']}/metadata/features_{date_range}_fixed.json\"\n",
    "s3_client.upload_file(local_metadata_path, config['s3']['bucket'], metadata_key)\n",
    "print(f\"  ✅ Metadata saved: s3://{config['s3']['bucket']}/{metadata_key}\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(local_metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FEATURE ENGINEERING SUMMARY (FIXED VERSION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n✅ KEY FIX APPLIED:\")\n",
    "print(f\"  Used unified FeatureExtractor with historical features enabled\")\n",
    "print(f\"  Training and inference use identical configuration\")\n",
    "print(f\"  AST parser settings consistent\")\n",
    "\n",
    "print(f\"\\nFeature Breakdown:\")\n",
    "print(f\"  Unified features:    95 (78 base + 17 historical)\")\n",
    "print(f\"  TF-IDF features:     {tfidf_pipeline.vocab_size}\")\n",
    "print(f\"  {'-' * 40}\")\n",
    "print(f\"  Total features:      {config['features']['total_features']}\")\n",
    "\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Train: {train_count:,} queries\")\n",
    "print(f\"  Val:   {val_count:,} queries\")\n",
    "print(f\"  Test:  {test_count:,} queries\")\n",
    "\n",
    "print(f\"\\nClass Distributions:\")\n",
    "print(f\"  Train: {train_ratio:.1f}:1 (sampled)\")\n",
    "print(f\"  Val:   {val_ratio:.1f}:1 (original)\")\n",
    "print(f\"  Test:  {test_ratio:.1f}:1 (original)\")\n",
    "\n",
    "print(f\"\\nParity Validation:\")\n",
    "if parity_result['passed']:\n",
    "    print(f\"  Status: ✅ PASSED\")\n",
    "    print(f\"  Mismatch rate: {parity_result['mismatch_rate']:.2f}%\")\n",
    "else:\n",
    "    print(f\"  Status: ❌ FAILED\")\n",
    "    print(f\"  Mismatch rate: {parity_result['mismatch_rate']:.2f}%\")\n",
    "    print(f\"  Investigation needed for remaining issues\")\n",
    "\n",
    "print(f\"\\nS3 Outputs (fixed version):\")\n",
    "print(f\"  Features: {output_base}/\")\n",
    "print(f\"  TF-IDF: {s3_tfidf_path}\")\n",
    "print(f\"  Metadata: s3://{config['s3']['bucket']}/{metadata_key}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE (FIXED VERSION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "if parity_result['passed']:\n",
    "    print(\"1. ✅ Proceed to notebook 04 for model training\")\n",
    "    print(\"2. Use the fixed feature datasets for training\")\n",
    "else:\n",
    "    print(\"1. ⚠️  Investigate remaining parity issues\")\n",
    "    print(\"2. Check AST parser behavior in detail\")\n",
    "    print(\"3. May need to disable AST features if issues persist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
